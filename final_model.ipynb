{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv file\n",
    "df = pd.read_csv(r\"C:\\Users\\admin\\Desktop\\Door-Detection\\Door-Detection\\dataset_289_samples_final_3am.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Distance0</th>\n",
       "      <th>Distance1</th>\n",
       "      <th>Distance2</th>\n",
       "      <th>Distance3</th>\n",
       "      <th>Distance4</th>\n",
       "      <th>Distance5</th>\n",
       "      <th>Distance6</th>\n",
       "      <th>Distance7</th>\n",
       "      <th>Distance8</th>\n",
       "      <th>...</th>\n",
       "      <th>Distance80</th>\n",
       "      <th>Distance81</th>\n",
       "      <th>Distance82</th>\n",
       "      <th>Distance83</th>\n",
       "      <th>Distance84</th>\n",
       "      <th>Distance85</th>\n",
       "      <th>Distance86</th>\n",
       "      <th>Distance87</th>\n",
       "      <th>Distance88</th>\n",
       "      <th>Distance89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>178</td>\n",
       "      <td>114</td>\n",
       "      <td>179</td>\n",
       "      <td>97</td>\n",
       "      <td>67</td>\n",
       "      <td>130</td>\n",
       "      <td>73</td>\n",
       "      <td>132</td>\n",
       "      <td>93</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>89</td>\n",
       "      <td>90</td>\n",
       "      <td>57</td>\n",
       "      <td>54</td>\n",
       "      <td>48</td>\n",
       "      <td>68</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>176</td>\n",
       "      <td>178</td>\n",
       "      <td>33</td>\n",
       "      <td>24</td>\n",
       "      <td>96</td>\n",
       "      <td>62</td>\n",
       "      <td>103</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>84</td>\n",
       "      <td>90</td>\n",
       "      <td>89</td>\n",
       "      <td>63</td>\n",
       "      <td>51</td>\n",
       "      <td>88</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>83</td>\n",
       "      <td>68</td>\n",
       "      <td>86</td>\n",
       "      <td>60</td>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>7</td>\n",
       "      <td>53</td>\n",
       "      <td>64</td>\n",
       "      <td>76</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "      <td>99</td>\n",
       "      <td>80</td>\n",
       "      <td>98</td>\n",
       "      <td>93</td>\n",
       "      <td>64</td>\n",
       "      <td>94</td>\n",
       "      <td>98</td>\n",
       "      <td>...</td>\n",
       "      <td>87</td>\n",
       "      <td>68</td>\n",
       "      <td>117</td>\n",
       "      <td>97</td>\n",
       "      <td>120</td>\n",
       "      <td>119</td>\n",
       "      <td>73</td>\n",
       "      <td>80</td>\n",
       "      <td>119</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>69</td>\n",
       "      <td>64</td>\n",
       "      <td>68</td>\n",
       "      <td>58</td>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>103</td>\n",
       "      <td>132</td>\n",
       "      <td>74</td>\n",
       "      <td>72</td>\n",
       "      <td>65</td>\n",
       "      <td>58</td>\n",
       "      <td>72</td>\n",
       "      <td>55</td>\n",
       "      <td>71</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0</td>\n",
       "      <td>357</td>\n",
       "      <td>192</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>90</td>\n",
       "      <td>108</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>7</td>\n",
       "      <td>115</td>\n",
       "      <td>89</td>\n",
       "      <td>98</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>103</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Type  Distance0  Distance1  Distance2  Distance3  Distance4  Distance5  \\\n",
       "0       0        178        114        179         97         67        130   \n",
       "1       0        179        176        178         33         24         96   \n",
       "2       1         86         86         86         83         68         86   \n",
       "3       1         99        100         99         80         98         93   \n",
       "4       1         70         70         69         64         68         58   \n",
       "..    ...        ...        ...        ...        ...        ...        ...   \n",
       "283     0        357        192          7          7          7         90   \n",
       "284     0          7          7          7          7          7          7   \n",
       "285     0        116          7        115         89         98          7   \n",
       "286     0         51         51         51          8         38         51   \n",
       "287     0         42         42         42          7          7          8   \n",
       "\n",
       "     Distance6  Distance7  Distance8  ...  Distance80  Distance81  Distance82  \\\n",
       "0           73        132         93  ...          92          89          90   \n",
       "1           62        103         75  ...          90          84          90   \n",
       "2           60         85         86  ...          57          76          76   \n",
       "3           64         94         98  ...          87          68         117   \n",
       "4            7         70         69  ...         103         132          74   \n",
       "..         ...        ...        ...  ...         ...         ...         ...   \n",
       "283        108          6          7  ...           7           7           9   \n",
       "284          7          8          8  ...           7           7           7   \n",
       "285          6          7        103  ...           6           7           7   \n",
       "286         51          6          8  ...           8           9           7   \n",
       "287          8          6          8  ...           7           7           9   \n",
       "\n",
       "     Distance83  Distance84  Distance85  Distance86  Distance87  Distance88  \\\n",
       "0            57          54          48          68          89          89   \n",
       "1            89          63          51          88          90          85   \n",
       "2            72          76           7          53          64          76   \n",
       "3            97         120         119          73          80         119   \n",
       "4            72          65          58          72          55          71   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "283           7           7           7           7           7           7   \n",
       "284           7           7           6           7           9           6   \n",
       "285           9           9           7           7           7           8   \n",
       "286           7           7           7           9           9           7   \n",
       "287           7           7           7           7           7           7   \n",
       "\n",
       "     Distance89  \n",
       "0          61.0  \n",
       "1          74.0  \n",
       "2          77.0  \n",
       "3          62.0  \n",
       "4          69.0  \n",
       "..          ...  \n",
       "283         7.0  \n",
       "284         7.0  \n",
       "285         7.0  \n",
       "286         7.0  \n",
       "287         7.0  \n",
       "\n",
       "[288 rows x 91 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 288 entries, 0 to 287\n",
      "Data columns (total 91 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Type        288 non-null    int64  \n",
      " 1   Distance0   288 non-null    int64  \n",
      " 2   Distance1   288 non-null    int64  \n",
      " 3   Distance2   288 non-null    int64  \n",
      " 4   Distance3   288 non-null    int64  \n",
      " 5   Distance4   288 non-null    int64  \n",
      " 6   Distance5   288 non-null    int64  \n",
      " 7   Distance6   288 non-null    int64  \n",
      " 8   Distance7   288 non-null    int64  \n",
      " 9   Distance8   288 non-null    int64  \n",
      " 10  Distance9   288 non-null    int64  \n",
      " 11  Distance10  288 non-null    int64  \n",
      " 12  Distance11  288 non-null    int64  \n",
      " 13  Distance12  288 non-null    int64  \n",
      " 14  Distance13  288 non-null    int64  \n",
      " 15  Distance14  288 non-null    int64  \n",
      " 16  Distance15  288 non-null    int64  \n",
      " 17  Distance16  288 non-null    int64  \n",
      " 18  Distance17  288 non-null    int64  \n",
      " 19  Distance18  288 non-null    int64  \n",
      " 20  Distance19  288 non-null    int64  \n",
      " 21  Distance20  288 non-null    int64  \n",
      " 22  Distance21  288 non-null    int64  \n",
      " 23  Distance22  288 non-null    int64  \n",
      " 24  Distance23  288 non-null    int64  \n",
      " 25  Distance24  288 non-null    int64  \n",
      " 26  Distance25  288 non-null    int64  \n",
      " 27  Distance26  288 non-null    int64  \n",
      " 28  Distance27  288 non-null    int64  \n",
      " 29  Distance28  288 non-null    int64  \n",
      " 30  Distance29  288 non-null    int64  \n",
      " 31  Distance30  288 non-null    int64  \n",
      " 32  Distance31  288 non-null    int64  \n",
      " 33  Distance32  288 non-null    int64  \n",
      " 34  Distance33  288 non-null    int64  \n",
      " 35  Distance34  288 non-null    int64  \n",
      " 36  Distance35  288 non-null    int64  \n",
      " 37  Distance36  288 non-null    int64  \n",
      " 38  Distance37  288 non-null    int64  \n",
      " 39  Distance38  288 non-null    int64  \n",
      " 40  Distance39  288 non-null    int64  \n",
      " 41  Distance40  288 non-null    int64  \n",
      " 42  Distance41  288 non-null    int64  \n",
      " 43  Distance42  288 non-null    int64  \n",
      " 44  Distance43  288 non-null    int64  \n",
      " 45  Distance44  288 non-null    int64  \n",
      " 46  Distance45  288 non-null    int64  \n",
      " 47  Distance46  288 non-null    int64  \n",
      " 48  Distance47  288 non-null    int64  \n",
      " 49  Distance48  288 non-null    int64  \n",
      " 50  Distance49  288 non-null    int64  \n",
      " 51  Distance50  288 non-null    int64  \n",
      " 52  Distance51  288 non-null    int64  \n",
      " 53  Distance52  288 non-null    int64  \n",
      " 54  Distance53  288 non-null    int64  \n",
      " 55  Distance54  288 non-null    int64  \n",
      " 56  Distance55  288 non-null    int64  \n",
      " 57  Distance56  288 non-null    int64  \n",
      " 58  Distance57  288 non-null    int64  \n",
      " 59  Distance58  288 non-null    int64  \n",
      " 60  Distance59  288 non-null    int64  \n",
      " 61  Distance60  288 non-null    int64  \n",
      " 62  Distance61  288 non-null    int64  \n",
      " 63  Distance62  288 non-null    int64  \n",
      " 64  Distance63  288 non-null    int64  \n",
      " 65  Distance64  288 non-null    int64  \n",
      " 66  Distance65  288 non-null    int64  \n",
      " 67  Distance66  288 non-null    int64  \n",
      " 68  Distance67  288 non-null    int64  \n",
      " 69  Distance68  288 non-null    int64  \n",
      " 70  Distance69  288 non-null    int64  \n",
      " 71  Distance70  288 non-null    int64  \n",
      " 72  Distance71  288 non-null    int64  \n",
      " 73  Distance72  288 non-null    int64  \n",
      " 74  Distance73  288 non-null    int64  \n",
      " 75  Distance74  288 non-null    int64  \n",
      " 76  Distance75  288 non-null    int64  \n",
      " 77  Distance76  288 non-null    int64  \n",
      " 78  Distance77  288 non-null    int64  \n",
      " 79  Distance78  288 non-null    int64  \n",
      " 80  Distance79  288 non-null    int64  \n",
      " 81  Distance80  288 non-null    int64  \n",
      " 82  Distance81  288 non-null    int64  \n",
      " 83  Distance82  288 non-null    int64  \n",
      " 84  Distance83  288 non-null    int64  \n",
      " 85  Distance84  288 non-null    int64  \n",
      " 86  Distance85  288 non-null    int64  \n",
      " 87  Distance86  288 non-null    int64  \n",
      " 88  Distance87  288 non-null    int64  \n",
      " 89  Distance88  288 non-null    int64  \n",
      " 90  Distance89  287 non-null    float64\n",
      "dtypes: float64(1), int64(90)\n",
      "memory usage: 204.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Distance0</th>\n",
       "      <th>Distance1</th>\n",
       "      <th>Distance2</th>\n",
       "      <th>Distance3</th>\n",
       "      <th>Distance4</th>\n",
       "      <th>Distance5</th>\n",
       "      <th>Distance6</th>\n",
       "      <th>Distance7</th>\n",
       "      <th>Distance8</th>\n",
       "      <th>...</th>\n",
       "      <th>Distance80</th>\n",
       "      <th>Distance81</th>\n",
       "      <th>Distance82</th>\n",
       "      <th>Distance83</th>\n",
       "      <th>Distance84</th>\n",
       "      <th>Distance85</th>\n",
       "      <th>Distance86</th>\n",
       "      <th>Distance87</th>\n",
       "      <th>Distance88</th>\n",
       "      <th>Distance89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>287.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.524306</td>\n",
       "      <td>109.055556</td>\n",
       "      <td>99.204861</td>\n",
       "      <td>95.829861</td>\n",
       "      <td>48.965278</td>\n",
       "      <td>48.302083</td>\n",
       "      <td>46.833333</td>\n",
       "      <td>39.135417</td>\n",
       "      <td>41.434028</td>\n",
       "      <td>65.809028</td>\n",
       "      <td>...</td>\n",
       "      <td>39.538194</td>\n",
       "      <td>29.284722</td>\n",
       "      <td>35.100694</td>\n",
       "      <td>42.006944</td>\n",
       "      <td>47.836806</td>\n",
       "      <td>39.031250</td>\n",
       "      <td>29.625000</td>\n",
       "      <td>27.548611</td>\n",
       "      <td>47.718750</td>\n",
       "      <td>24.303136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500278</td>\n",
       "      <td>78.840975</td>\n",
       "      <td>66.444625</td>\n",
       "      <td>61.873330</td>\n",
       "      <td>36.720797</td>\n",
       "      <td>40.261632</td>\n",
       "      <td>40.052666</td>\n",
       "      <td>32.076331</td>\n",
       "      <td>45.154120</td>\n",
       "      <td>47.830220</td>\n",
       "      <td>...</td>\n",
       "      <td>44.957029</td>\n",
       "      <td>32.836765</td>\n",
       "      <td>39.742029</td>\n",
       "      <td>42.463839</td>\n",
       "      <td>45.761361</td>\n",
       "      <td>56.471346</td>\n",
       "      <td>31.740317</td>\n",
       "      <td>29.549894</td>\n",
       "      <td>47.941174</td>\n",
       "      <td>28.374779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>58.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>83.500000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>121.250000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>75.250000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>63.250000</td>\n",
       "      <td>96.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>70.250000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>64.250000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>52.250000</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>47.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>217.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>777.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>146.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Type   Distance0   Distance1   Distance2   Distance3   Distance4  \\\n",
       "count  288.000000  288.000000  288.000000  288.000000  288.000000  288.000000   \n",
       "mean     0.524306  109.055556   99.204861   95.829861   48.965278   48.302083   \n",
       "std      0.500278   78.840975   66.444625   61.873330   36.720797   40.261632   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000   60.000000   58.500000   59.000000    7.750000    7.000000   \n",
       "50%      1.000000   93.000000   90.000000   83.500000   54.000000   51.000000   \n",
       "75%      1.000000  137.000000  127.250000  121.250000   82.000000   75.250000   \n",
       "max      1.000000  357.000000  357.000000  335.000000  161.000000  182.000000   \n",
       "\n",
       "        Distance5   Distance6   Distance7   Distance8  ...  Distance80  \\\n",
       "count  288.000000  288.000000  288.000000  288.000000  ...  288.000000   \n",
       "mean    46.833333   39.135417   41.434028   65.809028  ...   39.538194   \n",
       "std     40.052666   32.076331   45.154120   47.830220  ...   44.957029   \n",
       "min      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "25%      7.000000    7.000000    7.000000    9.000000  ...    7.000000   \n",
       "50%     53.500000   44.000000   13.000000   66.500000  ...    8.000000   \n",
       "75%     75.000000   62.000000   63.250000   96.250000  ...   70.250000   \n",
       "max    168.000000  143.000000  296.000000  292.000000  ...  217.000000   \n",
       "\n",
       "       Distance81  Distance82  Distance83  Distance84  Distance85  Distance86  \\\n",
       "count  288.000000  288.000000  288.000000  288.000000  288.000000  288.000000   \n",
       "mean    29.284722   35.100694   42.006944   47.836806   39.031250   29.625000   \n",
       "std     32.836765   39.742029   42.463839   45.761361   56.471346   31.740317   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      7.000000    7.000000    8.000000    8.000000    7.000000    7.000000   \n",
       "50%      8.000000    8.000000    9.000000   21.000000    8.000000    8.000000   \n",
       "75%     51.000000   64.250000   76.000000   84.000000   68.000000   55.000000   \n",
       "max    133.000000  243.000000  213.000000  197.000000  777.000000  175.000000   \n",
       "\n",
       "       Distance87  Distance88  Distance89  \n",
       "count  288.000000  288.000000  287.000000  \n",
       "mean    27.548611   47.718750   24.303136  \n",
       "std     29.549894   47.941174   28.374779  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      7.000000    7.000000    7.000000  \n",
       "50%      8.000000    9.000000    8.000000  \n",
       "75%     52.250000   80.500000   47.500000  \n",
       "max    137.000000  251.000000  146.000000  \n",
       "\n",
       "[8 rows x 91 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used the describe() method to acquire the mean, standard deviation of numerical values\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Distance0</th>\n",
       "      <th>Distance1</th>\n",
       "      <th>Distance2</th>\n",
       "      <th>Distance3</th>\n",
       "      <th>Distance4</th>\n",
       "      <th>Distance5</th>\n",
       "      <th>Distance6</th>\n",
       "      <th>Distance7</th>\n",
       "      <th>Distance8</th>\n",
       "      <th>...</th>\n",
       "      <th>Distance80</th>\n",
       "      <th>Distance81</th>\n",
       "      <th>Distance82</th>\n",
       "      <th>Distance83</th>\n",
       "      <th>Distance84</th>\n",
       "      <th>Distance85</th>\n",
       "      <th>Distance86</th>\n",
       "      <th>Distance87</th>\n",
       "      <th>Distance88</th>\n",
       "      <th>Distance89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "      <td>118</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "      <td>52</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>91</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>124</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>91</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>57</td>\n",
       "      <td>68</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>175</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>78</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>68</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>138</td>\n",
       "      <td>137</td>\n",
       "      <td>88</td>\n",
       "      <td>65</td>\n",
       "      <td>136</td>\n",
       "      <td>106</td>\n",
       "      <td>142</td>\n",
       "      <td>138</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>44</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>72</td>\n",
       "      <td>7</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>77</td>\n",
       "      <td>164</td>\n",
       "      <td>165</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>55</td>\n",
       "      <td>66</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>98</td>\n",
       "      <td>8</td>\n",
       "      <td>106</td>\n",
       "      <td>88</td>\n",
       "      <td>56</td>\n",
       "      <td>106</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>102</td>\n",
       "      <td>243</td>\n",
       "      <td>7</td>\n",
       "      <td>91</td>\n",
       "      <td>51</td>\n",
       "      <td>59</td>\n",
       "      <td>57</td>\n",
       "      <td>251</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>89</td>\n",
       "      <td>58</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>69</td>\n",
       "      <td>64</td>\n",
       "      <td>68</td>\n",
       "      <td>58</td>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>103</td>\n",
       "      <td>132</td>\n",
       "      <td>74</td>\n",
       "      <td>72</td>\n",
       "      <td>65</td>\n",
       "      <td>58</td>\n",
       "      <td>72</td>\n",
       "      <td>55</td>\n",
       "      <td>71</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>66</td>\n",
       "      <td>64</td>\n",
       "      <td>67</td>\n",
       "      <td>57</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>113</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>110</td>\n",
       "      <td>113</td>\n",
       "      <td>80</td>\n",
       "      <td>57</td>\n",
       "      <td>8</td>\n",
       "      <td>114</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Type  Distance0  Distance1  Distance2  Distance3  Distance4  Distance5  \\\n",
       "0       0         11        319        319        118          9          7   \n",
       "1       0         84         82         82         82          8          6   \n",
       "2       0        137        137         91          7          8         75   \n",
       "3       1         79         79         79         78          8          7   \n",
       "4       1        137        138        137         88         65        136   \n",
       "..    ...        ...        ...        ...        ...        ...        ...   \n",
       "283     1         73         72         72         72          8          8   \n",
       "284     1        105        105         98          8        106         88   \n",
       "285     1         89         89         89          7          6          7   \n",
       "286     1         70         70         69         64         68         58   \n",
       "287     0         67         67         67         66         64         67   \n",
       "\n",
       "     Distance6  Distance7  Distance8  ...  Distance80  Distance81  Distance82  \\\n",
       "0           70         52          8  ...          92           7           6   \n",
       "1           60          7          8  ...           7           7           9   \n",
       "2           57         68          8  ...           7           8           9   \n",
       "3            7         49         77  ...          67          63           7   \n",
       "4          106        142        138  ...          45          44          43   \n",
       "..         ...        ...        ...  ...         ...         ...         ...   \n",
       "283         72          7         72  ...           8           7          77   \n",
       "284         56        106         94  ...          76         102         243   \n",
       "285         89         58         90  ...           7           7           7   \n",
       "286          7         70         69  ...         103         132          74   \n",
       "287         57         67         67  ...         113          51           8   \n",
       "\n",
       "     Distance83  Distance84  Distance85  Distance86  Distance87  Distance88  \\\n",
       "0             8           8           7           9           6           6   \n",
       "1             9           7          91           7           7         124   \n",
       "2             7           7           9         175           6           7   \n",
       "3            68          68          68           7           7          68   \n",
       "4            44          44          44           7          44           7   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "283         164         165           8           6          55          66   \n",
       "284           7          91          51          59          57         251   \n",
       "285           8           7           8          51           8          51   \n",
       "286          72          65          58          72          55          71   \n",
       "287         110         113          80          57           8         114   \n",
       "\n",
       "     Distance89  \n",
       "0         129.0  \n",
       "1           6.0  \n",
       "2           6.0  \n",
       "3          69.0  \n",
       "4          44.0  \n",
       "..          ...  \n",
       "283        60.0  \n",
       "284         7.0  \n",
       "285         6.0  \n",
       "286        69.0  \n",
       "287         7.0  \n",
       "\n",
       "[288 rows x 91 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next shuffle the dataframe so that train_test_split could be done correctly\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.fillna(df.mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 288 entries, 0 to 287\n",
      "Data columns (total 91 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Type        288 non-null    int64  \n",
      " 1   Distance0   288 non-null    int64  \n",
      " 2   Distance1   288 non-null    int64  \n",
      " 3   Distance2   288 non-null    int64  \n",
      " 4   Distance3   288 non-null    int64  \n",
      " 5   Distance4   288 non-null    int64  \n",
      " 6   Distance5   288 non-null    int64  \n",
      " 7   Distance6   288 non-null    int64  \n",
      " 8   Distance7   288 non-null    int64  \n",
      " 9   Distance8   288 non-null    int64  \n",
      " 10  Distance9   288 non-null    int64  \n",
      " 11  Distance10  288 non-null    int64  \n",
      " 12  Distance11  288 non-null    int64  \n",
      " 13  Distance12  288 non-null    int64  \n",
      " 14  Distance13  288 non-null    int64  \n",
      " 15  Distance14  288 non-null    int64  \n",
      " 16  Distance15  288 non-null    int64  \n",
      " 17  Distance16  288 non-null    int64  \n",
      " 18  Distance17  288 non-null    int64  \n",
      " 19  Distance18  288 non-null    int64  \n",
      " 20  Distance19  288 non-null    int64  \n",
      " 21  Distance20  288 non-null    int64  \n",
      " 22  Distance21  288 non-null    int64  \n",
      " 23  Distance22  288 non-null    int64  \n",
      " 24  Distance23  288 non-null    int64  \n",
      " 25  Distance24  288 non-null    int64  \n",
      " 26  Distance25  288 non-null    int64  \n",
      " 27  Distance26  288 non-null    int64  \n",
      " 28  Distance27  288 non-null    int64  \n",
      " 29  Distance28  288 non-null    int64  \n",
      " 30  Distance29  288 non-null    int64  \n",
      " 31  Distance30  288 non-null    int64  \n",
      " 32  Distance31  288 non-null    int64  \n",
      " 33  Distance32  288 non-null    int64  \n",
      " 34  Distance33  288 non-null    int64  \n",
      " 35  Distance34  288 non-null    int64  \n",
      " 36  Distance35  288 non-null    int64  \n",
      " 37  Distance36  288 non-null    int64  \n",
      " 38  Distance37  288 non-null    int64  \n",
      " 39  Distance38  288 non-null    int64  \n",
      " 40  Distance39  288 non-null    int64  \n",
      " 41  Distance40  288 non-null    int64  \n",
      " 42  Distance41  288 non-null    int64  \n",
      " 43  Distance42  288 non-null    int64  \n",
      " 44  Distance43  288 non-null    int64  \n",
      " 45  Distance44  288 non-null    int64  \n",
      " 46  Distance45  288 non-null    int64  \n",
      " 47  Distance46  288 non-null    int64  \n",
      " 48  Distance47  288 non-null    int64  \n",
      " 49  Distance48  288 non-null    int64  \n",
      " 50  Distance49  288 non-null    int64  \n",
      " 51  Distance50  288 non-null    int64  \n",
      " 52  Distance51  288 non-null    int64  \n",
      " 53  Distance52  288 non-null    int64  \n",
      " 54  Distance53  288 non-null    int64  \n",
      " 55  Distance54  288 non-null    int64  \n",
      " 56  Distance55  288 non-null    int64  \n",
      " 57  Distance56  288 non-null    int64  \n",
      " 58  Distance57  288 non-null    int64  \n",
      " 59  Distance58  288 non-null    int64  \n",
      " 60  Distance59  288 non-null    int64  \n",
      " 61  Distance60  288 non-null    int64  \n",
      " 62  Distance61  288 non-null    int64  \n",
      " 63  Distance62  288 non-null    int64  \n",
      " 64  Distance63  288 non-null    int64  \n",
      " 65  Distance64  288 non-null    int64  \n",
      " 66  Distance65  288 non-null    int64  \n",
      " 67  Distance66  288 non-null    int64  \n",
      " 68  Distance67  288 non-null    int64  \n",
      " 69  Distance68  288 non-null    int64  \n",
      " 70  Distance69  288 non-null    int64  \n",
      " 71  Distance70  288 non-null    int64  \n",
      " 72  Distance71  288 non-null    int64  \n",
      " 73  Distance72  288 non-null    int64  \n",
      " 74  Distance73  288 non-null    int64  \n",
      " 75  Distance74  288 non-null    int64  \n",
      " 76  Distance75  288 non-null    int64  \n",
      " 77  Distance76  288 non-null    int64  \n",
      " 78  Distance77  288 non-null    int64  \n",
      " 79  Distance78  288 non-null    int64  \n",
      " 80  Distance79  288 non-null    int64  \n",
      " 81  Distance80  288 non-null    int64  \n",
      " 82  Distance81  288 non-null    int64  \n",
      " 83  Distance82  288 non-null    int64  \n",
      " 84  Distance83  288 non-null    int64  \n",
      " 85  Distance84  288 non-null    int64  \n",
      " 86  Distance85  288 non-null    int64  \n",
      " 87  Distance86  288 non-null    int64  \n",
      " 88  Distance87  288 non-null    int64  \n",
      " 89  Distance88  288 non-null    int64  \n",
      " 90  Distance89  288 non-null    float64\n",
      "dtypes: float64(1), int64(90)\n",
      "memory usage: 204.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split from sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('Type', axis = 1).values\n",
    "y = df['Type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11., 319., 319., ...,   6.,   6., 129.],\n",
       "       [ 84.,  82.,  82., ...,   7., 124.,   6.],\n",
       "       [137., 137.,  91., ...,   6.,   7.,   6.],\n",
       "       ...,\n",
       "       [ 89.,  89.,  89., ...,   8.,  51.,   6.],\n",
       "       [ 70.,  70.,  69., ...,  55.,  71.,  69.],\n",
       "       [ 67.,  67.,  67., ...,   8., 114.,   7.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train =  (216, 90)\n",
      "Shape of y_train =  (216,)\n",
      "Shape of X_test =  (72, 90)\n",
      "Shape of y_test =  (72,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train = \", X_train.shape)\n",
    "print(\"Shape of y_train = \", y_train.shape)\n",
    "print(\"Shape of X_test = \", X_test.shape)\n",
    "print(\"Shape of y_test = \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[237., 115., 160., ...,   7.,  45.,  59.],\n",
       "       [126., 155., 107., ...,   7.,   6.,   7.],\n",
       "       [139., 141., 116., ...,   6.,   8.,  38.],\n",
       "       ...,\n",
       "       [135., 122., 136., ...,   7.,   8.,   8.],\n",
       "       [104., 105., 105., ...,   7.,  79.,   7.],\n",
       "       [202.,  95.,  95., ...,   7.,   8.,  66.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "# Import MinMaxScaler from sklearn.preprocessing to scale the data between 0 and 1.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.6022409 , 0.73134328, ..., 0.05109489, 0.02788845,\n",
       "        0.04794521],\n",
       "       [0.17086835, 0.17086835, 0.18208955, ..., 0.04379562, 0.44621514,\n",
       "        0.04109589],\n",
       "       [0.3557423 , 0.3557423 , 0.26567164, ..., 0.59854015, 0.49003984,\n",
       "        0.04794521],\n",
       "       ...,\n",
       "       [0.50140056, 0.49859944, 0.25074627, ..., 0.05109489, 0.02788845,\n",
       "        0.05479452],\n",
       "       [0.67507003, 0.68067227, 0.71940299, ..., 0.34306569, 0.02788845,\n",
       "        0.04794521],\n",
       "       [0.03081232, 0.02240896, 0.0238806 , ..., 0.05109489, 0.03187251,\n",
       "        0.05479452]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66386555, 0.32212885, 0.47761194, ..., 0.05109489, 0.17928287,\n",
       "        0.40410959],\n",
       "       [0.35294118, 0.43417367, 0.31940299, ..., 0.05109489, 0.02390438,\n",
       "        0.04794521],\n",
       "       [0.38935574, 0.39495798, 0.34626866, ..., 0.04379562, 0.03187251,\n",
       "        0.26027397],\n",
       "       ...,\n",
       "       [0.37815126, 0.34173669, 0.40597015, ..., 0.05109489, 0.03187251,\n",
       "        0.05479452],\n",
       "       [0.29131653, 0.29411765, 0.31343284, ..., 0.05109489, 0.31474104,\n",
       "        0.04794521],\n",
       "       [0.56582633, 0.26610644, 0.28358209, ..., 0.05109489, 0.03187251,\n",
       "        0.45205479]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First used the most basic approach of Logistic Regression to train our model and got 86% accuracy on test data.\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.83      0.88        42\n",
      "           1       0.79      0.90      0.84        30\n",
      "\n",
      "    accuracy                           0.86        72\n",
      "   macro avg       0.86      0.87      0.86        72\n",
      "weighted avg       0.87      0.86      0.86        72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35,  7],\n",
       "       [ 3, 27]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 90)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a sequential model and then added in a first layer of 30 neurons.\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30, activation = 'relu'))\n",
    "\n",
    "# And then added a second layer with 15 neurons\n",
    "model.add(Dense(15, activation = 'relu'))\n",
    "\n",
    "# BINARY CLASSIFICATION\n",
    "# And then we added the last layer with one neuron and sigmoid activation function to output the value b/w 0 and 1\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.7253WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.7071 - val_loss: 0.6900\n",
      "Epoch 2/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6629 - val_loss: 0.6798\n",
      "Epoch 3/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6281 - val_loss: 0.6723\n",
      "Epoch 4/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6033 - val_loss: 0.6665\n",
      "Epoch 5/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5827 - val_loss: 0.6533\n",
      "Epoch 6/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5625 - val_loss: 0.6242\n",
      "Epoch 7/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5388 - val_loss: 0.5821\n",
      "Epoch 8/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5107 - val_loss: 0.5345\n",
      "Epoch 9/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4842 - val_loss: 0.4924\n",
      "Epoch 10/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4519 - val_loss: 0.4567\n",
      "Epoch 11/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4201 - val_loss: 0.4156\n",
      "Epoch 12/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3882 - val_loss: 0.3764\n",
      "Epoch 13/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3629 - val_loss: 0.3476\n",
      "Epoch 14/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3354 - val_loss: 0.3183\n",
      "Epoch 15/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3132 - val_loss: 0.2969\n",
      "Epoch 16/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2954 - val_loss: 0.2783\n",
      "Epoch 17/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2799 - val_loss: 0.2699\n",
      "Epoch 18/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2613 - val_loss: 0.2551\n",
      "Epoch 19/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2504 - val_loss: 0.2512\n",
      "Epoch 20/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2375 - val_loss: 0.2450\n",
      "Epoch 21/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2264 - val_loss: 0.2385\n",
      "Epoch 22/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2202 - val_loss: 0.2338\n",
      "Epoch 23/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2098 - val_loss: 0.2307\n",
      "Epoch 24/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1984 - val_loss: 0.2269\n",
      "Epoch 25/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1911 - val_loss: 0.2253\n",
      "Epoch 26/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1849 - val_loss: 0.2232\n",
      "Epoch 27/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1762 - val_loss: 0.2242\n",
      "Epoch 28/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1715 - val_loss: 0.2263\n",
      "Epoch 29/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1634 - val_loss: 0.2225\n",
      "Epoch 30/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1582 - val_loss: 0.2218\n",
      "Epoch 31/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1501 - val_loss: 0.2215\n",
      "Epoch 32/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1443 - val_loss: 0.2213\n",
      "Epoch 33/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1384 - val_loss: 0.2223\n",
      "Epoch 34/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1341 - val_loss: 0.2239\n",
      "Epoch 35/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1286 - val_loss: 0.2221\n",
      "Epoch 36/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1233 - val_loss: 0.2229\n",
      "Epoch 37/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1215 - val_loss: 0.2273\n",
      "Epoch 38/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1154 - val_loss: 0.2243\n",
      "Epoch 39/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1116 - val_loss: 0.2248\n",
      "Epoch 40/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1076 - val_loss: 0.2260\n",
      "Epoch 41/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1040 - val_loss: 0.2292\n",
      "Epoch 42/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0980 - val_loss: 0.2251\n",
      "Epoch 43/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0958 - val_loss: 0.2254\n",
      "Epoch 44/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0915 - val_loss: 0.2309\n",
      "Epoch 45/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0872 - val_loss: 0.2294\n",
      "Epoch 46/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0846 - val_loss: 0.2264\n",
      "Epoch 47/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0811 - val_loss: 0.2305\n",
      "Epoch 48/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0799 - val_loss: 0.2317\n",
      "Epoch 49/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0767 - val_loss: 0.2368\n",
      "Epoch 50/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0726 - val_loss: 0.2315\n",
      "Epoch 51/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0708 - val_loss: 0.2302\n",
      "Epoch 52/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0675 - val_loss: 0.2338\n",
      "Epoch 53/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0655 - val_loss: 0.2390\n",
      "Epoch 54/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0634 - val_loss: 0.2362\n",
      "Epoch 55/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0607 - val_loss: 0.2372\n",
      "Epoch 56/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0586 - val_loss: 0.2387\n",
      "Epoch 57/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0563 - val_loss: 0.2401\n",
      "Epoch 58/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0551 - val_loss: 0.2421\n",
      "Epoch 59/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0524 - val_loss: 0.2431\n",
      "Epoch 60/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0509 - val_loss: 0.2444\n",
      "Epoch 61/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0490 - val_loss: 0.2452\n",
      "Epoch 62/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0473 - val_loss: 0.2499\n",
      "Epoch 63/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0454 - val_loss: 0.2494\n",
      "Epoch 64/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0434 - val_loss: 0.2505\n",
      "Epoch 65/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0428 - val_loss: 0.2517\n",
      "Epoch 66/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0405 - val_loss: 0.2537\n",
      "Epoch 67/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0389 - val_loss: 0.2568\n",
      "Epoch 68/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 0.2575\n",
      "Epoch 69/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 0.2607\n",
      "Epoch 70/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 0.2633\n",
      "Epoch 71/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 0.2629\n",
      "Epoch 72/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.2625\n",
      "Epoch 73/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.2647\n",
      "Epoch 74/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.2711\n",
      "Epoch 75/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.2730\n",
      "Epoch 76/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.2734\n",
      "Epoch 77/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.2757\n",
      "Epoch 78/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.2802\n",
      "Epoch 79/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.2813\n",
      "Epoch 80/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.2829\n",
      "Epoch 81/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.2886\n",
      "Epoch 82/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.2902\n",
      "Epoch 83/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.2917\n",
      "Epoch 84/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.2915\n",
      "Epoch 85/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.2939\n",
      "Epoch 86/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0180 - val_loss: 0.2977\n",
      "Epoch 87/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.3018\n",
      "Epoch 88/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.3028\n",
      "Epoch 89/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.3052\n",
      "Epoch 90/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.3050\n",
      "Epoch 91/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.3110\n",
      "Epoch 92/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.3123\n",
      "Epoch 93/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.3133\n",
      "Epoch 94/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.3158\n",
      "Epoch 95/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0128 - val_loss: 0.3188\n",
      "Epoch 96/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0125 - val_loss: 0.3212\n",
      "Epoch 97/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.3231\n",
      "Epoch 98/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.3248\n",
      "Epoch 99/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.3281\n",
      "Epoch 100/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.3319\n",
      "Epoch 101/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.3347\n",
      "Epoch 102/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.3357\n",
      "Epoch 103/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 0.3361\n",
      "Epoch 104/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0095 - val_loss: 0.3399\n",
      "Epoch 105/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 0.3436\n",
      "Epoch 106/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.3435\n",
      "Epoch 107/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.3458\n",
      "Epoch 108/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.3483\n",
      "Epoch 109/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.3543\n",
      "Epoch 110/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 0.3570\n",
      "Epoch 111/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.3595\n",
      "Epoch 112/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.3584\n",
      "Epoch 113/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.3597\n",
      "Epoch 114/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.3616\n",
      "Epoch 115/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.3645\n",
      "Epoch 116/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.3661\n",
      "Epoch 117/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.3685\n",
      "Epoch 118/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.3693\n",
      "Epoch 119/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 0.3731\n",
      "Epoch 120/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.3748\n",
      "Epoch 121/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.3757\n",
      "Epoch 122/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.3781\n",
      "Epoch 123/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.3815\n",
      "Epoch 124/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.3823\n",
      "Epoch 125/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.3827\n",
      "Epoch 126/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 0.3850\n",
      "Epoch 127/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.3868\n",
      "Epoch 128/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.3903\n",
      "Epoch 129/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.3923\n",
      "Epoch 130/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.3923\n",
      "Epoch 131/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.3942\n",
      "Epoch 132/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.3977\n",
      "Epoch 133/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.4005\n",
      "Epoch 134/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.4005\n",
      "Epoch 135/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.4014\n",
      "Epoch 136/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.4037\n",
      "Epoch 137/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.4051\n",
      "Epoch 138/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.4086\n",
      "Epoch 139/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.4100\n",
      "Epoch 140/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.4113\n",
      "Epoch 141/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.4135\n",
      "Epoch 142/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.4143\n",
      "Epoch 143/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.4172\n",
      "Epoch 144/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.4194\n",
      "Epoch 145/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.4204\n",
      "Epoch 146/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.4209\n",
      "Epoch 147/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.4228\n",
      "Epoch 148/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.4248\n",
      "Epoch 149/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.4263\n",
      "Epoch 150/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.4287\n",
      "Epoch 151/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.4308\n",
      "Epoch 152/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.4307\n",
      "Epoch 153/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.4324\n",
      "Epoch 154/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.4345\n",
      "Epoch 155/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.4363\n",
      "Epoch 156/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.4377\n",
      "Epoch 157/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.4384\n",
      "Epoch 158/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.4400\n",
      "Epoch 159/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.4411\n",
      "Epoch 160/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.4426\n",
      "Epoch 161/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.4450\n",
      "Epoch 162/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.4470\n",
      "Epoch 163/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.4484\n",
      "Epoch 164/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.4498\n",
      "Epoch 165/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.4511\n",
      "Epoch 166/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.4518\n",
      "Epoch 167/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.4535\n",
      "Epoch 168/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.4540\n",
      "Epoch 169/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.4548\n",
      "Epoch 170/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.4570\n",
      "Epoch 171/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.4582\n",
      "Epoch 172/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.4596\n",
      "Epoch 173/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.4618\n",
      "Epoch 174/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.4628\n",
      "Epoch 175/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.4643\n",
      "Epoch 176/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.4649\n",
      "Epoch 177/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.4669\n",
      "Epoch 178/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.4686\n",
      "Epoch 179/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.4693\n",
      "Epoch 180/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.4706\n",
      "Epoch 181/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.4717\n",
      "Epoch 182/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.4722\n",
      "Epoch 183/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.4741\n",
      "Epoch 184/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.4760\n",
      "Epoch 185/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.4770\n",
      "Epoch 186/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.4774\n",
      "Epoch 187/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.4788\n",
      "Epoch 188/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.4804\n",
      "Epoch 189/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.4821\n",
      "Epoch 190/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.4832\n",
      "Epoch 191/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.4841\n",
      "Epoch 192/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.4847\n",
      "Epoch 193/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.4861\n",
      "Epoch 194/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.4883\n",
      "Epoch 195/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.4891\n",
      "Epoch 196/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.4895\n",
      "Epoch 197/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.4912\n",
      "Epoch 198/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.4923\n",
      "Epoch 199/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.4934\n",
      "Epoch 200/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.4952\n",
      "Epoch 201/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.4955\n",
      "Epoch 202/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.4963\n",
      "Epoch 203/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.4978\n",
      "Epoch 204/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.4988\n",
      "Epoch 205/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.5008\n",
      "Epoch 206/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.5017\n",
      "Epoch 207/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.5020\n",
      "Epoch 208/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.5041\n",
      "Epoch 209/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.5055\n",
      "Epoch 210/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.5065\n",
      "Epoch 211/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.5072\n",
      "Epoch 212/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.5070\n",
      "Epoch 213/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.5081\n",
      "Epoch 214/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.5109\n",
      "Epoch 215/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.5123\n",
      "Epoch 216/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.5124\n",
      "Epoch 217/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.5131\n",
      "Epoch 218/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.5139\n",
      "Epoch 219/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.5158\n",
      "Epoch 220/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.5167\n",
      "Epoch 221/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.5174\n",
      "Epoch 222/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.5184\n",
      "Epoch 223/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.5196\n",
      "Epoch 224/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.5206\n",
      "Epoch 225/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.9836e-04 - val_loss: 0.5221\n",
      "Epoch 226/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.8366e-04 - val_loss: 0.5227\n",
      "Epoch 227/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.7936e-04 - val_loss: 0.5239\n",
      "Epoch 228/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.6104e-04 - val_loss: 0.5247\n",
      "Epoch 229/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.5602e-04 - val_loss: 0.5255\n",
      "Epoch 230/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.4215e-04 - val_loss: 0.5268\n",
      "Epoch 231/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.3490e-04 - val_loss: 0.5285\n",
      "Epoch 232/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.1770e-04 - val_loss: 0.5295\n",
      "Epoch 233/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.0722e-04 - val_loss: 0.5297\n",
      "Epoch 234/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.0564e-04 - val_loss: 0.5307\n",
      "Epoch 235/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.8439e-04 - val_loss: 0.5321\n",
      "Epoch 236/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.7791e-04 - val_loss: 0.5332\n",
      "Epoch 237/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.6646e-04 - val_loss: 0.5342\n",
      "Epoch 238/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.5218e-04 - val_loss: 0.5345\n",
      "Epoch 239/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.4193e-04 - val_loss: 0.5359\n",
      "Epoch 240/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.3397e-04 - val_loss: 0.5373\n",
      "Epoch 241/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.2839e-04 - val_loss: 0.5384\n",
      "Epoch 242/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.1482e-04 - val_loss: 0.5392\n",
      "Epoch 243/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.0809e-04 - val_loss: 0.5397\n",
      "Epoch 244/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.9975e-04 - val_loss: 0.5412\n",
      "Epoch 245/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.9387e-04 - val_loss: 0.5413\n",
      "Epoch 246/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.8116e-04 - val_loss: 0.5423\n",
      "Epoch 247/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 7.7072e-04 - val_loss: 0.5433\n",
      "Epoch 248/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.6295e-04 - val_loss: 0.5440\n",
      "Epoch 249/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.5458e-04 - val_loss: 0.5450\n",
      "Epoch 250/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.4769e-04 - val_loss: 0.5461\n",
      "Epoch 251/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.4015e-04 - val_loss: 0.5471\n",
      "Epoch 252/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.3025e-04 - val_loss: 0.5478\n",
      "Epoch 253/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.2450e-04 - val_loss: 0.5485\n",
      "Epoch 254/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.1384e-04 - val_loss: 0.5497\n",
      "Epoch 255/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.1304e-04 - val_loss: 0.5502\n",
      "Epoch 256/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.0157e-04 - val_loss: 0.5512\n",
      "Epoch 257/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.9780e-04 - val_loss: 0.5518\n",
      "Epoch 258/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.8769e-04 - val_loss: 0.5530\n",
      "Epoch 259/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.7939e-04 - val_loss: 0.5538\n",
      "Epoch 260/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.7353e-04 - val_loss: 0.5545\n",
      "Epoch 261/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.6653e-04 - val_loss: 0.5555\n",
      "Epoch 262/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.5920e-04 - val_loss: 0.5563\n",
      "Epoch 263/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.4824e-04 - val_loss: 0.5571\n",
      "Epoch 264/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.4560e-04 - val_loss: 0.5586\n",
      "Epoch 265/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.3617e-04 - val_loss: 0.5593\n",
      "Epoch 266/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.2982e-04 - val_loss: 0.5596\n",
      "Epoch 267/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.2476e-04 - val_loss: 0.5608\n",
      "Epoch 268/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.1767e-04 - val_loss: 0.5621\n",
      "Epoch 269/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.1052e-04 - val_loss: 0.5624\n",
      "Epoch 270/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.0641e-04 - val_loss: 0.5631\n",
      "Epoch 271/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.9779e-04 - val_loss: 0.5639\n",
      "Epoch 272/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.9371e-04 - val_loss: 0.5652\n",
      "Epoch 273/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.8911e-04 - val_loss: 0.5658\n",
      "Epoch 274/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.8237e-04 - val_loss: 0.5666\n",
      "Epoch 275/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.7927e-04 - val_loss: 0.5675\n",
      "Epoch 276/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.7469e-04 - val_loss: 0.5678\n",
      "Epoch 277/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.6536e-04 - val_loss: 0.5691\n",
      "Epoch 278/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.5759e-04 - val_loss: 0.5702\n",
      "Epoch 279/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.5349e-04 - val_loss: 0.5712\n",
      "Epoch 280/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.4877e-04 - val_loss: 0.5715\n",
      "Epoch 281/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.4132e-04 - val_loss: 0.5722\n",
      "Epoch 282/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.4130e-04 - val_loss: 0.5732\n",
      "Epoch 283/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.3294e-04 - val_loss: 0.5742\n",
      "Epoch 284/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.2736e-04 - val_loss: 0.5755\n",
      "Epoch 285/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.2389e-04 - val_loss: 0.5763\n",
      "Epoch 286/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.1841e-04 - val_loss: 0.5771\n",
      "Epoch 287/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.1234e-04 - val_loss: 0.5774\n",
      "Epoch 288/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.0698e-04 - val_loss: 0.5781\n",
      "Epoch 289/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.0450e-04 - val_loss: 0.5791\n",
      "Epoch 290/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.9912e-04 - val_loss: 0.5797\n",
      "Epoch 291/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.9345e-04 - val_loss: 0.5801\n",
      "Epoch 292/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.9183e-04 - val_loss: 0.5819\n",
      "Epoch 293/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.8732e-04 - val_loss: 0.5824\n",
      "Epoch 294/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.8237e-04 - val_loss: 0.5830\n",
      "Epoch 295/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.7626e-04 - val_loss: 0.5833\n",
      "Epoch 296/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.7173e-04 - val_loss: 0.5848\n",
      "Epoch 297/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.6627e-04 - val_loss: 0.5855\n",
      "Epoch 298/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.6316e-04 - val_loss: 0.5863\n",
      "Epoch 299/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.6321e-04 - val_loss: 0.5875\n",
      "Epoch 300/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.5895e-04 - val_loss: 0.5887\n",
      "Epoch 301/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.5397e-04 - val_loss: 0.5889\n",
      "Epoch 302/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.4814e-04 - val_loss: 0.5899\n",
      "Epoch 303/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.4413e-04 - val_loss: 0.5905\n",
      "Epoch 304/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.4016e-04 - val_loss: 0.5910\n",
      "Epoch 305/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.3603e-04 - val_loss: 0.5919\n",
      "Epoch 306/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.3026e-04 - val_loss: 0.5928\n",
      "Epoch 307/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.2865e-04 - val_loss: 0.5935\n",
      "Epoch 308/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.2350e-04 - val_loss: 0.5938\n",
      "Epoch 309/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.1954e-04 - val_loss: 0.5948\n",
      "Epoch 310/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.1599e-04 - val_loss: 0.5963\n",
      "Epoch 311/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.1240e-04 - val_loss: 0.5969\n",
      "Epoch 312/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.0911e-04 - val_loss: 0.5966\n",
      "Epoch 313/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.0483e-04 - val_loss: 0.5971\n",
      "Epoch 314/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.0412e-04 - val_loss: 0.5985\n",
      "Epoch 315/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.9850e-04 - val_loss: 0.5994\n",
      "Epoch 316/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.9492e-04 - val_loss: 0.5996\n",
      "Epoch 317/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.9238e-04 - val_loss: 0.6002\n",
      "Epoch 318/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.8946e-04 - val_loss: 0.6016\n",
      "Epoch 319/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.8429e-04 - val_loss: 0.6023\n",
      "Epoch 320/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.8077e-04 - val_loss: 0.6029\n",
      "Epoch 321/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.7880e-04 - val_loss: 0.6029\n",
      "Epoch 322/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.7577e-04 - val_loss: 0.6040\n",
      "Epoch 323/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.7155e-04 - val_loss: 0.6055\n",
      "Epoch 324/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.6991e-04 - val_loss: 0.6062\n",
      "Epoch 325/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.6604e-04 - val_loss: 0.6067\n",
      "Epoch 326/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 3.6475e-04 - val_loss: 0.6064\n",
      "Epoch 327/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.6296e-04 - val_loss: 0.6078\n",
      "Epoch 328/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5639e-04 - val_loss: 0.6086\n",
      "Epoch 329/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5406e-04 - val_loss: 0.6095\n",
      "Epoch 330/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5186e-04 - val_loss: 0.6096\n",
      "Epoch 331/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5132e-04 - val_loss: 0.6111\n",
      "Epoch 332/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4466e-04 - val_loss: 0.6123\n",
      "Epoch 333/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4246e-04 - val_loss: 0.6123\n",
      "Epoch 334/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3988e-04 - val_loss: 0.6122\n",
      "Epoch 335/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3595e-04 - val_loss: 0.6133\n",
      "Epoch 336/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3350e-04 - val_loss: 0.6140\n",
      "Epoch 337/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3213e-04 - val_loss: 0.6149\n",
      "Epoch 338/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.2971e-04 - val_loss: 0.6151\n",
      "Epoch 339/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.2653e-04 - val_loss: 0.6157\n",
      "Epoch 340/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.2300e-04 - val_loss: 0.6163\n",
      "Epoch 341/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.2056e-04 - val_loss: 0.6173\n",
      "Epoch 342/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.1872e-04 - val_loss: 0.6185\n",
      "Epoch 343/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.1534e-04 - val_loss: 0.6187\n",
      "Epoch 344/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.1386e-04 - val_loss: 0.6191\n",
      "Epoch 345/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.1064e-04 - val_loss: 0.6198\n",
      "Epoch 346/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.0780e-04 - val_loss: 0.6203\n",
      "Epoch 347/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.0571e-04 - val_loss: 0.6211\n",
      "Epoch 348/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.0261e-04 - val_loss: 0.6221\n",
      "Epoch 349/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.0436e-04 - val_loss: 0.6225\n",
      "Epoch 350/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.9937e-04 - val_loss: 0.6239\n",
      "Epoch 351/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.9613e-04 - val_loss: 0.6245\n",
      "Epoch 352/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.9393e-04 - val_loss: 0.6252\n",
      "Epoch 353/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.9076e-04 - val_loss: 0.6255\n",
      "Epoch 354/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.8918e-04 - val_loss: 0.6261\n",
      "Epoch 355/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.8797e-04 - val_loss: 0.6271\n",
      "Epoch 356/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.8444e-04 - val_loss: 0.6273\n",
      "Epoch 357/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.8189e-04 - val_loss: 0.6276\n",
      "Epoch 358/600\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.7994e-04 - val_loss: 0.6284\n",
      "Epoch 359/600\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.7887e-04 - val_loss: 0.6286\n",
      "Epoch 360/600\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.7638e-04 - val_loss: 0.6302\n",
      "Epoch 361/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.7342e-04 - val_loss: 0.6307\n",
      "Epoch 362/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.7154e-04 - val_loss: 0.6313\n",
      "Epoch 363/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.6936e-04 - val_loss: 0.6320\n",
      "Epoch 364/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.6726e-04 - val_loss: 0.6322\n",
      "Epoch 365/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.6486e-04 - val_loss: 0.6329\n",
      "Epoch 366/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.6313e-04 - val_loss: 0.6338\n",
      "Epoch 367/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.6073e-04 - val_loss: 0.6345\n",
      "Epoch 368/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.5930e-04 - val_loss: 0.6347\n",
      "Epoch 369/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.5770e-04 - val_loss: 0.6352\n",
      "Epoch 370/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.5583e-04 - val_loss: 0.6361\n",
      "Epoch 371/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.5315e-04 - val_loss: 0.6367\n",
      "Epoch 372/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.5310e-04 - val_loss: 0.6375\n",
      "Epoch 373/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.5078e-04 - val_loss: 0.6381\n",
      "Epoch 374/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.4790e-04 - val_loss: 0.6387\n",
      "Epoch 375/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.4576e-04 - val_loss: 0.6395\n",
      "Epoch 376/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.4391e-04 - val_loss: 0.6401\n",
      "Epoch 377/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.4198e-04 - val_loss: 0.6407\n",
      "Epoch 378/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.4114e-04 - val_loss: 0.6413\n",
      "Epoch 379/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3871e-04 - val_loss: 0.6420\n",
      "Epoch 380/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3676e-04 - val_loss: 0.6424\n",
      "Epoch 381/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3638e-04 - val_loss: 0.6436\n",
      "Epoch 382/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3371e-04 - val_loss: 0.6442\n",
      "Epoch 383/600\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.3161e-04 - val_loss: 0.6447\n",
      "Epoch 384/600\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.2970e-04 - val_loss: 0.6450\n",
      "Epoch 385/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.2818e-04 - val_loss: 0.6453\n",
      "Epoch 386/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.2753e-04 - val_loss: 0.6458\n",
      "Epoch 387/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.2726e-04 - val_loss: 0.6470\n",
      "Epoch 388/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.2389e-04 - val_loss: 0.6478\n",
      "Epoch 389/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.2184e-04 - val_loss: 0.6484\n",
      "Epoch 390/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.2121e-04 - val_loss: 0.6490\n",
      "Epoch 391/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.1867e-04 - val_loss: 0.6496\n",
      "Epoch 392/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.1723e-04 - val_loss: 0.6503\n",
      "Epoch 393/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.1580e-04 - val_loss: 0.6505\n",
      "Epoch 394/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.1513e-04 - val_loss: 0.6511\n",
      "Epoch 395/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.1254e-04 - val_loss: 0.6521\n",
      "Epoch 396/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.1171e-04 - val_loss: 0.6529\n",
      "Epoch 397/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.1058e-04 - val_loss: 0.6537\n",
      "Epoch 398/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.0890e-04 - val_loss: 0.6541\n",
      "Epoch 399/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.0821e-04 - val_loss: 0.6542\n",
      "Epoch 400/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.0597e-04 - val_loss: 0.6552\n",
      "Epoch 401/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.0447e-04 - val_loss: 0.6564\n",
      "Epoch 402/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.0243e-04 - val_loss: 0.6571\n",
      "Epoch 403/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.0147e-04 - val_loss: 0.6569\n",
      "Epoch 404/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9987e-04 - val_loss: 0.6576\n",
      "Epoch 405/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9789e-04 - val_loss: 0.6582\n",
      "Epoch 406/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9773e-04 - val_loss: 0.6593\n",
      "Epoch 407/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9604e-04 - val_loss: 0.6599\n",
      "Epoch 408/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9445e-04 - val_loss: 0.6606\n",
      "Epoch 409/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9360e-04 - val_loss: 0.6612\n",
      "Epoch 410/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9123e-04 - val_loss: 0.6615\n",
      "Epoch 411/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9003e-04 - val_loss: 0.6620\n",
      "Epoch 412/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8876e-04 - val_loss: 0.6623\n",
      "Epoch 413/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8758e-04 - val_loss: 0.6628\n",
      "Epoch 414/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8799e-04 - val_loss: 0.6637\n",
      "Epoch 415/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8579e-04 - val_loss: 0.6647\n",
      "Epoch 416/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8539e-04 - val_loss: 0.6654\n",
      "Epoch 417/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8211e-04 - val_loss: 0.6656\n",
      "Epoch 418/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8168e-04 - val_loss: 0.6661\n",
      "Epoch 419/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8041e-04 - val_loss: 0.6666\n",
      "Epoch 420/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.7848e-04 - val_loss: 0.6675\n",
      "Epoch 421/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.7737e-04 - val_loss: 0.6683\n",
      "Epoch 422/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.7597e-04 - val_loss: 0.6689\n",
      "Epoch 423/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.7525e-04 - val_loss: 0.6689\n",
      "Epoch 424/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.7479e-04 - val_loss: 0.6698\n",
      "Epoch 425/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.7286e-04 - val_loss: 0.6708\n",
      "Epoch 426/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.7137e-04 - val_loss: 0.6713\n",
      "Epoch 427/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.7142e-04 - val_loss: 0.6716\n",
      "Epoch 428/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6944e-04 - val_loss: 0.6719\n",
      "Epoch 429/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6836e-04 - val_loss: 0.6729\n",
      "Epoch 430/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6795e-04 - val_loss: 0.6738\n",
      "Epoch 431/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6587e-04 - val_loss: 0.6741\n",
      "Epoch 432/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6487e-04 - val_loss: 0.6746\n",
      "Epoch 433/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6370e-04 - val_loss: 0.6751\n",
      "Epoch 434/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6267e-04 - val_loss: 0.6755\n",
      "Epoch 435/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6160e-04 - val_loss: 0.6763\n",
      "Epoch 436/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6077e-04 - val_loss: 0.6770\n",
      "Epoch 437/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5955e-04 - val_loss: 0.6775\n",
      "Epoch 438/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5913e-04 - val_loss: 0.6778\n",
      "Epoch 439/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5845e-04 - val_loss: 0.6789\n",
      "Epoch 440/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5701e-04 - val_loss: 0.6790\n",
      "Epoch 441/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5527e-04 - val_loss: 0.6797\n",
      "Epoch 442/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5410e-04 - val_loss: 0.6801\n",
      "Epoch 443/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5322e-04 - val_loss: 0.6808\n",
      "Epoch 444/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5236e-04 - val_loss: 0.6817\n",
      "Epoch 445/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5136e-04 - val_loss: 0.6821\n",
      "Epoch 446/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5057e-04 - val_loss: 0.6824\n",
      "Epoch 447/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4962e-04 - val_loss: 0.6833\n",
      "Epoch 448/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4825e-04 - val_loss: 0.6839\n",
      "Epoch 449/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4783e-04 - val_loss: 0.6845\n",
      "Epoch 450/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4747e-04 - val_loss: 0.6849\n",
      "Epoch 451/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4615e-04 - val_loss: 0.6853\n",
      "Epoch 452/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4482e-04 - val_loss: 0.6859\n",
      "Epoch 453/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4356e-04 - val_loss: 0.6868\n",
      "Epoch 454/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4293e-04 - val_loss: 0.6876\n",
      "Epoch 455/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4168e-04 - val_loss: 0.6880\n",
      "Epoch 456/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4107e-04 - val_loss: 0.6883\n",
      "Epoch 457/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4005e-04 - val_loss: 0.6890\n",
      "Epoch 458/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3948e-04 - val_loss: 0.6897\n",
      "Epoch 459/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3856e-04 - val_loss: 0.6900\n",
      "Epoch 460/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3800e-04 - val_loss: 0.6907\n",
      "Epoch 461/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3679e-04 - val_loss: 0.6914\n",
      "Epoch 462/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3596e-04 - val_loss: 0.6920\n",
      "Epoch 463/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3576e-04 - val_loss: 0.6923\n",
      "Epoch 464/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3416e-04 - val_loss: 0.6928\n",
      "Epoch 465/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3398e-04 - val_loss: 0.6939\n",
      "Epoch 466/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3284e-04 - val_loss: 0.6943\n",
      "Epoch 467/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3180e-04 - val_loss: 0.6952\n",
      "Epoch 468/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3074e-04 - val_loss: 0.6957\n",
      "Epoch 469/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2998e-04 - val_loss: 0.6960\n",
      "Epoch 470/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2918e-04 - val_loss: 0.6967\n",
      "Epoch 471/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2833e-04 - val_loss: 0.6969\n",
      "Epoch 472/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2768e-04 - val_loss: 0.6975\n",
      "Epoch 473/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2675e-04 - val_loss: 0.6983\n",
      "Epoch 474/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2596e-04 - val_loss: 0.6990\n",
      "Epoch 475/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2555e-04 - val_loss: 0.6995\n",
      "Epoch 476/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2452e-04 - val_loss: 0.6998\n",
      "Epoch 477/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2399e-04 - val_loss: 0.7005\n",
      "Epoch 478/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2305e-04 - val_loss: 0.7011\n",
      "Epoch 479/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2246e-04 - val_loss: 0.7018\n",
      "Epoch 480/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2137e-04 - val_loss: 0.7021\n",
      "Epoch 481/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2077e-04 - val_loss: 0.7028\n",
      "Epoch 482/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2004e-04 - val_loss: 0.7032\n",
      "Epoch 483/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1927e-04 - val_loss: 0.7037\n",
      "Epoch 484/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1836e-04 - val_loss: 0.7042\n",
      "Epoch 485/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1804e-04 - val_loss: 0.7048\n",
      "Epoch 486/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1709e-04 - val_loss: 0.7050\n",
      "Epoch 487/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1640e-04 - val_loss: 0.7058\n",
      "Epoch 488/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1560e-04 - val_loss: 0.7065\n",
      "Epoch 489/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1495e-04 - val_loss: 0.7071\n",
      "Epoch 490/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1414e-04 - val_loss: 0.7074\n",
      "Epoch 491/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1399e-04 - val_loss: 0.7079\n",
      "Epoch 492/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1302e-04 - val_loss: 0.7085\n",
      "Epoch 493/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1217e-04 - val_loss: 0.7091\n",
      "Epoch 494/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1147e-04 - val_loss: 0.7094\n",
      "Epoch 495/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1083e-04 - val_loss: 0.7102\n",
      "Epoch 496/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1065e-04 - val_loss: 0.7112\n",
      "Epoch 497/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0962e-04 - val_loss: 0.7119\n",
      "Epoch 498/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0905e-04 - val_loss: 0.7124\n",
      "Epoch 499/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0820e-04 - val_loss: 0.7128\n",
      "Epoch 500/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0751e-04 - val_loss: 0.7128\n",
      "Epoch 501/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0696e-04 - val_loss: 0.7135\n",
      "Epoch 502/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0671e-04 - val_loss: 0.7141\n",
      "Epoch 503/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0563e-04 - val_loss: 0.7146\n",
      "Epoch 504/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0517e-04 - val_loss: 0.7150\n",
      "Epoch 505/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0480e-04 - val_loss: 0.7155\n",
      "Epoch 506/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0380e-04 - val_loss: 0.7164\n",
      "Epoch 507/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0310e-04 - val_loss: 0.7170\n",
      "Epoch 508/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0253e-04 - val_loss: 0.7176\n",
      "Epoch 509/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0192e-04 - val_loss: 0.7182\n",
      "Epoch 510/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0132e-04 - val_loss: 0.7187\n",
      "Epoch 511/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0065e-04 - val_loss: 0.7191\n",
      "Epoch 512/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0021e-04 - val_loss: 0.7193\n",
      "Epoch 513/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.9695e-05 - val_loss: 0.7197\n",
      "Epoch 514/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.9267e-05 - val_loss: 0.7206\n",
      "Epoch 515/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.8435e-05 - val_loss: 0.7210\n",
      "Epoch 516/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.7803e-05 - val_loss: 0.7216\n",
      "Epoch 517/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.7223e-05 - val_loss: 0.7224\n",
      "Epoch 518/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.6643e-05 - val_loss: 0.7228\n",
      "Epoch 519/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.5936e-05 - val_loss: 0.7230\n",
      "Epoch 520/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.5487e-05 - val_loss: 0.7235\n",
      "Epoch 521/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.5146e-05 - val_loss: 0.7237\n",
      "Epoch 522/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.4967e-05 - val_loss: 0.7245\n",
      "Epoch 523/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.3925e-05 - val_loss: 0.7253\n",
      "Epoch 524/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.3464e-05 - val_loss: 0.7259\n",
      "Epoch 525/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.3357e-05 - val_loss: 0.7260\n",
      "Epoch 526/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.2129e-05 - val_loss: 0.7269\n",
      "Epoch 527/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.1800e-05 - val_loss: 0.7278\n",
      "Epoch 528/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.1130e-05 - val_loss: 0.7283\n",
      "Epoch 529/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.0818e-05 - val_loss: 0.7285\n",
      "Epoch 530/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.0025e-05 - val_loss: 0.7288\n",
      "Epoch 531/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.9559e-05 - val_loss: 0.7292\n",
      "Epoch 532/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.9072e-05 - val_loss: 0.7297\n",
      "Epoch 533/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.8622e-05 - val_loss: 0.7305\n",
      "Epoch 534/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.8079e-05 - val_loss: 0.7307\n",
      "Epoch 535/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.7440e-05 - val_loss: 0.7312\n",
      "Epoch 536/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.7148e-05 - val_loss: 0.7321\n",
      "Epoch 537/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.6664e-05 - val_loss: 0.7326\n",
      "Epoch 538/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.5938e-05 - val_loss: 0.7328\n",
      "Epoch 539/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.5734e-05 - val_loss: 0.7333\n",
      "Epoch 540/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.5089e-05 - val_loss: 0.7338\n",
      "Epoch 541/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.4658e-05 - val_loss: 0.7343\n",
      "Epoch 542/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.4131e-05 - val_loss: 0.7353\n",
      "Epoch 543/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.3578e-05 - val_loss: 0.7357\n",
      "Epoch 544/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.3136e-05 - val_loss: 0.7360\n",
      "Epoch 545/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.2797e-05 - val_loss: 0.7365\n",
      "Epoch 546/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.2331e-05 - val_loss: 0.7371\n",
      "Epoch 547/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.2226e-05 - val_loss: 0.7373\n",
      "Epoch 548/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.1498e-05 - val_loss: 0.7379\n",
      "Epoch 549/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.0966e-05 - val_loss: 0.7386\n",
      "Epoch 550/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.0305e-05 - val_loss: 0.7394\n",
      "Epoch 551/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.0016e-05 - val_loss: 0.7398\n",
      "Epoch 552/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.9842e-05 - val_loss: 0.7402\n",
      "Epoch 553/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.9117e-05 - val_loss: 0.7409\n",
      "Epoch 554/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.8701e-05 - val_loss: 0.7418\n",
      "Epoch 555/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.8206e-05 - val_loss: 0.7420\n",
      "Epoch 556/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.7791e-05 - val_loss: 0.7426\n",
      "Epoch 557/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.7275e-05 - val_loss: 0.7429\n",
      "Epoch 558/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.6899e-05 - val_loss: 0.7433\n",
      "Epoch 559/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.6542e-05 - val_loss: 0.7437\n",
      "Epoch 560/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.6037e-05 - val_loss: 0.7448\n",
      "Epoch 561/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.5681e-05 - val_loss: 0.7456\n",
      "Epoch 562/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.5344e-05 - val_loss: 0.7459\n",
      "Epoch 563/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 7.4903e-05 - val_loss: 0.7462\n",
      "Epoch 564/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.4299e-05 - val_loss: 0.7465\n",
      "Epoch 565/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.3914e-05 - val_loss: 0.7470\n",
      "Epoch 566/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.3505e-05 - val_loss: 0.7478\n",
      "Epoch 567/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.3062e-05 - val_loss: 0.7483\n",
      "Epoch 568/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.2699e-05 - val_loss: 0.7485\n",
      "Epoch 569/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.2548e-05 - val_loss: 0.7488\n",
      "Epoch 570/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.2307e-05 - val_loss: 0.7494\n",
      "Epoch 571/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.1592e-05 - val_loss: 0.7503\n",
      "Epoch 572/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.1072e-05 - val_loss: 0.7506\n",
      "Epoch 573/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.0911e-05 - val_loss: 0.7509\n",
      "Epoch 574/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.0416e-05 - val_loss: 0.7517\n",
      "Epoch 575/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.0512e-05 - val_loss: 0.7526\n",
      "Epoch 576/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.9808e-05 - val_loss: 0.7528\n",
      "Epoch 577/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.9215e-05 - val_loss: 0.7532\n",
      "Epoch 578/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.8760e-05 - val_loss: 0.7537\n",
      "Epoch 579/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.8549e-05 - val_loss: 0.7542\n",
      "Epoch 580/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.8072e-05 - val_loss: 0.7545\n",
      "Epoch 581/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.7651e-05 - val_loss: 0.7552\n",
      "Epoch 582/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.7284e-05 - val_loss: 0.7560\n",
      "Epoch 583/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.7245e-05 - val_loss: 0.7561\n",
      "Epoch 584/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.7044e-05 - val_loss: 0.7569\n",
      "Epoch 585/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.6346e-05 - val_loss: 0.7577\n",
      "Epoch 586/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.5810e-05 - val_loss: 0.7578\n",
      "Epoch 587/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.5603e-05 - val_loss: 0.7580\n",
      "Epoch 588/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.5495e-05 - val_loss: 0.7588\n",
      "Epoch 589/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.4888e-05 - val_loss: 0.7592\n",
      "Epoch 590/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.4383e-05 - val_loss: 0.7598\n",
      "Epoch 591/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.4116e-05 - val_loss: 0.7602\n",
      "Epoch 592/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.3749e-05 - val_loss: 0.7607\n",
      "Epoch 593/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.3348e-05 - val_loss: 0.7612\n",
      "Epoch 594/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.3039e-05 - val_loss: 0.7617\n",
      "Epoch 595/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.2692e-05 - val_loss: 0.7620\n",
      "Epoch 596/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.2439e-05 - val_loss: 0.7628\n",
      "Epoch 597/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.1987e-05 - val_loss: 0.7634\n",
      "Epoch 598/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.1695e-05 - val_loss: 0.7638\n",
      "Epoch 599/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.1633e-05 - val_loss: 0.7642\n",
      "Epoch 600/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.1099e-05 - val_loss: 0.7646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x90786ca8c8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = X_train, y = y_train, epochs = 600, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x9078cf2448>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can take a look at the graph, and it’s showing that the model is overfitted as loss is increasing.\n",
    "losses = pd.DataFrame(model.history.history)\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "\n",
    "# BINARY CLASSIFICATION\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we tried to add Early stopping callbacks and dropout layer to reduce overfitting\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class EarlyStopping in module tensorflow.python.keras.callbacks:\n",
      "\n",
      "class EarlyStopping(Callback)\n",
      " |  EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
      " |  \n",
      " |  Stop training when a monitored metric has stopped improving.\n",
      " |  \n",
      " |  Assuming the goal of a training is to minimize the loss. With this, the\n",
      " |  metric to be monitored would be `'loss'`, and mode would be `'min'`. A\n",
      " |  `model.fit()` training loop will check at end of every epoch whether\n",
      " |  the loss is no longer decreasing, considering the `min_delta` and\n",
      " |  `patience` if applicable. Once it's found no longer decreasing,\n",
      " |  `model.stop_training` is marked True and the training terminates.\n",
      " |  \n",
      " |  The quantity to be monitored needs to be available in `logs` dict.\n",
      " |  To make it so, pass the loss or metrics at `model.compile()`.\n",
      " |  \n",
      " |  Arguments:\n",
      " |    monitor: Quantity to be monitored.\n",
      " |    min_delta: Minimum change in the monitored quantity\n",
      " |        to qualify as an improvement, i.e. an absolute\n",
      " |        change of less than min_delta, will count as no\n",
      " |        improvement.\n",
      " |    patience: Number of epochs with no improvement\n",
      " |        after which training will be stopped.\n",
      " |    verbose: verbosity mode.\n",
      " |    mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode,\n",
      " |        training will stop when the quantity\n",
      " |        monitored has stopped decreasing; in `\"max\"`\n",
      " |        mode it will stop when the quantity\n",
      " |        monitored has stopped increasing; in `\"auto\"`\n",
      " |        mode, the direction is automatically inferred\n",
      " |        from the name of the monitored quantity.\n",
      " |    baseline: Baseline value for the monitored quantity.\n",
      " |        Training will stop if the model doesn't show improvement over the\n",
      " |        baseline.\n",
      " |    restore_best_weights: Whether to restore model weights from\n",
      " |        the epoch with the best value of the monitored quantity.\n",
      " |        If False, the model weights obtained at the last step of\n",
      " |        training are used.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  >>> callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
      " |  >>> # This callback will stop the training when there is no improvement in\n",
      " |  >>> # the validation loss for three consecutive epochs.\n",
      " |  >>> model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n",
      " |  >>> model.compile(tf.keras.optimizers.SGD(), loss='mse')\n",
      " |  >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n",
      " |  ...                     epochs=10, batch_size=1, callbacks=[callback],\n",
      " |  ...                     verbose=0)\n",
      " |  >>> len(history.history['loss'])  # Only 4 epochs are run.\n",
      " |  4\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      EarlyStopping\n",
      " |      Callback\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_monitor_value(self, logs)\n",
      " |  \n",
      " |  on_epoch_end(self, epoch, logs=None)\n",
      " |      Called at the end of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict, metric results for this training epoch, and for the\n",
      " |            validation epoch if validation is performed. Validation result keys\n",
      " |            are prefixed with `val_`.\n",
      " |  \n",
      " |  on_train_begin(self, logs=None)\n",
      " |      Called at the beginning of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_end(self, logs=None)\n",
      " |      Called at the end of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently the output of the last call to `on_epoch_end()`\n",
      " |            is passed to this argument for this method but that may change in\n",
      " |            the future.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Callback:\n",
      " |  \n",
      " |  on_batch_begin(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_begin`.\n",
      " |  \n",
      " |  on_batch_end(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_end`.\n",
      " |  \n",
      " |  on_epoch_begin(self, epoch, logs=None)\n",
      " |      Called at the start of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.predict_step`,\n",
      " |            it typically returns a dict with a key 'outputs' containing\n",
      " |            the model's outputs.\n",
      " |  \n",
      " |  on_predict_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_predict_begin(self, logs=None)\n",
      " |      Called at the beginning of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_end(self, logs=None)\n",
      " |      Called at the end of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the beginning of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.test_step`. Typically,\n",
      " |            the values of the `Model`'s metrics are returned.  Example:\n",
      " |            `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  on_test_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the end of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_test_begin(self, logs=None)\n",
      " |      Called at the beginning of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_end(self, logs=None)\n",
      " |      Called at the end of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently the output of the last call to\n",
      " |            `on_test_batch_end()` is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.train_step`. Typically,\n",
      " |            the values of the `Model`'s metrics are returned.  Example:\n",
      " |            `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  on_train_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  set_model(self, model)\n",
      " |  \n",
      " |  set_params(self, params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Callback:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(EarlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6610 - val_loss: 0.6498\n",
      "Epoch 2/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.6399\n",
      "Epoch 3/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5815 - val_loss: 0.6245\n",
      "Epoch 4/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5503 - val_loss: 0.5962\n",
      "Epoch 5/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5188 - val_loss: 0.5583\n",
      "Epoch 6/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4862 - val_loss: 0.5259\n",
      "Epoch 7/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4548 - val_loss: 0.4864\n",
      "Epoch 8/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4234 - val_loss: 0.4520\n",
      "Epoch 9/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3918 - val_loss: 0.4218\n",
      "Epoch 10/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3624 - val_loss: 0.3824\n",
      "Epoch 11/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3344 - val_loss: 0.3572\n",
      "Epoch 12/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3104 - val_loss: 0.3302\n",
      "Epoch 13/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2894 - val_loss: 0.3125\n",
      "Epoch 14/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2726 - val_loss: 0.2898\n",
      "Epoch 15/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2583 - val_loss: 0.2843\n",
      "Epoch 16/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2449 - val_loss: 0.2685\n",
      "Epoch 17/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2301 - val_loss: 0.2620\n",
      "Epoch 18/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2226 - val_loss: 0.2621\n",
      "Epoch 19/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2102 - val_loss: 0.2510\n",
      "Epoch 20/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2061 - val_loss: 0.2490\n",
      "Epoch 21/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1909 - val_loss: 0.2497\n",
      "Epoch 22/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1865 - val_loss: 0.2464\n",
      "Epoch 23/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1776 - val_loss: 0.2418\n",
      "Epoch 24/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1720 - val_loss: 0.2419\n",
      "Epoch 25/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1724 - val_loss: 0.2431\n",
      "Epoch 26/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1570 - val_loss: 0.2397\n",
      "Epoch 27/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1519 - val_loss: 0.2382\n",
      "Epoch 28/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1457 - val_loss: 0.2393\n",
      "Epoch 29/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1394 - val_loss: 0.2392\n",
      "Epoch 30/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1340 - val_loss: 0.2402\n",
      "Epoch 31/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1313 - val_loss: 0.2414\n",
      "Epoch 32/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1274 - val_loss: 0.2418\n",
      "Epoch 33/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1199 - val_loss: 0.2445\n",
      "Epoch 34/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1155 - val_loss: 0.2438\n",
      "Epoch 35/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1111 - val_loss: 0.2439\n",
      "Epoch 36/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1064 - val_loss: 0.2448\n",
      "Epoch 37/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1038 - val_loss: 0.2472\n",
      "Epoch 38/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0985 - val_loss: 0.2480\n",
      "Epoch 39/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0956 - val_loss: 0.2498\n",
      "Epoch 40/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0917 - val_loss: 0.2510\n",
      "Epoch 41/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0878 - val_loss: 0.2507\n",
      "Epoch 42/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0848 - val_loss: 0.2534\n",
      "Epoch 43/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0813 - val_loss: 0.2566\n",
      "Epoch 44/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0777 - val_loss: 0.2582\n",
      "Epoch 45/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0756 - val_loss: 0.2604\n",
      "Epoch 46/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0718 - val_loss: 0.2611\n",
      "Epoch 47/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0687 - val_loss: 0.2620\n",
      "Epoch 48/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0662 - val_loss: 0.2661\n",
      "Epoch 49/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0639 - val_loss: 0.2688\n",
      "Epoch 50/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0603 - val_loss: 0.2721\n",
      "Epoch 51/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0582 - val_loss: 0.2738\n",
      "Epoch 52/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0550 - val_loss: 0.2799\n",
      "Epoch 00052: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x9078dee208>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=600, validation_data=(X_test, y_test), callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x90791ade88>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss = pd.DataFrame(model.history.history)\n",
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# BINARY CLSSIFICATION\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6938 - val_loss: 0.6873\n",
      "Epoch 2/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6758 - val_loss: 0.6805\n",
      "Epoch 3/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6555 - val_loss: 0.6760\n",
      "Epoch 4/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6620 - val_loss: 0.6716\n",
      "Epoch 5/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6558 - val_loss: 0.6677\n",
      "Epoch 6/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6337 - val_loss: 0.6640\n",
      "Epoch 7/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6342 - val_loss: 0.6591\n",
      "Epoch 8/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6472 - val_loss: 0.6530\n",
      "Epoch 9/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6271 - val_loss: 0.6455\n",
      "Epoch 10/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6310 - val_loss: 0.6376\n",
      "Epoch 11/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6266 - val_loss: 0.6291\n",
      "Epoch 12/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5785 - val_loss: 0.6203\n",
      "Epoch 13/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6013 - val_loss: 0.6099\n",
      "Epoch 14/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6032 - val_loss: 0.6002\n",
      "Epoch 15/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5884 - val_loss: 0.5897\n",
      "Epoch 16/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5666 - val_loss: 0.5786\n",
      "Epoch 17/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5758 - val_loss: 0.5703\n",
      "Epoch 18/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5652 - val_loss: 0.5605\n",
      "Epoch 19/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5478 - val_loss: 0.5457\n",
      "Epoch 20/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5488 - val_loss: 0.5257\n",
      "Epoch 21/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5164 - val_loss: 0.5102\n",
      "Epoch 22/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5012 - val_loss: 0.4974\n",
      "Epoch 23/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4656 - val_loss: 0.4803\n",
      "Epoch 24/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4815 - val_loss: 0.4640\n",
      "Epoch 25/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5093 - val_loss: 0.4478\n",
      "Epoch 26/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4663 - val_loss: 0.4351\n",
      "Epoch 27/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4803 - val_loss: 0.4115\n",
      "Epoch 28/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4402 - val_loss: 0.3916\n",
      "Epoch 29/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4775 - val_loss: 0.3819\n",
      "Epoch 30/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4051 - val_loss: 0.3725\n",
      "Epoch 31/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4029 - val_loss: 0.3662\n",
      "Epoch 32/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4019 - val_loss: 0.3550\n",
      "Epoch 33/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3671 - val_loss: 0.3319\n",
      "Epoch 34/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3994 - val_loss: 0.3153\n",
      "Epoch 35/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3947 - val_loss: 0.3036\n",
      "Epoch 36/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4227 - val_loss: 0.2991\n",
      "Epoch 37/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4077 - val_loss: 0.2958\n",
      "Epoch 38/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3657 - val_loss: 0.2926\n",
      "Epoch 39/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3349 - val_loss: 0.2871\n",
      "Epoch 40/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3501 - val_loss: 0.2841\n",
      "Epoch 41/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3228 - val_loss: 0.2781\n",
      "Epoch 42/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3415 - val_loss: 0.2736\n",
      "Epoch 43/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2831 - val_loss: 0.2691\n",
      "Epoch 44/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3191 - val_loss: 0.2674\n",
      "Epoch 45/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2900 - val_loss: 0.2606\n",
      "Epoch 46/600\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3090 - val_loss: 0.2501\n",
      "Epoch 47/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3105 - val_loss: 0.2450\n",
      "Epoch 48/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3232 - val_loss: 0.2461\n",
      "Epoch 49/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2928 - val_loss: 0.2459\n",
      "Epoch 50/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3026 - val_loss: 0.2436\n",
      "Epoch 51/600\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3477 - val_loss: 0.2432\n",
      "Epoch 52/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3192 - val_loss: 0.2470\n",
      "Epoch 53/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2999 - val_loss: 0.2474\n",
      "Epoch 54/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2588 - val_loss: 0.2492\n",
      "Epoch 55/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2899 - val_loss: 0.2427\n",
      "Epoch 56/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2618 - val_loss: 0.2386\n",
      "Epoch 57/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3003 - val_loss: 0.2383\n",
      "Epoch 58/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2832 - val_loss: 0.2385\n",
      "Epoch 59/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2831 - val_loss: 0.2324\n",
      "Epoch 60/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2463 - val_loss: 0.2327\n",
      "Epoch 61/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2443 - val_loss: 0.2330\n",
      "Epoch 62/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2695 - val_loss: 0.2325\n",
      "Epoch 63/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2931 - val_loss: 0.2302\n",
      "Epoch 64/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2410 - val_loss: 0.2314\n",
      "Epoch 65/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2978 - val_loss: 0.2307\n",
      "Epoch 66/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2791 - val_loss: 0.2325\n",
      "Epoch 67/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2360 - val_loss: 0.2313\n",
      "Epoch 68/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2029 - val_loss: 0.2313\n",
      "Epoch 69/600\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.233 - 0s 2ms/step - loss: 0.2268 - val_loss: 0.2329\n",
      "Epoch 70/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2135 - val_loss: 0.2255\n",
      "Epoch 71/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2532 - val_loss: 0.2238\n",
      "Epoch 72/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2171 - val_loss: 0.2242\n",
      "Epoch 73/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2164 - val_loss: 0.2257\n",
      "Epoch 74/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2143 - val_loss: 0.2260\n",
      "Epoch 75/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2156 - val_loss: 0.2279\n",
      "Epoch 76/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2411 - val_loss: 0.2312\n",
      "Epoch 77/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2293 - val_loss: 0.2291\n",
      "Epoch 78/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2129 - val_loss: 0.2276\n",
      "Epoch 79/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1934 - val_loss: 0.2262\n",
      "Epoch 80/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2146 - val_loss: 0.2273\n",
      "Epoch 81/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1836 - val_loss: 0.2268\n",
      "Epoch 82/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1658 - val_loss: 0.2282\n",
      "Epoch 83/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2012 - val_loss: 0.2275\n",
      "Epoch 84/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1703 - val_loss: 0.2263\n",
      "Epoch 85/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1650 - val_loss: 0.2275\n",
      "Epoch 86/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1896 - val_loss: 0.2299\n",
      "Epoch 87/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1629 - val_loss: 0.2332\n",
      "Epoch 88/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1750 - val_loss: 0.2362\n",
      "Epoch 89/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1988 - val_loss: 0.2392\n",
      "Epoch 90/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1683 - val_loss: 0.2397\n",
      "Epoch 91/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1726 - val_loss: 0.2394\n",
      "Epoch 92/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1696 - val_loss: 0.2416\n",
      "Epoch 93/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1606 - val_loss: 0.2445\n",
      "Epoch 94/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1657 - val_loss: 0.2458\n",
      "Epoch 95/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1391 - val_loss: 0.2453\n",
      "Epoch 96/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1534 - val_loss: 0.2461\n",
      "Epoch 00096: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x907921c688>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=600, validation_data=(X_test, y_test), callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x9078790d08>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-41-bc83193b8b59>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94        42\n",
      "           1       0.90      0.93      0.92        30\n",
      "\n",
      "    accuracy                           0.93        72\n",
      "   macro avg       0.93      0.93      0.93        72\n",
      "weighted avg       0.93      0.93      0.93        72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39  3]\n",
      " [ 2 28]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting model to .h5 and .tflite formats to be used for inference on microcontroller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating the exact same model, including its weights and the optimizer\n",
    "model = tf.keras.models.load_model('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpjlvrz7c5\\assets\n",
      "Size of h5 model: 69.78515625 KB\n",
      "Size of tflite model: 14.19921875 KB\n",
      "Decreased for factor: 4.914718019257221\n"
     ]
    }
   ],
   "source": [
    "# Converting the model without quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Saving the model to the disk\n",
    "open(\"final_quant.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "# size of the .h5 model\n",
    "h5_in_kb = os.path.getsize(\"final_model.h5\") / 1024\n",
    "print(\"Size of h5 model: {} KB\".format(h5_in_kb))\n",
    "\n",
    "# size of the .tflite model\n",
    "tflite_in_kb = os.path.getsize(\"final_quant.tflite\") / 1024\n",
    "print(\"Size of tflite model: {} KB\".format(tflite_in_kb))\n",
    "print(\"Decreased for factor: {}\".format(h5_in_kb/tflite_in_kb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpuao2nkbc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpuao2nkbc\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8960"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the model to the TensorFlow Lite format with float16 quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()\n",
    "# Save to disk\n",
    "open(\"float16_quant.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp9nxeo3tp\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp9nxeo3tp\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6624"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the model to the TensorFlow Lite format with dynamic range quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "# Save to disk\n",
    "open(\"final_dynamic_range_quantization.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1488: set_learning_phase (from tensorflow.python.keras.backend) is deprecated and will be removed after 2020-10-11.\n",
      "Instructions for updating:\n",
      "Simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1488: set_learning_phase (from tensorflow.python.keras.backend) is deprecated and will be removed after 2020-10-11.\n",
      "Instructions for updating:\n",
      "Simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpa58_r41q\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpa58_r41q\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\admin\\AppData\\Local\\Temp\\tmpa58_r41q\\variables\\variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\admin\\AppData\\Local\\Temp\\tmpa58_r41q\\variables\\variables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'__saved_model_init_op', 'serving_default'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'__saved_model_init_op', 'serving_default'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input tensors info: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input tensors info: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_6_input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_6_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: serving_default_dense_6_input:0, shape: (-1, 90), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: serving_default_dense_6_input:0, shape: (-1, 90), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:output tensors info: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:output tensors info: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: StatefulPartitionedCall:0, shape: (-1, 1), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: StatefulPartitionedCall:0, shape: (-1, 1), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\admin\\AppData\\Local\\Temp\\tmpa58_r41q\\variables\\variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\admin\\AppData\\Local\\Temp\\tmpa58_r41q\\variables\\variables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\util.py:275: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\util.py:275: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\convert_to_constants.py:854: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\convert_to_constants.py:854: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5872"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the model to the tensorFlow lite format with full integer quantization\n",
    "scans = tf.cast(X_test, tf.float32)\n",
    "scans_data = tf.data.Dataset.from_tensor_slices(scans).batch(1)\n",
    "def representative_dataset_gen():\n",
    "  for input in scans_data.take(70):\n",
    "    # Get sample input data as a numpy array in a method of your choosing.\n",
    "    yield [input]\n",
    "    \n",
    "converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(\"final_model.h5\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.float32\n",
    "tflite_model = converter.convert()\n",
    "# Save to disk\n",
    "open(\"final_full_integer_quant.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = pd.read_csv(r\"valitation_set_3_n_5_p.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Distance0</th>\n",
       "      <th>Distance1</th>\n",
       "      <th>Distance2</th>\n",
       "      <th>Distance3</th>\n",
       "      <th>Distance4</th>\n",
       "      <th>Distance5</th>\n",
       "      <th>Distance6</th>\n",
       "      <th>Distance7</th>\n",
       "      <th>Distance8</th>\n",
       "      <th>...</th>\n",
       "      <th>Distance80</th>\n",
       "      <th>Distance81</th>\n",
       "      <th>Distance82</th>\n",
       "      <th>Distance83</th>\n",
       "      <th>Distance84</th>\n",
       "      <th>Distance85</th>\n",
       "      <th>Distance86</th>\n",
       "      <th>Distance87</th>\n",
       "      <th>Distance88</th>\n",
       "      <th>Distance89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>106</td>\n",
       "      <td>105</td>\n",
       "      <td>74</td>\n",
       "      <td>48</td>\n",
       "      <td>102</td>\n",
       "      <td>45</td>\n",
       "      <td>108</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>93</td>\n",
       "      <td>72</td>\n",
       "      <td>93</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>75</td>\n",
       "      <td>7</td>\n",
       "      <td>91</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>75</td>\n",
       "      <td>72</td>\n",
       "      <td>64</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>87</td>\n",
       "      <td>...</td>\n",
       "      <td>89</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>82</td>\n",
       "      <td>8</td>\n",
       "      <td>104</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>102</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>77</td>\n",
       "      <td>193</td>\n",
       "      <td>79</td>\n",
       "      <td>7</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>50</td>\n",
       "      <td>77</td>\n",
       "      <td>80</td>\n",
       "      <td>65</td>\n",
       "      <td>122</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>133</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>54</td>\n",
       "      <td>55</td>\n",
       "      <td>38</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>64</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "      <td>63</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>57</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>8</td>\n",
       "      <td>59</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>61</td>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "      <td>61</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>66</td>\n",
       "      <td>68</td>\n",
       "      <td>7</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>60</td>\n",
       "      <td>48</td>\n",
       "      <td>61</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>82</td>\n",
       "      <td>80</td>\n",
       "      <td>70</td>\n",
       "      <td>61</td>\n",
       "      <td>82</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>54</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>47</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>82</td>\n",
       "      <td>81</td>\n",
       "      <td>70</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>7</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>70</td>\n",
       "      <td>46</td>\n",
       "      <td>6</td>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Distance0  Distance1  Distance2  Distance3  Distance4  Distance5  \\\n",
       "0     0        105        106        105         74         48        102   \n",
       "1     0         76         76         75         72         64         75   \n",
       "2     0         78         78         77        193         79          7   \n",
       "3     1         55         54         55         38         55         55   \n",
       "4     1         60         60         59         57         59         59   \n",
       "5     1         67         66         67         67         66         68   \n",
       "6     1         83         82         80         70         61         82   \n",
       "7     1         81         82         81         70         82         82   \n",
       "\n",
       "   Distance6  Distance7  Distance8  ...  Distance80  Distance81  Distance82  \\\n",
       "0         45        108         46  ...          82          93          72   \n",
       "1         75         75         87  ...          89         102         102   \n",
       "2         78         78          6  ...          76          50          77   \n",
       "3         54         54         54  ...          64          62          63   \n",
       "4          8         59         57  ...          61           7          47   \n",
       "5          7         67         67  ...           9          49          60   \n",
       "6         81          7         69  ...          61          61          64   \n",
       "7          7         81         81  ...          70          46           6   \n",
       "\n",
       "   Distance83  Distance84  Distance85  Distance86  Distance87  Distance88  \\\n",
       "0          93          30          33          75           7          91   \n",
       "1          82           8         104           8          45         102   \n",
       "2          80          65         122           7           7         133   \n",
       "3          62          63          63          63           7          63   \n",
       "4          61          60          60           6          60          60   \n",
       "5          48          61          64           6          54          64   \n",
       "6          60          54          46           7          48          47   \n",
       "7          69          70          70           7           7          72   \n",
       "\n",
       "   Distance89  \n",
       "0          65  \n",
       "1          59  \n",
       "2          58  \n",
       "3           8  \n",
       "4          49  \n",
       "5           7  \n",
       "6          45  \n",
       "7          72  \n",
       "\n",
       "[8 rows x 91 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validate = validation_set.drop('Type', axis = 1).values\n",
    "y_validate = validation_set['Type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[105, 106, 105,  74,  48, 102,  45, 108,  46,  49,  35,   7,   9,\n",
       "         93, 101,  99,  35,  42,  63,  96,   6,  38, 100,  36, 108,  92,\n",
       "          6,  47,  54,  74,  68,  81,  67,  56,  50,   7, 112, 111,  68,\n",
       "        118, 117,  80,  51, 107, 111, 110,  84,   7, 107,  71,  49,  74,\n",
       "        108, 109,  60,   8,  33,  75, 127, 124, 130,  60,  94,  61,   8,\n",
       "        138,   8, 138, 112, 169,   8,  62,  92, 110,  64,   6,  71,  94,\n",
       "        113,  56,  82,  93,  72,  93,  30,  33,  75,   7,  91,  65],\n",
       "       [ 76,  76,  75,  72,  64,  75,  75,  75,  87,  86,  85,   8,  85,\n",
       "         87,  54,  84,  85,  53,   8,  94,   7,  56, 132, 134,  56, 129,\n",
       "         61,  71,   6,  46,  82, 102, 114,  70,   8, 113, 112, 113,  97,\n",
       "          8,  95, 106,   6, 111, 111, 105, 104,  90,  63, 103,  53,  56,\n",
       "        103,  94, 102,   7, 104,  99, 104,   5, 103,  52, 101,  58,   8,\n",
       "        102,   6, 101,  97, 102,   8, 102,  81, 102,  80,  58,  84, 102,\n",
       "        102,   7,  89, 102, 102,  82,   8, 104,   8,  45, 102,  59],\n",
       "       [ 78,  78,  77, 193,  79,   7,  78,  78,   6,  64,  77,   7,  76,\n",
       "         56,  74,  75,  67,   8,  76,  75,  66,  75,  75,   8,  75,  74,\n",
       "         75,  47,  75,  74,  74,  65,  75,   8,  72,  74,  73,  75,  69,\n",
       "         74,  67,   6,  74,  49,  75,  75,  74,   8,  75,  75,  75,  46,\n",
       "         75,  75,  75,  47,  75,  74,  47,   6,  74,  53,   8,  75,   6,\n",
       "         75,  73,  75,  76,  75,   7,  75,  69,  76,  76,   7,   6,  77,\n",
       "         77,   8,  76,  50,  77,  80,  65, 122,   7,   7, 133,  58],\n",
       "       [ 55,  54,  55,  38,  55,  55,  54,  54,  54,  54,   9,  53,  54,\n",
       "         54,   0,  53,  53,   8,  54,  54,  53,  53,  54,  53,  54,  54,\n",
       "         53,  53,  54,  53,  53,  53,  53,  53,  53,  50,  53,  53,  54,\n",
       "         53,  53,   8,  53,  53,  53,   8,  54,  54,  54,  54,  54,   8,\n",
       "         54,  51,  54,  56,  55,  55,  55,   8,  56,  53,  57,  58,   8,\n",
       "         58,  50,  66,  64,  64,  64,  54,  66,  66,  64,  65,  64,   8,\n",
       "         63,  50,  64,  62,  63,  62,  63,  63,  63,   7,  63,   8],\n",
       "       [ 60,  60,  59,  57,  59,  59,   8,  59,  57,  58,   7,  58,  58,\n",
       "         56,  57,  58,   8,   6,  48,  58,  56,  58,  57,  52,  57,  58,\n",
       "         59,  59,  57,  57,  55,  58,  56,  59,  58,  58,  59,  49,  60,\n",
       "          8,  60,   7,   8,   8,  67,   8,  96,  72,  93,   8,  81,   7,\n",
       "         93,   8,  96,  60,   8,  96,  97,   7,  53,  57,  89, 106,  56,\n",
       "        106,   8,  64,  62,  63,  58,  63,  58,  62,  62,  61,  60,  60,\n",
       "         45,  46,  61,   7,  47,  61,  60,  60,   6,  60,  60,  49],\n",
       "       [ 67,  66,  67,  67,  66,  68,   7,  67,  67,  66,   7,  65,  67,\n",
       "         65,  59,  65,  66,  67,  67,  66,  52,  52,  66,  69,  68,  68,\n",
       "          8,  48,  58,  61,  68,  53,  52,  53,   7,   8,  52,  51,  51,\n",
       "         49,  51,  51,  52,  50,  51,  49,  49,  52,  50,  51,  52,   7,\n",
       "         52,  52,  51,   7,  50,  51,  49,   7,  51,  51,   7,  52,  50,\n",
       "         51,  51,  52,  51,  52,  52,  52,   7,   6,  53,  63,   6,  63,\n",
       "         64,  64,   9,  49,  60,  48,  61,  64,   6,  54,  64,   7],\n",
       "       [ 83,  82,  80,  70,  61,  82,  81,   7,  69,   7,  68,  54,  67,\n",
       "          8,   6,  60,   6,  62,  62,  60,  58,   6,  59,  60,  60,  59,\n",
       "          8,  59,  60,  52,  54,  58,  59,  59,  59,  60,  59,  61,  61,\n",
       "         55,  62,   8,  60,   8,  59,  49,  61,  60,  59,  59,  59,  51,\n",
       "         53,  50,  53,   6,   8,  52,  82,  48, 106,  52,  52,  51,  50,\n",
       "        115,  48, 114,  61, 115,  50, 114,  49, 115, 101,  82,  50, 114,\n",
       "        114,  61,  61,  61,  64,  60,  54,  46,   7,  48,  47,  45],\n",
       "       [ 81,  82,  81,  70,  82,  82,   7,  81,  81,  84,  82,  69,  81,\n",
       "          8,  46,  78,  79,  71,  50,  77,  59,  60,  59,  77,  76,  76,\n",
       "         62,  75,   6,  76,  76,  76,  57,  65,  75,  76,  57,  76,  75,\n",
       "         76,  76,  59,  76,   7,  75,   6,  49,  72,  51,  71,  65,  49,\n",
       "         75,  75,  53,   9,  76,  74,  73,   7,  73,  73,  50,  73,  59,\n",
       "         73,   7,  73,  72,  72,  73,  72,   9,  69,  72,  60,  56,   7,\n",
       "         72,  49,  70,  46,   6,  69,  70,  70,   7,   7,  72,  72]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29411765, 0.29691877, 0.31343284, 0.62184874, 0.26373626,\n",
       "        0.60714286, 0.35714286, 0.36486486, 0.15753425, 0.38582677,\n",
       "        0.14522822, 0.03977273, 0.03846154, 0.44075829, 0.50248756,\n",
       "        0.50253807, 0.17326733, 0.29787234, 0.3480663 , 0.49740933,\n",
       "        0.03225806, 0.17511521, 0.5988024 , 0.20930233, 0.77142857,\n",
       "        0.54761905, 0.02764977, 0.31972789, 0.30508475, 0.3627451 ,\n",
       "        0.40963855, 0.28222997, 0.21202532, 0.28571429, 0.32467532,\n",
       "        0.03825137, 0.47058824, 0.54411765, 0.46575342, 0.56730769,\n",
       "        0.58208955, 0.46242775, 0.28977273, 0.41472868, 0.3447205 ,\n",
       "        0.46413502, 0.48837209, 0.04697987, 0.44957983, 0.29957806,\n",
       "        0.21777778, 0.5323741 , 0.53465347, 0.6300578 , 0.25      ,\n",
       "        0.04907975, 0.19760479, 0.37313433, 0.51209677, 0.90510949,\n",
       "        0.55084746, 0.36809816, 0.46078431, 0.30808081, 0.06722689,\n",
       "        0.61883408, 0.05797101, 0.67317073, 0.60540541, 0.9494382 ,\n",
       "        0.04519774, 0.35227273, 0.4972973 , 0.61797753, 0.27004219,\n",
       "        0.02158273, 0.64545455, 0.55621302, 0.4501992 , 0.3255814 ,\n",
       "        0.37788018, 0.69924812, 0.2962963 , 0.43661972, 0.15228426,\n",
       "        0.20245399, 0.42857143, 0.05109489, 0.3625498 , 0.44520548],\n",
       "       [0.21288515, 0.21288515, 0.2238806 , 0.60504202, 0.35164835,\n",
       "        0.44642857, 0.5952381 , 0.25337838, 0.29794521, 0.67716535,\n",
       "        0.3526971 , 0.04545455, 0.36324786, 0.41232227, 0.26865672,\n",
       "        0.42639594, 0.42079208, 0.37588652, 0.0441989 , 0.48704663,\n",
       "        0.03763441, 0.25806452, 0.79041916, 0.77906977, 0.4       ,\n",
       "        0.76785714, 0.28110599, 0.4829932 , 0.03389831, 0.2254902 ,\n",
       "        0.4939759 , 0.3554007 , 0.36075949, 0.35714286, 0.05194805,\n",
       "        0.61748634, 0.47058824, 0.55392157, 0.66438356, 0.03846154,\n",
       "        0.47263682, 0.61271676, 0.03409091, 0.43023256, 0.3447205 ,\n",
       "        0.44303797, 0.60465116, 0.60402685, 0.26470588, 0.43459916,\n",
       "        0.23555556, 0.4028777 , 0.50990099, 0.5433526 , 0.425     ,\n",
       "        0.04294479, 0.62275449, 0.49253731, 0.41935484, 0.03649635,\n",
       "        0.43644068, 0.3190184 , 0.49509804, 0.29292929, 0.06722689,\n",
       "        0.4573991 , 0.04347826, 0.49268293, 0.52432432, 0.57303371,\n",
       "        0.04519774, 0.57954545, 0.43783784, 0.57303371, 0.33755274,\n",
       "        0.20863309, 0.76363636, 0.6035503 , 0.4063745 , 0.04069767,\n",
       "        0.41013825, 0.76691729, 0.41975309, 0.38497653, 0.04060914,\n",
       "        0.63803681, 0.04571429, 0.32846715, 0.4063745 , 0.40410959],\n",
       "       [0.21848739, 0.21848739, 0.22985075, 1.62184874, 0.43406593,\n",
       "        0.04166667, 0.61904762, 0.26351351, 0.02054795, 0.50393701,\n",
       "        0.31950207, 0.03977273, 0.32478632, 0.26540284, 0.3681592 ,\n",
       "        0.38071066, 0.33168317, 0.05673759, 0.4198895 , 0.38860104,\n",
       "        0.35483871, 0.34562212, 0.4491018 , 0.04651163, 0.53571429,\n",
       "        0.44047619, 0.34562212, 0.31972789, 0.42372881, 0.3627451 ,\n",
       "        0.44578313, 0.22648084, 0.23734177, 0.04081633, 0.46753247,\n",
       "        0.40437158, 0.30672269, 0.36764706, 0.47260274, 0.35576923,\n",
       "        0.33333333, 0.03468208, 0.42045455, 0.18992248, 0.23291925,\n",
       "        0.3164557 , 0.43023256, 0.05369128, 0.31512605, 0.3164557 ,\n",
       "        0.33333333, 0.33093525, 0.37128713, 0.43352601, 0.3125    ,\n",
       "        0.28834356, 0.4491018 , 0.3681592 , 0.18951613, 0.04379562,\n",
       "        0.31355932, 0.32515337, 0.03921569, 0.37878788, 0.05042017,\n",
       "        0.33632287, 0.52898551, 0.36585366, 0.41081081, 0.42134831,\n",
       "        0.03954802, 0.42613636, 0.37297297, 0.42696629, 0.32067511,\n",
       "        0.02517986, 0.05454545, 0.4556213 , 0.30677291, 0.04651163,\n",
       "        0.35023041, 0.37593985, 0.31687243, 0.37558685, 0.32994924,\n",
       "        0.74846626, 0.04      , 0.05109489, 0.52988048, 0.39726027],\n",
       "       [0.15406162, 0.1512605 , 0.1641791 , 0.31932773, 0.3021978 ,\n",
       "        0.32738095, 0.42857143, 0.18243243, 0.18493151, 0.42519685,\n",
       "        0.0373444 , 0.30113636, 0.23076923, 0.25592417, 0.        ,\n",
       "        0.26903553, 0.26237624, 0.05673759, 0.29834254, 0.27979275,\n",
       "        0.28494624, 0.24423963, 0.32335329, 0.30813953, 0.38571429,\n",
       "        0.32142857, 0.24423963, 0.36054422, 0.30508475, 0.25980392,\n",
       "        0.31927711, 0.18466899, 0.16772152, 0.27040816, 0.34415584,\n",
       "        0.27322404, 0.22268908, 0.25980392, 0.36986301, 0.25480769,\n",
       "        0.26368159, 0.04624277, 0.30113636, 0.20542636, 0.16459627,\n",
       "        0.03375527, 0.31395349, 0.36241611, 0.22689076, 0.2278481 ,\n",
       "        0.24      , 0.05755396, 0.26732673, 0.29479769, 0.225     ,\n",
       "        0.34355828, 0.32934132, 0.27363184, 0.22177419, 0.05839416,\n",
       "        0.23728814, 0.32515337, 0.27941176, 0.29292929, 0.06722689,\n",
       "        0.26008969, 0.36231884, 0.32195122, 0.34594595, 0.35955056,\n",
       "        0.36158192, 0.30681818, 0.35675676, 0.37078652, 0.27004219,\n",
       "        0.23381295, 0.58181818, 0.04733728, 0.25099602, 0.29069767,\n",
       "        0.29493088, 0.46616541, 0.25925926, 0.29107981, 0.31979695,\n",
       "        0.38650307, 0.36      , 0.05109489, 0.25099602, 0.05479452],\n",
       "       [0.16806723, 0.16806723, 0.1761194 , 0.4789916 , 0.32417582,\n",
       "        0.35119048, 0.06349206, 0.19932432, 0.19520548, 0.45669291,\n",
       "        0.02904564, 0.32954545, 0.24786325, 0.26540284, 0.28358209,\n",
       "        0.29441624, 0.03960396, 0.04255319, 0.26519337, 0.30051813,\n",
       "        0.30107527, 0.26728111, 0.34131737, 0.30232558, 0.40714286,\n",
       "        0.3452381 , 0.2718894 , 0.40136054, 0.3220339 , 0.27941176,\n",
       "        0.3313253 , 0.20209059, 0.17721519, 0.30102041, 0.37662338,\n",
       "        0.31693989, 0.24789916, 0.24019608, 0.4109589 , 0.03846154,\n",
       "        0.29850746, 0.04046243, 0.04545455, 0.03100775, 0.20807453,\n",
       "        0.03375527, 0.55813953, 0.48322148, 0.3907563 , 0.03375527,\n",
       "        0.36      , 0.05035971, 0.46039604, 0.04624277, 0.4       ,\n",
       "        0.36809816, 0.04790419, 0.47761194, 0.39112903, 0.05109489,\n",
       "        0.22457627, 0.34969325, 0.43627451, 0.53535354, 0.47058824,\n",
       "        0.47533632, 0.05797101, 0.31219512, 0.33513514, 0.35393258,\n",
       "        0.32768362, 0.35795455, 0.31351351, 0.34831461, 0.26160338,\n",
       "        0.21942446, 0.54545455, 0.35502959, 0.17928287, 0.26744186,\n",
       "        0.28110599, 0.05263158, 0.19341564, 0.28638498, 0.30456853,\n",
       "        0.36809816, 0.03428571, 0.4379562 , 0.23904382, 0.33561644],\n",
       "       [0.18767507, 0.18487395, 0.2       , 0.56302521, 0.36263736,\n",
       "        0.4047619 , 0.05555556, 0.22635135, 0.22945205, 0.51968504,\n",
       "        0.02904564, 0.36931818, 0.28632479, 0.30805687, 0.29353234,\n",
       "        0.32994924, 0.32673267, 0.4751773 , 0.37016575, 0.34196891,\n",
       "        0.27956989, 0.23963134, 0.39520958, 0.40116279, 0.48571429,\n",
       "        0.4047619 , 0.03686636, 0.32653061, 0.32768362, 0.29901961,\n",
       "        0.40963855, 0.18466899, 0.16455696, 0.27040816, 0.04545455,\n",
       "        0.04371585, 0.21848739, 0.25      , 0.34931507, 0.23557692,\n",
       "        0.25373134, 0.29479769, 0.29545455, 0.19379845, 0.15838509,\n",
       "        0.20675105, 0.28488372, 0.34899329, 0.21008403, 0.21518987,\n",
       "        0.23111111, 0.05035971, 0.25742574, 0.30057803, 0.2125    ,\n",
       "        0.04294479, 0.2994012 , 0.25373134, 0.19758065, 0.05109489,\n",
       "        0.21610169, 0.31288344, 0.03431373, 0.26262626, 0.42016807,\n",
       "        0.22869955, 0.36956522, 0.25365854, 0.27567568, 0.29213483,\n",
       "        0.29378531, 0.29545455, 0.03783784, 0.03370787, 0.22362869,\n",
       "        0.22661871, 0.05454545, 0.37278107, 0.25498008, 0.37209302,\n",
       "        0.04147465, 0.36842105, 0.24691358, 0.22535211, 0.30964467,\n",
       "        0.39263804, 0.03428571, 0.39416058, 0.25498008, 0.04794521],\n",
       "       [0.232493  , 0.22969188, 0.23880597, 0.58823529, 0.33516484,\n",
       "        0.48809524, 0.64285714, 0.02364865, 0.23630137, 0.05511811,\n",
       "        0.28215768, 0.30681818, 0.28632479, 0.03791469, 0.02985075,\n",
       "        0.30456853, 0.02970297, 0.43971631, 0.34254144, 0.31088083,\n",
       "        0.31182796, 0.02764977, 0.35329341, 0.34883721, 0.42857143,\n",
       "        0.35119048, 0.03686636, 0.40136054, 0.33898305, 0.25490196,\n",
       "        0.3253012 , 0.20209059, 0.18670886, 0.30102041, 0.38311688,\n",
       "        0.32786885, 0.24789916, 0.29901961, 0.41780822, 0.26442308,\n",
       "        0.30845771, 0.04624277, 0.34090909, 0.03100775, 0.18322981,\n",
       "        0.20675105, 0.35465116, 0.40268456, 0.24789916, 0.24894515,\n",
       "        0.26222222, 0.36690647, 0.26237624, 0.28901734, 0.22083333,\n",
       "        0.03680982, 0.04790419, 0.25870647, 0.33064516, 0.35036496,\n",
       "        0.44915254, 0.3190184 , 0.25490196, 0.25757576, 0.42016807,\n",
       "        0.51569507, 0.34782609, 0.55609756, 0.32972973, 0.64606742,\n",
       "        0.28248588, 0.64772727, 0.26486486, 0.64606742, 0.42616034,\n",
       "        0.29496403, 0.45454545, 0.67455621, 0.45418327, 0.35465116,\n",
       "        0.28110599, 0.45864662, 0.26337449, 0.28169014, 0.27411168,\n",
       "        0.28220859, 0.04      , 0.35036496, 0.187251  , 0.30821918],\n",
       "       [0.22689076, 0.22969188, 0.24179104, 0.58823529, 0.45054945,\n",
       "        0.48809524, 0.05555556, 0.27364865, 0.27739726, 0.66141732,\n",
       "        0.34024896, 0.39204545, 0.34615385, 0.03791469, 0.22885572,\n",
       "        0.39593909, 0.39108911, 0.5035461 , 0.27624309, 0.39896373,\n",
       "        0.3172043 , 0.2764977 , 0.35329341, 0.44767442, 0.54285714,\n",
       "        0.45238095, 0.28571429, 0.51020408, 0.03389831, 0.37254902,\n",
       "        0.45783133, 0.26480836, 0.18037975, 0.33163265, 0.48701299,\n",
       "        0.41530055, 0.2394958 , 0.37254902, 0.51369863, 0.36538462,\n",
       "        0.37810945, 0.34104046, 0.43181818, 0.02713178, 0.23291925,\n",
       "        0.02531646, 0.28488372, 0.48322148, 0.21428571, 0.29957806,\n",
       "        0.28888889, 0.35251799, 0.37128713, 0.43352601, 0.22083333,\n",
       "        0.05521472, 0.45508982, 0.3681592 , 0.29435484, 0.05109489,\n",
       "        0.30932203, 0.44785276, 0.24509804, 0.36868687, 0.49579832,\n",
       "        0.32735426, 0.05072464, 0.35609756, 0.38918919, 0.40449438,\n",
       "        0.41242938, 0.40909091, 0.04864865, 0.38764045, 0.30379747,\n",
       "        0.21582734, 0.50909091, 0.04142012, 0.28685259, 0.28488372,\n",
       "        0.32258065, 0.34586466, 0.02469136, 0.32394366, 0.35532995,\n",
       "        0.42944785, 0.04      , 0.05109489, 0.28685259, 0.49315068]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = model.predict_classes(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
