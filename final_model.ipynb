{
 "cells": [
  {
   "source": [
    "# Data Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\admin\\Desktop\\Door-Detection\\Door-Detection\\dataset_289_samples_final_3am.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Distance0</th>\n",
       "      <th>Distance1</th>\n",
       "      <th>Distance2</th>\n",
       "      <th>Distance3</th>\n",
       "      <th>Distance4</th>\n",
       "      <th>Distance5</th>\n",
       "      <th>Distance6</th>\n",
       "      <th>Distance7</th>\n",
       "      <th>Distance8</th>\n",
       "      <th>...</th>\n",
       "      <th>Distance80</th>\n",
       "      <th>Distance81</th>\n",
       "      <th>Distance82</th>\n",
       "      <th>Distance83</th>\n",
       "      <th>Distance84</th>\n",
       "      <th>Distance85</th>\n",
       "      <th>Distance86</th>\n",
       "      <th>Distance87</th>\n",
       "      <th>Distance88</th>\n",
       "      <th>Distance89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>178</td>\n",
       "      <td>114</td>\n",
       "      <td>179</td>\n",
       "      <td>97</td>\n",
       "      <td>67</td>\n",
       "      <td>130</td>\n",
       "      <td>73</td>\n",
       "      <td>132</td>\n",
       "      <td>93</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>89</td>\n",
       "      <td>90</td>\n",
       "      <td>57</td>\n",
       "      <td>54</td>\n",
       "      <td>48</td>\n",
       "      <td>68</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>176</td>\n",
       "      <td>178</td>\n",
       "      <td>33</td>\n",
       "      <td>24</td>\n",
       "      <td>96</td>\n",
       "      <td>62</td>\n",
       "      <td>103</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>84</td>\n",
       "      <td>90</td>\n",
       "      <td>89</td>\n",
       "      <td>63</td>\n",
       "      <td>51</td>\n",
       "      <td>88</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>83</td>\n",
       "      <td>68</td>\n",
       "      <td>86</td>\n",
       "      <td>60</td>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>7</td>\n",
       "      <td>53</td>\n",
       "      <td>64</td>\n",
       "      <td>76</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "      <td>99</td>\n",
       "      <td>80</td>\n",
       "      <td>98</td>\n",
       "      <td>93</td>\n",
       "      <td>64</td>\n",
       "      <td>94</td>\n",
       "      <td>98</td>\n",
       "      <td>...</td>\n",
       "      <td>87</td>\n",
       "      <td>68</td>\n",
       "      <td>117</td>\n",
       "      <td>97</td>\n",
       "      <td>120</td>\n",
       "      <td>119</td>\n",
       "      <td>73</td>\n",
       "      <td>80</td>\n",
       "      <td>119</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>69</td>\n",
       "      <td>64</td>\n",
       "      <td>68</td>\n",
       "      <td>58</td>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>103</td>\n",
       "      <td>132</td>\n",
       "      <td>74</td>\n",
       "      <td>72</td>\n",
       "      <td>65</td>\n",
       "      <td>58</td>\n",
       "      <td>72</td>\n",
       "      <td>55</td>\n",
       "      <td>71</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0</td>\n",
       "      <td>357</td>\n",
       "      <td>192</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>90</td>\n",
       "      <td>108</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>7</td>\n",
       "      <td>115</td>\n",
       "      <td>89</td>\n",
       "      <td>98</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>103</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Type  Distance0  Distance1  Distance2  Distance3  Distance4  Distance5  \\\n",
       "0       0        178        114        179         97         67        130   \n",
       "1       0        179        176        178         33         24         96   \n",
       "2       1         86         86         86         83         68         86   \n",
       "3       1         99        100         99         80         98         93   \n",
       "4       1         70         70         69         64         68         58   \n",
       "..    ...        ...        ...        ...        ...        ...        ...   \n",
       "283     0        357        192          7          7          7         90   \n",
       "284     0          7          7          7          7          7          7   \n",
       "285     0        116          7        115         89         98          7   \n",
       "286     0         51         51         51          8         38         51   \n",
       "287     0         42         42         42          7          7          8   \n",
       "\n",
       "     Distance6  Distance7  Distance8  ...  Distance80  Distance81  Distance82  \\\n",
       "0           73        132         93  ...          92          89          90   \n",
       "1           62        103         75  ...          90          84          90   \n",
       "2           60         85         86  ...          57          76          76   \n",
       "3           64         94         98  ...          87          68         117   \n",
       "4            7         70         69  ...         103         132          74   \n",
       "..         ...        ...        ...  ...         ...         ...         ...   \n",
       "283        108          6          7  ...           7           7           9   \n",
       "284          7          8          8  ...           7           7           7   \n",
       "285          6          7        103  ...           6           7           7   \n",
       "286         51          6          8  ...           8           9           7   \n",
       "287          8          6          8  ...           7           7           9   \n",
       "\n",
       "     Distance83  Distance84  Distance85  Distance86  Distance87  Distance88  \\\n",
       "0            57          54          48          68          89          89   \n",
       "1            89          63          51          88          90          85   \n",
       "2            72          76           7          53          64          76   \n",
       "3            97         120         119          73          80         119   \n",
       "4            72          65          58          72          55          71   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "283           7           7           7           7           7           7   \n",
       "284           7           7           6           7           9           6   \n",
       "285           9           9           7           7           7           8   \n",
       "286           7           7           7           9           9           7   \n",
       "287           7           7           7           7           7           7   \n",
       "\n",
       "     Distance89  \n",
       "0          61.0  \n",
       "1          74.0  \n",
       "2          77.0  \n",
       "3          62.0  \n",
       "4          69.0  \n",
       "..          ...  \n",
       "283         7.0  \n",
       "284         7.0  \n",
       "285         7.0  \n",
       "286         7.0  \n",
       "287         7.0  \n",
       "\n",
       "[288 rows x 91 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 288 entries, 0 to 287\n",
      "Data columns (total 91 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Type        288 non-null    int64  \n",
      " 1   Distance0   288 non-null    int64  \n",
      " 2   Distance1   288 non-null    int64  \n",
      " 3   Distance2   288 non-null    int64  \n",
      " 4   Distance3   288 non-null    int64  \n",
      " 5   Distance4   288 non-null    int64  \n",
      " 6   Distance5   288 non-null    int64  \n",
      " 7   Distance6   288 non-null    int64  \n",
      " 8   Distance7   288 non-null    int64  \n",
      " 9   Distance8   288 non-null    int64  \n",
      " 10  Distance9   288 non-null    int64  \n",
      " 11  Distance10  288 non-null    int64  \n",
      " 12  Distance11  288 non-null    int64  \n",
      " 13  Distance12  288 non-null    int64  \n",
      " 14  Distance13  288 non-null    int64  \n",
      " 15  Distance14  288 non-null    int64  \n",
      " 16  Distance15  288 non-null    int64  \n",
      " 17  Distance16  288 non-null    int64  \n",
      " 18  Distance17  288 non-null    int64  \n",
      " 19  Distance18  288 non-null    int64  \n",
      " 20  Distance19  288 non-null    int64  \n",
      " 21  Distance20  288 non-null    int64  \n",
      " 22  Distance21  288 non-null    int64  \n",
      " 23  Distance22  288 non-null    int64  \n",
      " 24  Distance23  288 non-null    int64  \n",
      " 25  Distance24  288 non-null    int64  \n",
      " 26  Distance25  288 non-null    int64  \n",
      " 27  Distance26  288 non-null    int64  \n",
      " 28  Distance27  288 non-null    int64  \n",
      " 29  Distance28  288 non-null    int64  \n",
      " 30  Distance29  288 non-null    int64  \n",
      " 31  Distance30  288 non-null    int64  \n",
      " 32  Distance31  288 non-null    int64  \n",
      " 33  Distance32  288 non-null    int64  \n",
      " 34  Distance33  288 non-null    int64  \n",
      " 35  Distance34  288 non-null    int64  \n",
      " 36  Distance35  288 non-null    int64  \n",
      " 37  Distance36  288 non-null    int64  \n",
      " 38  Distance37  288 non-null    int64  \n",
      " 39  Distance38  288 non-null    int64  \n",
      " 40  Distance39  288 non-null    int64  \n",
      " 41  Distance40  288 non-null    int64  \n",
      " 42  Distance41  288 non-null    int64  \n",
      " 43  Distance42  288 non-null    int64  \n",
      " 44  Distance43  288 non-null    int64  \n",
      " 45  Distance44  288 non-null    int64  \n",
      " 46  Distance45  288 non-null    int64  \n",
      " 47  Distance46  288 non-null    int64  \n",
      " 48  Distance47  288 non-null    int64  \n",
      " 49  Distance48  288 non-null    int64  \n",
      " 50  Distance49  288 non-null    int64  \n",
      " 51  Distance50  288 non-null    int64  \n",
      " 52  Distance51  288 non-null    int64  \n",
      " 53  Distance52  288 non-null    int64  \n",
      " 54  Distance53  288 non-null    int64  \n",
      " 55  Distance54  288 non-null    int64  \n",
      " 56  Distance55  288 non-null    int64  \n",
      " 57  Distance56  288 non-null    int64  \n",
      " 58  Distance57  288 non-null    int64  \n",
      " 59  Distance58  288 non-null    int64  \n",
      " 60  Distance59  288 non-null    int64  \n",
      " 61  Distance60  288 non-null    int64  \n",
      " 62  Distance61  288 non-null    int64  \n",
      " 63  Distance62  288 non-null    int64  \n",
      " 64  Distance63  288 non-null    int64  \n",
      " 65  Distance64  288 non-null    int64  \n",
      " 66  Distance65  288 non-null    int64  \n",
      " 67  Distance66  288 non-null    int64  \n",
      " 68  Distance67  288 non-null    int64  \n",
      " 69  Distance68  288 non-null    int64  \n",
      " 70  Distance69  288 non-null    int64  \n",
      " 71  Distance70  288 non-null    int64  \n",
      " 72  Distance71  288 non-null    int64  \n",
      " 73  Distance72  288 non-null    int64  \n",
      " 74  Distance73  288 non-null    int64  \n",
      " 75  Distance74  288 non-null    int64  \n",
      " 76  Distance75  288 non-null    int64  \n",
      " 77  Distance76  288 non-null    int64  \n",
      " 78  Distance77  288 non-null    int64  \n",
      " 79  Distance78  288 non-null    int64  \n",
      " 80  Distance79  288 non-null    int64  \n",
      " 81  Distance80  288 non-null    int64  \n",
      " 82  Distance81  288 non-null    int64  \n",
      " 83  Distance82  288 non-null    int64  \n",
      " 84  Distance83  288 non-null    int64  \n",
      " 85  Distance84  288 non-null    int64  \n",
      " 86  Distance85  288 non-null    int64  \n",
      " 87  Distance86  288 non-null    int64  \n",
      " 88  Distance87  288 non-null    int64  \n",
      " 89  Distance88  288 non-null    int64  \n",
      " 90  Distance89  287 non-null    float64\n",
      "dtypes: float64(1), int64(90)\n",
      "memory usage: 204.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Distance0</th>\n",
       "      <th>Distance1</th>\n",
       "      <th>Distance2</th>\n",
       "      <th>Distance3</th>\n",
       "      <th>Distance4</th>\n",
       "      <th>Distance5</th>\n",
       "      <th>Distance6</th>\n",
       "      <th>Distance7</th>\n",
       "      <th>Distance8</th>\n",
       "      <th>...</th>\n",
       "      <th>Distance80</th>\n",
       "      <th>Distance81</th>\n",
       "      <th>Distance82</th>\n",
       "      <th>Distance83</th>\n",
       "      <th>Distance84</th>\n",
       "      <th>Distance85</th>\n",
       "      <th>Distance86</th>\n",
       "      <th>Distance87</th>\n",
       "      <th>Distance88</th>\n",
       "      <th>Distance89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>287.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.524306</td>\n",
       "      <td>109.055556</td>\n",
       "      <td>99.204861</td>\n",
       "      <td>95.829861</td>\n",
       "      <td>48.965278</td>\n",
       "      <td>48.302083</td>\n",
       "      <td>46.833333</td>\n",
       "      <td>39.135417</td>\n",
       "      <td>41.434028</td>\n",
       "      <td>65.809028</td>\n",
       "      <td>...</td>\n",
       "      <td>39.538194</td>\n",
       "      <td>29.284722</td>\n",
       "      <td>35.100694</td>\n",
       "      <td>42.006944</td>\n",
       "      <td>47.836806</td>\n",
       "      <td>39.031250</td>\n",
       "      <td>29.625000</td>\n",
       "      <td>27.548611</td>\n",
       "      <td>47.718750</td>\n",
       "      <td>24.303136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500278</td>\n",
       "      <td>78.840975</td>\n",
       "      <td>66.444625</td>\n",
       "      <td>61.873330</td>\n",
       "      <td>36.720797</td>\n",
       "      <td>40.261632</td>\n",
       "      <td>40.052666</td>\n",
       "      <td>32.076331</td>\n",
       "      <td>45.154120</td>\n",
       "      <td>47.830220</td>\n",
       "      <td>...</td>\n",
       "      <td>44.957029</td>\n",
       "      <td>32.836765</td>\n",
       "      <td>39.742029</td>\n",
       "      <td>42.463839</td>\n",
       "      <td>45.761361</td>\n",
       "      <td>56.471346</td>\n",
       "      <td>31.740317</td>\n",
       "      <td>29.549894</td>\n",
       "      <td>47.941174</td>\n",
       "      <td>28.374779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>58.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>83.500000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>121.250000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>75.250000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>63.250000</td>\n",
       "      <td>96.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>70.250000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>64.250000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>52.250000</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>47.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>217.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>777.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>146.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Type   Distance0   Distance1   Distance2   Distance3   Distance4  \\\n",
       "count  288.000000  288.000000  288.000000  288.000000  288.000000  288.000000   \n",
       "mean     0.524306  109.055556   99.204861   95.829861   48.965278   48.302083   \n",
       "std      0.500278   78.840975   66.444625   61.873330   36.720797   40.261632   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000   60.000000   58.500000   59.000000    7.750000    7.000000   \n",
       "50%      1.000000   93.000000   90.000000   83.500000   54.000000   51.000000   \n",
       "75%      1.000000  137.000000  127.250000  121.250000   82.000000   75.250000   \n",
       "max      1.000000  357.000000  357.000000  335.000000  161.000000  182.000000   \n",
       "\n",
       "        Distance5   Distance6   Distance7   Distance8  ...  Distance80  \\\n",
       "count  288.000000  288.000000  288.000000  288.000000  ...  288.000000   \n",
       "mean    46.833333   39.135417   41.434028   65.809028  ...   39.538194   \n",
       "std     40.052666   32.076331   45.154120   47.830220  ...   44.957029   \n",
       "min      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "25%      7.000000    7.000000    7.000000    9.000000  ...    7.000000   \n",
       "50%     53.500000   44.000000   13.000000   66.500000  ...    8.000000   \n",
       "75%     75.000000   62.000000   63.250000   96.250000  ...   70.250000   \n",
       "max    168.000000  143.000000  296.000000  292.000000  ...  217.000000   \n",
       "\n",
       "       Distance81  Distance82  Distance83  Distance84  Distance85  Distance86  \\\n",
       "count  288.000000  288.000000  288.000000  288.000000  288.000000  288.000000   \n",
       "mean    29.284722   35.100694   42.006944   47.836806   39.031250   29.625000   \n",
       "std     32.836765   39.742029   42.463839   45.761361   56.471346   31.740317   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      7.000000    7.000000    8.000000    8.000000    7.000000    7.000000   \n",
       "50%      8.000000    8.000000    9.000000   21.000000    8.000000    8.000000   \n",
       "75%     51.000000   64.250000   76.000000   84.000000   68.000000   55.000000   \n",
       "max    133.000000  243.000000  213.000000  197.000000  777.000000  175.000000   \n",
       "\n",
       "       Distance87  Distance88  Distance89  \n",
       "count  288.000000  288.000000  287.000000  \n",
       "mean    27.548611   47.718750   24.303136  \n",
       "std     29.549894   47.941174   28.374779  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      7.000000    7.000000    7.000000  \n",
       "50%      8.000000    9.000000    8.000000  \n",
       "75%     52.250000   80.500000   47.500000  \n",
       "max    137.000000  251.000000  146.000000  \n",
       "\n",
       "[8 rows x 91 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Distance0</th>\n",
       "      <th>Distance1</th>\n",
       "      <th>Distance2</th>\n",
       "      <th>Distance3</th>\n",
       "      <th>Distance4</th>\n",
       "      <th>Distance5</th>\n",
       "      <th>Distance6</th>\n",
       "      <th>Distance7</th>\n",
       "      <th>Distance8</th>\n",
       "      <th>...</th>\n",
       "      <th>Distance80</th>\n",
       "      <th>Distance81</th>\n",
       "      <th>Distance82</th>\n",
       "      <th>Distance83</th>\n",
       "      <th>Distance84</th>\n",
       "      <th>Distance85</th>\n",
       "      <th>Distance86</th>\n",
       "      <th>Distance87</th>\n",
       "      <th>Distance88</th>\n",
       "      <th>Distance89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>214</td>\n",
       "      <td>103</td>\n",
       "      <td>200</td>\n",
       "      <td>99</td>\n",
       "      <td>131</td>\n",
       "      <td>71</td>\n",
       "      <td>56</td>\n",
       "      <td>209</td>\n",
       "      <td>73</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>777</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>162</td>\n",
       "      <td>164</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>58</td>\n",
       "      <td>94</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>67</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>150</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>74</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>324</td>\n",
       "      <td>117</td>\n",
       "      <td>196</td>\n",
       "      <td>61</td>\n",
       "      <td>71</td>\n",
       "      <td>116</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>92</td>\n",
       "      <td>83</td>\n",
       "      <td>82</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>8</td>\n",
       "      <td>61</td>\n",
       "      <td>65</td>\n",
       "      <td>57</td>\n",
       "      <td>52</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>108</td>\n",
       "      <td>115</td>\n",
       "      <td>113</td>\n",
       "      <td>94</td>\n",
       "      <td>68</td>\n",
       "      <td>59</td>\n",
       "      <td>55</td>\n",
       "      <td>93</td>\n",
       "      <td>...</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>71</td>\n",
       "      <td>8</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>7</td>\n",
       "      <td>59</td>\n",
       "      <td>95</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>118</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>77</td>\n",
       "      <td>80</td>\n",
       "      <td>79</td>\n",
       "      <td>74</td>\n",
       "      <td>79</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>102</td>\n",
       "      <td>97</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>71</td>\n",
       "      <td>77</td>\n",
       "      <td>85</td>\n",
       "      <td>84</td>\n",
       "      <td>149</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Type  Distance0  Distance1  Distance2  Distance3  Distance4  Distance5  \\\n",
       "0       0        214        103        200         99        131         71   \n",
       "1       1        163        162        164          7          8         58   \n",
       "2       0          6          6          6          7          7          7   \n",
       "3       0        324        117        196         61         71        116   \n",
       "4       0         93         93         92         83         82          7   \n",
       "..    ...        ...        ...        ...        ...        ...        ...   \n",
       "283     0         65         65         65          8         61         65   \n",
       "284     1        115        108        115        113         94         68   \n",
       "285     0         28          7          7          7          7          8   \n",
       "286     0        119        119        118          8          7          6   \n",
       "287     1         79         79         80         77         80         79   \n",
       "\n",
       "     Distance6  Distance7  Distance8  ...  Distance80  Distance81  Distance82  \\\n",
       "0           56        209         73  ...           7           7           6   \n",
       "1           94          8          8  ...           7           6           6   \n",
       "2            6          5          7  ...          74           9           8   \n",
       "3            7          7        111  ...           6           6           8   \n",
       "4           49          7          7  ...           7          82           9   \n",
       "..         ...        ...        ...  ...         ...         ...         ...   \n",
       "283         57         52          8  ...           7           7           7   \n",
       "284         59         55         93  ...          96          96          71   \n",
       "285          6         28         27  ...           7           7           9   \n",
       "286          8         46         90  ...           9           7           7   \n",
       "287         74         79         56  ...         102          97         100   \n",
       "\n",
       "     Distance83  Distance84  Distance85  Distance86  Distance87  Distance88  \\\n",
       "0             7           8         777           7           6           9   \n",
       "1            67           8           8           8           6         150   \n",
       "2             8           8           8           6           6           7   \n",
       "3             8           6           8           8           9           8   \n",
       "4             8           8           8           8           7           6   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "283           7           8           7           6           6           7   \n",
       "284           8          96          96           7          59          95   \n",
       "285           7           7           7           7           7           7   \n",
       "286           8           8          36           7          37          36   \n",
       "287          80          71          77          85          84         149   \n",
       "\n",
       "     Distance89  \n",
       "0           NaN  \n",
       "1           8.0  \n",
       "2           8.0  \n",
       "3           8.0  \n",
       "4          69.0  \n",
       "..          ...  \n",
       "283         7.0  \n",
       "284        55.0  \n",
       "285         7.0  \n",
       "286         8.0  \n",
       "287        85.0  \n",
       "\n",
       "[288 rows x 91 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffling the data\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.fillna(df.mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 288 entries, 0 to 287\n",
      "Data columns (total 91 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Type        288 non-null    int64  \n",
      " 1   Distance0   288 non-null    int64  \n",
      " 2   Distance1   288 non-null    int64  \n",
      " 3   Distance2   288 non-null    int64  \n",
      " 4   Distance3   288 non-null    int64  \n",
      " 5   Distance4   288 non-null    int64  \n",
      " 6   Distance5   288 non-null    int64  \n",
      " 7   Distance6   288 non-null    int64  \n",
      " 8   Distance7   288 non-null    int64  \n",
      " 9   Distance8   288 non-null    int64  \n",
      " 10  Distance9   288 non-null    int64  \n",
      " 11  Distance10  288 non-null    int64  \n",
      " 12  Distance11  288 non-null    int64  \n",
      " 13  Distance12  288 non-null    int64  \n",
      " 14  Distance13  288 non-null    int64  \n",
      " 15  Distance14  288 non-null    int64  \n",
      " 16  Distance15  288 non-null    int64  \n",
      " 17  Distance16  288 non-null    int64  \n",
      " 18  Distance17  288 non-null    int64  \n",
      " 19  Distance18  288 non-null    int64  \n",
      " 20  Distance19  288 non-null    int64  \n",
      " 21  Distance20  288 non-null    int64  \n",
      " 22  Distance21  288 non-null    int64  \n",
      " 23  Distance22  288 non-null    int64  \n",
      " 24  Distance23  288 non-null    int64  \n",
      " 25  Distance24  288 non-null    int64  \n",
      " 26  Distance25  288 non-null    int64  \n",
      " 27  Distance26  288 non-null    int64  \n",
      " 28  Distance27  288 non-null    int64  \n",
      " 29  Distance28  288 non-null    int64  \n",
      " 30  Distance29  288 non-null    int64  \n",
      " 31  Distance30  288 non-null    int64  \n",
      " 32  Distance31  288 non-null    int64  \n",
      " 33  Distance32  288 non-null    int64  \n",
      " 34  Distance33  288 non-null    int64  \n",
      " 35  Distance34  288 non-null    int64  \n",
      " 36  Distance35  288 non-null    int64  \n",
      " 37  Distance36  288 non-null    int64  \n",
      " 38  Distance37  288 non-null    int64  \n",
      " 39  Distance38  288 non-null    int64  \n",
      " 40  Distance39  288 non-null    int64  \n",
      " 41  Distance40  288 non-null    int64  \n",
      " 42  Distance41  288 non-null    int64  \n",
      " 43  Distance42  288 non-null    int64  \n",
      " 44  Distance43  288 non-null    int64  \n",
      " 45  Distance44  288 non-null    int64  \n",
      " 46  Distance45  288 non-null    int64  \n",
      " 47  Distance46  288 non-null    int64  \n",
      " 48  Distance47  288 non-null    int64  \n",
      " 49  Distance48  288 non-null    int64  \n",
      " 50  Distance49  288 non-null    int64  \n",
      " 51  Distance50  288 non-null    int64  \n",
      " 52  Distance51  288 non-null    int64  \n",
      " 53  Distance52  288 non-null    int64  \n",
      " 54  Distance53  288 non-null    int64  \n",
      " 55  Distance54  288 non-null    int64  \n",
      " 56  Distance55  288 non-null    int64  \n",
      " 57  Distance56  288 non-null    int64  \n",
      " 58  Distance57  288 non-null    int64  \n",
      " 59  Distance58  288 non-null    int64  \n",
      " 60  Distance59  288 non-null    int64  \n",
      " 61  Distance60  288 non-null    int64  \n",
      " 62  Distance61  288 non-null    int64  \n",
      " 63  Distance62  288 non-null    int64  \n",
      " 64  Distance63  288 non-null    int64  \n",
      " 65  Distance64  288 non-null    int64  \n",
      " 66  Distance65  288 non-null    int64  \n",
      " 67  Distance66  288 non-null    int64  \n",
      " 68  Distance67  288 non-null    int64  \n",
      " 69  Distance68  288 non-null    int64  \n",
      " 70  Distance69  288 non-null    int64  \n",
      " 71  Distance70  288 non-null    int64  \n",
      " 72  Distance71  288 non-null    int64  \n",
      " 73  Distance72  288 non-null    int64  \n",
      " 74  Distance73  288 non-null    int64  \n",
      " 75  Distance74  288 non-null    int64  \n",
      " 76  Distance75  288 non-null    int64  \n",
      " 77  Distance76  288 non-null    int64  \n",
      " 78  Distance77  288 non-null    int64  \n",
      " 79  Distance78  288 non-null    int64  \n",
      " 80  Distance79  288 non-null    int64  \n",
      " 81  Distance80  288 non-null    int64  \n",
      " 82  Distance81  288 non-null    int64  \n",
      " 83  Distance82  288 non-null    int64  \n",
      " 84  Distance83  288 non-null    int64  \n",
      " 85  Distance84  288 non-null    int64  \n",
      " 86  Distance85  288 non-null    int64  \n",
      " 87  Distance86  288 non-null    int64  \n",
      " 88  Distance87  288 non-null    int64  \n",
      " 89  Distance88  288 non-null    int64  \n",
      " 90  Distance89  288 non-null    float64\n",
      "dtypes: float64(1), int64(90)\n",
      "memory usage: 204.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "source": [
    "# Model Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('Type', axis = 1).values\n",
    "y = df['Type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[214.        , 103.        , 200.        , ...,   6.        ,\n",
       "          9.        ,  24.30313589],\n",
       "       [163.        , 162.        , 164.        , ...,   6.        ,\n",
       "        150.        ,   8.        ],\n",
       "       [  6.        ,   6.        ,   6.        , ...,   6.        ,\n",
       "          7.        ,   8.        ],\n",
       "       ...,\n",
       "       [ 28.        ,   7.        ,   7.        , ...,   7.        ,\n",
       "          7.        ,   7.        ],\n",
       "       [119.        , 119.        , 118.        , ...,  37.        ,\n",
       "         36.        ,   8.        ],\n",
       "       [ 79.        ,  79.        ,  80.        , ...,  84.        ,\n",
       "        149.        ,  85.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 1], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train =  (216, 90)\n",
      "Shape of y_train =  (216,)\n",
      "Shape of X_test =  (72, 90)\n",
      "Shape of y_test =  (72,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train = \", X_train.shape)\n",
    "print(\"Shape of y_train = \", y_train.shape)\n",
    "print(\"Shape of X_test = \", X_test.shape)\n",
    "print(\"Shape of y_test = \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[237., 115., 160., ...,   7.,  45.,  59.],\n",
       "       [180., 180., 181., ...,   8.,   6.,   9.],\n",
       "       [ 64.,  64.,  65., ...,   8.,   8.,   8.],\n",
       "       ...,\n",
       "       [135., 122., 136., ...,   7.,   8.,   8.],\n",
       "       [ 65.,  65.,  66., ...,   8.,  65.,   8.],\n",
       "       [168., 169., 167., ...,   7.,   7.,   7.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1372549 , 0.1372549 , 0.15360502, ..., 0.05223881, 0.03187251,\n",
       "        0.1875    ],\n",
       "       [0.232493  , 0.22969188, 0.26332288, ..., 0.06716418, 0.03585657,\n",
       "        0.05555556],\n",
       "       [0.02240896, 0.02240896, 0.02194357, ..., 0.05970149, 0.03187251,\n",
       "        0.51388889],\n",
       "       ...,\n",
       "       [0.30532213, 0.30532213, 0.34169279, ..., 0.05970149, 0.32669323,\n",
       "        0.05555556],\n",
       "       [0.05322129, 0.05322129, 0.02507837, ..., 0.06716418, 0.02788845,\n",
       "        0.04861111],\n",
       "       [0.01960784, 0.01960784, 0.02194357, ..., 0.23134328, 0.11952191,\n",
       "        0.21527778]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66386555, 0.32212885, 0.5015674 , ..., 0.05223881, 0.17928287,\n",
       "        0.40972222],\n",
       "       [0.50420168, 0.50420168, 0.56739812, ..., 0.05970149, 0.02390438,\n",
       "        0.0625    ],\n",
       "       [0.17927171, 0.17927171, 0.20376176, ..., 0.05970149, 0.03187251,\n",
       "        0.05555556],\n",
       "       ...,\n",
       "       [0.37815126, 0.34173669, 0.42633229, ..., 0.05223881, 0.03187251,\n",
       "        0.05555556],\n",
       "       [0.18207283, 0.18207283, 0.20689655, ..., 0.05970149, 0.25896414,\n",
       "        0.05555556],\n",
       "       [0.47058824, 0.47338936, 0.52351097, ..., 0.05223881, 0.02788845,\n",
       "        0.04861111]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "source": [
    "## Logistic Regression Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.80      0.85        35\n",
      "           1       0.83      0.92      0.87        37\n",
      "\n",
      "    accuracy                           0.86        72\n",
      "   macro avg       0.87      0.86      0.86        72\n",
      "weighted avg       0.87      0.86      0.86        72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28,  7],\n",
       "       [ 3, 34]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "source": [
    "## Neural Network Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 90)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30, activation = 'relu'))\n",
    "model.add(Dense(15, activation = 'relu'))\n",
    "\n",
    "# BINARY CLASSIFICATION\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.7902 - val_loss: 0.7140\n",
      "Epoch 2/600\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6946 - val_loss: 0.6729\n",
      "Epoch 3/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6645 - val_loss: 0.6565\n",
      "Epoch 4/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6477 - val_loss: 0.6427\n",
      "Epoch 5/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6328 - val_loss: 0.6297\n",
      "Epoch 6/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6169 - val_loss: 0.6158\n",
      "Epoch 7/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5991 - val_loss: 0.5990\n",
      "Epoch 8/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5791 - val_loss: 0.5803\n",
      "Epoch 9/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5561 - val_loss: 0.5580\n",
      "Epoch 10/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5300 - val_loss: 0.5322\n",
      "Epoch 11/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4998 - val_loss: 0.4964\n",
      "Epoch 12/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4667 - val_loss: 0.4589\n",
      "Epoch 13/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4249 - val_loss: 0.4323\n",
      "Epoch 14/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3895 - val_loss: 0.3988\n",
      "Epoch 15/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3560 - val_loss: 0.3707\n",
      "Epoch 16/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3274 - val_loss: 0.3496\n",
      "Epoch 17/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3025 - val_loss: 0.3335\n",
      "Epoch 18/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2811 - val_loss: 0.3210\n",
      "Epoch 19/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2645 - val_loss: 0.3124\n",
      "Epoch 20/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2500 - val_loss: 0.3053\n",
      "Epoch 21/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2359 - val_loss: 0.3004\n",
      "Epoch 22/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2241 - val_loss: 0.2962\n",
      "Epoch 23/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2117 - val_loss: 0.2964\n",
      "Epoch 24/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2050 - val_loss: 0.2930\n",
      "Epoch 25/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1921 - val_loss: 0.2903\n",
      "Epoch 26/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1836 - val_loss: 0.2878\n",
      "Epoch 27/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1746 - val_loss: 0.2871\n",
      "Epoch 28/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1659 - val_loss: 0.2873\n",
      "Epoch 29/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1588 - val_loss: 0.2868\n",
      "Epoch 30/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1515 - val_loss: 0.2864\n",
      "Epoch 31/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1448 - val_loss: 0.2866\n",
      "Epoch 32/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1361 - val_loss: 0.2876\n",
      "Epoch 33/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1292 - val_loss: 0.2874\n",
      "Epoch 34/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1235 - val_loss: 0.2870\n",
      "Epoch 35/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1245 - val_loss: 0.2924\n",
      "Epoch 36/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1109 - val_loss: 0.2920\n",
      "Epoch 37/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1065 - val_loss: 0.2879\n",
      "Epoch 38/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0981 - val_loss: 0.2883\n",
      "Epoch 39/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0930 - val_loss: 0.2917\n",
      "Epoch 40/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0888 - val_loss: 0.2942\n",
      "Epoch 41/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0844 - val_loss: 0.2985\n",
      "Epoch 42/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0791 - val_loss: 0.3020\n",
      "Epoch 43/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0752 - val_loss: 0.3038\n",
      "Epoch 44/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0715 - val_loss: 0.3068\n",
      "Epoch 45/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0680 - val_loss: 0.3082\n",
      "Epoch 46/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0654 - val_loss: 0.3111\n",
      "Epoch 47/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0621 - val_loss: 0.3129\n",
      "Epoch 48/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0585 - val_loss: 0.3146\n",
      "Epoch 49/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0574 - val_loss: 0.3193\n",
      "Epoch 50/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0548 - val_loss: 0.3250\n",
      "Epoch 51/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0506 - val_loss: 0.3240\n",
      "Epoch 52/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0484 - val_loss: 0.3242\n",
      "Epoch 53/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0467 - val_loss: 0.3276\n",
      "Epoch 54/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0444 - val_loss: 0.3306\n",
      "Epoch 55/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0429 - val_loss: 0.3353\n",
      "Epoch 56/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0402 - val_loss: 0.3355\n",
      "Epoch 57/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0383 - val_loss: 0.3399\n",
      "Epoch 58/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 0.3442\n",
      "Epoch 59/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 0.3428\n",
      "Epoch 60/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 0.3470\n",
      "Epoch 61/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.3496\n",
      "Epoch 62/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.3545\n",
      "Epoch 63/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.3554\n",
      "Epoch 64/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.3563\n",
      "Epoch 65/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.3643\n",
      "Epoch 66/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.3638\n",
      "Epoch 67/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.3655\n",
      "Epoch 68/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.3698\n",
      "Epoch 69/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.3753\n",
      "Epoch 70/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.3745\n",
      "Epoch 71/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.3751\n",
      "Epoch 72/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.3818\n",
      "Epoch 73/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.3820\n",
      "Epoch 74/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.3828\n",
      "Epoch 75/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.3892\n",
      "Epoch 76/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.3935\n",
      "Epoch 77/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.3917\n",
      "Epoch 78/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.3957\n",
      "Epoch 79/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.3993\n",
      "Epoch 80/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.3988\n",
      "Epoch 81/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.4062\n",
      "Epoch 82/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0145 - val_loss: 0.4083\n",
      "Epoch 83/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.4080\n",
      "Epoch 84/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0131 - val_loss: 0.4159\n",
      "Epoch 85/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.4170\n",
      "Epoch 86/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.4184\n",
      "Epoch 87/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.4205\n",
      "Epoch 88/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.4204\n",
      "Epoch 89/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.4261\n",
      "Epoch 90/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.4248\n",
      "Epoch 91/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.4293\n",
      "Epoch 92/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.4355\n",
      "Epoch 93/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.4352\n",
      "Epoch 94/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.4359\n",
      "Epoch 95/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.4364\n",
      "Epoch 96/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.4395\n",
      "Epoch 97/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.4442\n",
      "Epoch 98/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.4453\n",
      "Epoch 99/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 0.4464\n",
      "Epoch 100/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.4497\n",
      "Epoch 101/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.4512\n",
      "Epoch 102/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.4534\n",
      "Epoch 103/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.4553\n",
      "Epoch 104/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 0.4567\n",
      "Epoch 105/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.4630\n",
      "Epoch 106/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.4633\n",
      "Epoch 107/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.4617\n",
      "Epoch 108/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.4670\n",
      "Epoch 109/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.4692\n",
      "Epoch 110/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.4697\n",
      "Epoch 111/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.4722\n",
      "Epoch 112/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.4758\n",
      "Epoch 113/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.4769\n",
      "Epoch 114/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.4781\n",
      "Epoch 115/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.4808\n",
      "Epoch 116/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.4828\n",
      "Epoch 117/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.4829\n",
      "Epoch 118/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.4853\n",
      "Epoch 119/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.4863\n",
      "Epoch 120/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.4876\n",
      "Epoch 121/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.4899\n",
      "Epoch 122/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.4943\n",
      "Epoch 123/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.4938\n",
      "Epoch 124/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.4944\n",
      "Epoch 125/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.4979\n",
      "Epoch 126/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.4995\n",
      "Epoch 127/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.4998\n",
      "Epoch 128/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.5037\n",
      "Epoch 129/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.5033\n",
      "Epoch 130/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.5033\n",
      "Epoch 131/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.5059\n",
      "Epoch 132/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.5072\n",
      "Epoch 133/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.5070\n",
      "Epoch 134/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.5077\n",
      "Epoch 135/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.5102\n",
      "Epoch 136/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.5140\n",
      "Epoch 137/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.5134\n",
      "Epoch 138/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.5129\n",
      "Epoch 139/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.5152\n",
      "Epoch 140/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.5169\n",
      "Epoch 141/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.5201\n",
      "Epoch 142/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.5192\n",
      "Epoch 143/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.5191\n",
      "Epoch 144/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.5202\n",
      "Epoch 145/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.5224\n",
      "Epoch 146/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.5239\n",
      "Epoch 147/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.5272\n",
      "Epoch 148/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.5258\n",
      "Epoch 149/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.5263\n",
      "Epoch 150/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.5285\n",
      "Epoch 151/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.5302\n",
      "Epoch 152/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.5296\n",
      "Epoch 153/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.5317\n",
      "Epoch 154/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.5314\n",
      "Epoch 155/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.5342\n",
      "Epoch 156/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.5343\n",
      "Epoch 157/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.5355\n",
      "Epoch 158/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.5367\n",
      "Epoch 159/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.5388\n",
      "Epoch 160/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.5404\n",
      "Epoch 161/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.5407\n",
      "Epoch 162/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.5428\n",
      "Epoch 163/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.5426\n",
      "Epoch 164/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.5428\n",
      "Epoch 165/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.5439\n",
      "Epoch 166/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.5460\n",
      "Epoch 167/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.5483\n",
      "Epoch 168/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.5480\n",
      "Epoch 169/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.5483\n",
      "Epoch 170/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.5491\n",
      "Epoch 171/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.5512\n",
      "Epoch 172/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.5518\n",
      "Epoch 173/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.5523\n",
      "Epoch 174/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.5538\n",
      "Epoch 175/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.5539\n",
      "Epoch 176/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.5559\n",
      "Epoch 177/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.5575\n",
      "Epoch 178/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.5578\n",
      "Epoch 179/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.5582\n",
      "Epoch 180/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.5598\n",
      "Epoch 181/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.9199e-04 - val_loss: 0.5605\n",
      "Epoch 182/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.7725e-04 - val_loss: 0.5607\n",
      "Epoch 183/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.6431e-04 - val_loss: 0.5622\n",
      "Epoch 184/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.4041e-04 - val_loss: 0.5636\n",
      "Epoch 185/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.2945e-04 - val_loss: 0.5647\n",
      "Epoch 186/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.1722e-04 - val_loss: 0.5664\n",
      "Epoch 187/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.0458e-04 - val_loss: 0.5678\n",
      "Epoch 188/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.8599e-04 - val_loss: 0.5686\n",
      "Epoch 189/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.7224e-04 - val_loss: 0.5686\n",
      "Epoch 190/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.6239e-04 - val_loss: 0.5695\n",
      "Epoch 191/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.4748e-04 - val_loss: 0.5703\n",
      "Epoch 192/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.3442e-04 - val_loss: 0.5724\n",
      "Epoch 193/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.2745e-04 - val_loss: 0.5723\n",
      "Epoch 194/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.0896e-04 - val_loss: 0.5728\n",
      "Epoch 195/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.0303e-04 - val_loss: 0.5744\n",
      "Epoch 196/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.8752e-04 - val_loss: 0.5759\n",
      "Epoch 197/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.7530e-04 - val_loss: 0.5765\n",
      "Epoch 198/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.6351e-04 - val_loss: 0.5774\n",
      "Epoch 199/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.5404e-04 - val_loss: 0.5789\n",
      "Epoch 200/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.4102e-04 - val_loss: 0.5791\n",
      "Epoch 201/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.3222e-04 - val_loss: 0.5799\n",
      "Epoch 202/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.2184e-04 - val_loss: 0.5809\n",
      "Epoch 203/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.1167e-04 - val_loss: 0.5813\n",
      "Epoch 204/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.0558e-04 - val_loss: 0.5828\n",
      "Epoch 205/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.9099e-04 - val_loss: 0.5839\n",
      "Epoch 206/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.8203e-04 - val_loss: 0.5846\n",
      "Epoch 207/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.7393e-04 - val_loss: 0.5854\n",
      "Epoch 208/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.6511e-04 - val_loss: 0.5866\n",
      "Epoch 209/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.5827e-04 - val_loss: 0.5871\n",
      "Epoch 210/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.4749e-04 - val_loss: 0.5879\n",
      "Epoch 211/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.3741e-04 - val_loss: 0.5888\n",
      "Epoch 212/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.3601e-04 - val_loss: 0.5898\n",
      "Epoch 213/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.2418e-04 - val_loss: 0.5908\n",
      "Epoch 214/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.1727e-04 - val_loss: 0.5913\n",
      "Epoch 215/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.0797e-04 - val_loss: 0.5915\n",
      "Epoch 216/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.0016e-04 - val_loss: 0.5930\n",
      "Epoch 217/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.9057e-04 - val_loss: 0.5940\n",
      "Epoch 218/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.8348e-04 - val_loss: 0.5952\n",
      "Epoch 219/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.7771e-04 - val_loss: 0.5955\n",
      "Epoch 220/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.6912e-04 - val_loss: 0.5974\n",
      "Epoch 221/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.6207e-04 - val_loss: 0.5984\n",
      "Epoch 222/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.5506e-04 - val_loss: 0.5991\n",
      "Epoch 223/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.4726e-04 - val_loss: 0.5996\n",
      "Epoch 224/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.4159e-04 - val_loss: 0.6005\n",
      "Epoch 225/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.3508e-04 - val_loss: 0.6007\n",
      "Epoch 226/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.2795e-04 - val_loss: 0.6023\n",
      "Epoch 227/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.2171e-04 - val_loss: 0.6026\n",
      "Epoch 228/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.1655e-04 - val_loss: 0.6027\n",
      "Epoch 229/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.1241e-04 - val_loss: 0.6049\n",
      "Epoch 230/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.0391e-04 - val_loss: 0.6054\n",
      "Epoch 231/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.9750e-04 - val_loss: 0.6051\n",
      "Epoch 232/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.9201e-04 - val_loss: 0.6065\n",
      "Epoch 233/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.8574e-04 - val_loss: 0.6079\n",
      "Epoch 234/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.8218e-04 - val_loss: 0.6074\n",
      "Epoch 235/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.7310e-04 - val_loss: 0.6085\n",
      "Epoch 236/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.6838e-04 - val_loss: 0.6094\n",
      "Epoch 237/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.6307e-04 - val_loss: 0.6114\n",
      "Epoch 238/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.5632e-04 - val_loss: 0.6118\n",
      "Epoch 239/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.5279e-04 - val_loss: 0.6124\n",
      "Epoch 240/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.4852e-04 - val_loss: 0.6130\n",
      "Epoch 241/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.4248e-04 - val_loss: 0.6137\n",
      "Epoch 242/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.3584e-04 - val_loss: 0.6149\n",
      "Epoch 243/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.3135e-04 - val_loss: 0.6155\n",
      "Epoch 244/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.2937e-04 - val_loss: 0.6157\n",
      "Epoch 245/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.2720e-04 - val_loss: 0.6182\n",
      "Epoch 246/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.1807e-04 - val_loss: 0.6179\n",
      "Epoch 247/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 4.1261e-04 - val_loss: 0.6197\n",
      "Epoch 248/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.0706e-04 - val_loss: 0.6197\n",
      "Epoch 249/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.0622e-04 - val_loss: 0.6196\n",
      "Epoch 250/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.9768e-04 - val_loss: 0.6209\n",
      "Epoch 251/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.9433e-04 - val_loss: 0.6222\n",
      "Epoch 252/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.8989e-04 - val_loss: 0.6226\n",
      "Epoch 253/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.8920e-04 - val_loss: 0.6231\n",
      "Epoch 254/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.8160e-04 - val_loss: 0.6248\n",
      "Epoch 255/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.7695e-04 - val_loss: 0.6251\n",
      "Epoch 256/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.7533e-04 - val_loss: 0.6246\n",
      "Epoch 257/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.6992e-04 - val_loss: 0.6266\n",
      "Epoch 258/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.6534e-04 - val_loss: 0.6282\n",
      "Epoch 259/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.6204e-04 - val_loss: 0.6283\n",
      "Epoch 260/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5812e-04 - val_loss: 0.6285\n",
      "Epoch 261/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5365e-04 - val_loss: 0.6300\n",
      "Epoch 262/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5038e-04 - val_loss: 0.6306\n",
      "Epoch 263/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4933e-04 - val_loss: 0.6319\n",
      "Epoch 264/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4452e-04 - val_loss: 0.6312\n",
      "Epoch 265/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3994e-04 - val_loss: 0.6327\n",
      "Epoch 266/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3640e-04 - val_loss: 0.6331\n",
      "Epoch 267/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3322e-04 - val_loss: 0.6342\n",
      "Epoch 268/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.2914e-04 - val_loss: 0.6347\n",
      "Epoch 269/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.2718e-04 - val_loss: 0.6350\n",
      "Epoch 270/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.2353e-04 - val_loss: 0.6350\n",
      "Epoch 271/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.1925e-04 - val_loss: 0.6363\n",
      "Epoch 272/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.1707e-04 - val_loss: 0.6382\n",
      "Epoch 273/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.1663e-04 - val_loss: 0.6370\n",
      "Epoch 274/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.1156e-04 - val_loss: 0.6393\n",
      "Epoch 275/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.0702e-04 - val_loss: 0.6395\n",
      "Epoch 276/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.0352e-04 - val_loss: 0.6402\n",
      "Epoch 277/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.0073e-04 - val_loss: 0.6404\n",
      "Epoch 278/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.9768e-04 - val_loss: 0.6414\n",
      "Epoch 279/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.9469e-04 - val_loss: 0.6423\n",
      "Epoch 280/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.9265e-04 - val_loss: 0.6431\n",
      "Epoch 281/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.8928e-04 - val_loss: 0.6438\n",
      "Epoch 282/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.8613e-04 - val_loss: 0.6442\n",
      "Epoch 283/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.8352e-04 - val_loss: 0.6449\n",
      "Epoch 284/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.8130e-04 - val_loss: 0.6450\n",
      "Epoch 285/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.7795e-04 - val_loss: 0.6461\n",
      "Epoch 286/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.7535e-04 - val_loss: 0.6468\n",
      "Epoch 287/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.7373e-04 - val_loss: 0.6465\n",
      "Epoch 288/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.7055e-04 - val_loss: 0.6479\n",
      "Epoch 289/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.6820e-04 - val_loss: 0.6483\n",
      "Epoch 290/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.6586e-04 - val_loss: 0.6487\n",
      "Epoch 291/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.6305e-04 - val_loss: 0.6494\n",
      "Epoch 292/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.6103e-04 - val_loss: 0.6504\n",
      "Epoch 293/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.5869e-04 - val_loss: 0.6511\n",
      "Epoch 294/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.5671e-04 - val_loss: 0.6511\n",
      "Epoch 295/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.5346e-04 - val_loss: 0.6523\n",
      "Epoch 296/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.5151e-04 - val_loss: 0.6534\n",
      "Epoch 297/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.4910e-04 - val_loss: 0.6532\n",
      "Epoch 298/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.4698e-04 - val_loss: 0.6553\n",
      "Epoch 299/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.4527e-04 - val_loss: 0.6549\n",
      "Epoch 300/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.4213e-04 - val_loss: 0.6558\n",
      "Epoch 301/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3967e-04 - val_loss: 0.6563\n",
      "Epoch 302/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3812e-04 - val_loss: 0.6569\n",
      "Epoch 303/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3605e-04 - val_loss: 0.6578\n",
      "Epoch 304/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3351e-04 - val_loss: 0.6579\n",
      "Epoch 305/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3147e-04 - val_loss: 0.6587\n",
      "Epoch 306/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.2996e-04 - val_loss: 0.6600\n",
      "Epoch 307/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.2694e-04 - val_loss: 0.6597\n",
      "Epoch 308/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.2579e-04 - val_loss: 0.6602\n",
      "Epoch 309/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.2327e-04 - val_loss: 0.6613\n",
      "Epoch 310/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.2114e-04 - val_loss: 0.6615\n",
      "Epoch 311/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.1985e-04 - val_loss: 0.6620\n",
      "Epoch 312/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.1731e-04 - val_loss: 0.6630\n",
      "Epoch 313/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.1591e-04 - val_loss: 0.6631\n",
      "Epoch 314/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.1368e-04 - val_loss: 0.6643\n",
      "Epoch 315/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.1197e-04 - val_loss: 0.6653\n",
      "Epoch 316/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.1043e-04 - val_loss: 0.6653\n",
      "Epoch 317/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.0875e-04 - val_loss: 0.6660\n",
      "Epoch 318/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.0654e-04 - val_loss: 0.6662\n",
      "Epoch 319/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.0497e-04 - val_loss: 0.6674\n",
      "Epoch 320/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.0358e-04 - val_loss: 0.6671\n",
      "Epoch 321/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.0111e-04 - val_loss: 0.6678\n",
      "Epoch 322/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9924e-04 - val_loss: 0.6695\n",
      "Epoch 323/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9797e-04 - val_loss: 0.6697\n",
      "Epoch 324/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9605e-04 - val_loss: 0.6700\n",
      "Epoch 325/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9454e-04 - val_loss: 0.6708\n",
      "Epoch 326/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9324e-04 - val_loss: 0.6719\n",
      "Epoch 327/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.9137e-04 - val_loss: 0.6728\n",
      "Epoch 328/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8966e-04 - val_loss: 0.6727\n",
      "Epoch 329/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8851e-04 - val_loss: 0.6743\n",
      "Epoch 330/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8637e-04 - val_loss: 0.6740\n",
      "Epoch 331/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8485e-04 - val_loss: 0.6747\n",
      "Epoch 332/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8330e-04 - val_loss: 0.6746\n",
      "Epoch 333/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8198e-04 - val_loss: 0.6750\n",
      "Epoch 334/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8066e-04 - val_loss: 0.6758\n",
      "Epoch 335/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.7895e-04 - val_loss: 0.6763\n",
      "Epoch 336/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.7788e-04 - val_loss: 0.6768\n",
      "Epoch 337/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.7595e-04 - val_loss: 0.6780\n",
      "Epoch 338/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.7442e-04 - val_loss: 0.6781\n",
      "Epoch 339/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.7307e-04 - val_loss: 0.6786\n",
      "Epoch 340/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.7199e-04 - val_loss: 0.6798\n",
      "Epoch 341/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.7110e-04 - val_loss: 0.6797\n",
      "Epoch 342/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6932e-04 - val_loss: 0.6802\n",
      "Epoch 343/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6763e-04 - val_loss: 0.6806\n",
      "Epoch 344/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6694e-04 - val_loss: 0.6818\n",
      "Epoch 345/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6550e-04 - val_loss: 0.6829\n",
      "Epoch 346/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6344e-04 - val_loss: 0.6828\n",
      "Epoch 347/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6254e-04 - val_loss: 0.6834\n",
      "Epoch 348/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6134e-04 - val_loss: 0.6836\n",
      "Epoch 349/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5969e-04 - val_loss: 0.6845\n",
      "Epoch 350/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5853e-04 - val_loss: 0.6849\n",
      "Epoch 351/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5735e-04 - val_loss: 0.6850\n",
      "Epoch 352/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5683e-04 - val_loss: 0.6853\n",
      "Epoch 353/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5491e-04 - val_loss: 0.6868\n",
      "Epoch 354/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5386e-04 - val_loss: 0.6873\n",
      "Epoch 355/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5289e-04 - val_loss: 0.6872\n",
      "Epoch 356/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5139e-04 - val_loss: 0.6882\n",
      "Epoch 357/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.5040e-04 - val_loss: 0.6882\n",
      "Epoch 358/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4903e-04 - val_loss: 0.6888\n",
      "Epoch 359/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4771e-04 - val_loss: 0.6905\n",
      "Epoch 360/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4715e-04 - val_loss: 0.6909\n",
      "Epoch 361/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4591e-04 - val_loss: 0.6913\n",
      "Epoch 362/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4510e-04 - val_loss: 0.6920\n",
      "Epoch 363/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4359e-04 - val_loss: 0.6913\n",
      "Epoch 364/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4266e-04 - val_loss: 0.6921\n",
      "Epoch 365/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4164e-04 - val_loss: 0.6927\n",
      "Epoch 366/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4037e-04 - val_loss: 0.6938\n",
      "Epoch 367/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3968e-04 - val_loss: 0.6949\n",
      "Epoch 368/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3826e-04 - val_loss: 0.6939\n",
      "Epoch 369/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3727e-04 - val_loss: 0.6947\n",
      "Epoch 370/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3619e-04 - val_loss: 0.6950\n",
      "Epoch 371/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3487e-04 - val_loss: 0.6960\n",
      "Epoch 372/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3414e-04 - val_loss: 0.6966\n",
      "Epoch 373/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3315e-04 - val_loss: 0.6975\n",
      "Epoch 374/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3202e-04 - val_loss: 0.6982\n",
      "Epoch 375/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3111e-04 - val_loss: 0.6987\n",
      "Epoch 376/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.3023e-04 - val_loss: 0.6984\n",
      "Epoch 377/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2972e-04 - val_loss: 0.6986\n",
      "Epoch 378/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2812e-04 - val_loss: 0.7000\n",
      "Epoch 379/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2759e-04 - val_loss: 0.7006\n",
      "Epoch 380/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2679e-04 - val_loss: 0.7004\n",
      "Epoch 381/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2540e-04 - val_loss: 0.7015\n",
      "Epoch 382/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2469e-04 - val_loss: 0.7021\n",
      "Epoch 383/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2352e-04 - val_loss: 0.7028\n",
      "Epoch 384/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2273e-04 - val_loss: 0.7033\n",
      "Epoch 385/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2170e-04 - val_loss: 0.7039\n",
      "Epoch 386/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.2078e-04 - val_loss: 0.7042\n",
      "Epoch 387/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1981e-04 - val_loss: 0.7047\n",
      "Epoch 388/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1917e-04 - val_loss: 0.7056\n",
      "Epoch 389/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1814e-04 - val_loss: 0.7051\n",
      "Epoch 390/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1752e-04 - val_loss: 0.7062\n",
      "Epoch 391/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1651e-04 - val_loss: 0.7065\n",
      "Epoch 392/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1571e-04 - val_loss: 0.7064\n",
      "Epoch 393/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1510e-04 - val_loss: 0.7067\n",
      "Epoch 394/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1405e-04 - val_loss: 0.7075\n",
      "Epoch 395/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1347e-04 - val_loss: 0.7085\n",
      "Epoch 396/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1290e-04 - val_loss: 0.7086\n",
      "Epoch 397/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1191e-04 - val_loss: 0.7092\n",
      "Epoch 398/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1080e-04 - val_loss: 0.7098\n",
      "Epoch 399/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1022e-04 - val_loss: 0.7109\n",
      "Epoch 400/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0932e-04 - val_loss: 0.7111\n",
      "Epoch 401/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0864e-04 - val_loss: 0.7111\n",
      "Epoch 402/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0860e-04 - val_loss: 0.7129\n",
      "Epoch 403/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0715e-04 - val_loss: 0.7125\n",
      "Epoch 404/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0655e-04 - val_loss: 0.7130\n",
      "Epoch 405/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0596e-04 - val_loss: 0.7135\n",
      "Epoch 406/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0501e-04 - val_loss: 0.7143\n",
      "Epoch 407/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0441e-04 - val_loss: 0.7146\n",
      "Epoch 408/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0352e-04 - val_loss: 0.7151\n",
      "Epoch 409/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0277e-04 - val_loss: 0.7158\n",
      "Epoch 410/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0197e-04 - val_loss: 0.7160\n",
      "Epoch 411/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0168e-04 - val_loss: 0.7161\n",
      "Epoch 412/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0164e-04 - val_loss: 0.7178\n",
      "Epoch 413/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.0049e-04 - val_loss: 0.7167\n",
      "Epoch 414/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.9259e-05 - val_loss: 0.7169\n",
      "Epoch 415/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.8585e-05 - val_loss: 0.7179\n",
      "Epoch 416/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.7960e-05 - val_loss: 0.7190\n",
      "Epoch 417/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.7231e-05 - val_loss: 0.7189\n",
      "Epoch 418/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.6715e-05 - val_loss: 0.7194\n",
      "Epoch 419/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.5871e-05 - val_loss: 0.7202\n",
      "Epoch 420/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.5419e-05 - val_loss: 0.7204\n",
      "Epoch 421/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.4668e-05 - val_loss: 0.7210\n",
      "Epoch 422/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.3974e-05 - val_loss: 0.7221\n",
      "Epoch 423/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.3353e-05 - val_loss: 0.7220\n",
      "Epoch 424/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.2823e-05 - val_loss: 0.7224\n",
      "Epoch 425/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.2565e-05 - val_loss: 0.7227\n",
      "Epoch 426/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.1346e-05 - val_loss: 0.7239\n",
      "Epoch 427/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.0987e-05 - val_loss: 0.7247\n",
      "Epoch 428/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 9.0404e-05 - val_loss: 0.7246\n",
      "Epoch 429/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.9650e-05 - val_loss: 0.7248\n",
      "Epoch 430/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.9063e-05 - val_loss: 0.7258\n",
      "Epoch 431/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.8386e-05 - val_loss: 0.7265\n",
      "Epoch 432/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.7964e-05 - val_loss: 0.7262\n",
      "Epoch 433/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.7407e-05 - val_loss: 0.7265\n",
      "Epoch 434/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.6777e-05 - val_loss: 0.7280\n",
      "Epoch 435/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.6324e-05 - val_loss: 0.7285\n",
      "Epoch 436/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.5673e-05 - val_loss: 0.7291\n",
      "Epoch 437/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.5020e-05 - val_loss: 0.7288\n",
      "Epoch 438/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.4603e-05 - val_loss: 0.7296\n",
      "Epoch 439/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.4048e-05 - val_loss: 0.7301\n",
      "Epoch 440/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.3427e-05 - val_loss: 0.7301\n",
      "Epoch 441/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.2796e-05 - val_loss: 0.7305\n",
      "Epoch 442/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.2369e-05 - val_loss: 0.7305\n",
      "Epoch 443/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.1784e-05 - val_loss: 0.7321\n",
      "Epoch 444/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.1279e-05 - val_loss: 0.7323\n",
      "Epoch 445/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.0651e-05 - val_loss: 0.7324\n",
      "Epoch 446/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.0132e-05 - val_loss: 0.7328\n",
      "Epoch 447/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.9686e-05 - val_loss: 0.7338\n",
      "Epoch 448/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.9249e-05 - val_loss: 0.7346\n",
      "Epoch 449/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.8828e-05 - val_loss: 0.7339\n",
      "Epoch 450/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.8328e-05 - val_loss: 0.7354\n",
      "Epoch 451/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.7803e-05 - val_loss: 0.7358\n",
      "Epoch 452/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.7302e-05 - val_loss: 0.7360\n",
      "Epoch 453/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.6715e-05 - val_loss: 0.7361\n",
      "Epoch 454/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.6505e-05 - val_loss: 0.7369\n",
      "Epoch 455/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.5901e-05 - val_loss: 0.7375\n",
      "Epoch 456/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.5242e-05 - val_loss: 0.7379\n",
      "Epoch 457/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.4989e-05 - val_loss: 0.7387\n",
      "Epoch 458/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.4311e-05 - val_loss: 0.7386\n",
      "Epoch 459/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.4103e-05 - val_loss: 0.7391\n",
      "Epoch 460/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.3453e-05 - val_loss: 0.7396\n",
      "Epoch 461/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.3334e-05 - val_loss: 0.7409\n",
      "Epoch 462/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.2481e-05 - val_loss: 0.7409\n",
      "Epoch 463/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.1963e-05 - val_loss: 0.7409\n",
      "Epoch 464/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.1550e-05 - val_loss: 0.7415\n",
      "Epoch 465/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.1129e-05 - val_loss: 0.7418\n",
      "Epoch 466/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.0609e-05 - val_loss: 0.7422\n",
      "Epoch 467/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.0206e-05 - val_loss: 0.7427\n",
      "Epoch 468/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.9732e-05 - val_loss: 0.7429\n",
      "Epoch 469/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.9324e-05 - val_loss: 0.7438\n",
      "Epoch 470/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.8863e-05 - val_loss: 0.7441\n",
      "Epoch 471/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.8479e-05 - val_loss: 0.7441\n",
      "Epoch 472/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.8210e-05 - val_loss: 0.7454\n",
      "Epoch 473/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.7653e-05 - val_loss: 0.7455\n",
      "Epoch 474/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.7279e-05 - val_loss: 0.7462\n",
      "Epoch 475/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.6798e-05 - val_loss: 0.7466\n",
      "Epoch 476/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.6392e-05 - val_loss: 0.7469\n",
      "Epoch 477/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.5992e-05 - val_loss: 0.7469\n",
      "Epoch 478/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.5543e-05 - val_loss: 0.7471\n",
      "Epoch 479/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.5329e-05 - val_loss: 0.7480\n",
      "Epoch 480/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.4814e-05 - val_loss: 0.7490\n",
      "Epoch 481/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.4243e-05 - val_loss: 0.7486\n",
      "Epoch 482/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.4093e-05 - val_loss: 0.7497\n",
      "Epoch 483/600\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 6.3624e-05 - val_loss: 0.7497\n",
      "Epoch 484/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 6.3443e-05 - val_loss: 0.7508\n",
      "Epoch 485/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.3037e-05 - val_loss: 0.7502\n",
      "Epoch 486/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.2610e-05 - val_loss: 0.7511\n",
      "Epoch 487/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.2024e-05 - val_loss: 0.7513\n",
      "Epoch 488/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.1728e-05 - val_loss: 0.7521\n",
      "Epoch 489/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.1214e-05 - val_loss: 0.7525\n",
      "Epoch 490/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.0934e-05 - val_loss: 0.7526\n",
      "Epoch 491/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.0532e-05 - val_loss: 0.7534\n",
      "Epoch 492/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.0180e-05 - val_loss: 0.7537\n",
      "Epoch 493/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.9976e-05 - val_loss: 0.7544\n",
      "Epoch 494/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.9398e-05 - val_loss: 0.7541\n",
      "Epoch 495/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.9089e-05 - val_loss: 0.7548\n",
      "Epoch 496/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.8739e-05 - val_loss: 0.7553\n",
      "Epoch 497/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.8394e-05 - val_loss: 0.7561\n",
      "Epoch 498/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.8045e-05 - val_loss: 0.7561\n",
      "Epoch 499/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.7738e-05 - val_loss: 0.7562\n",
      "Epoch 500/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.7339e-05 - val_loss: 0.7569\n",
      "Epoch 501/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.6929e-05 - val_loss: 0.7578\n",
      "Epoch 502/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.6631e-05 - val_loss: 0.7580\n",
      "Epoch 503/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.6302e-05 - val_loss: 0.7588\n",
      "Epoch 504/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.6079e-05 - val_loss: 0.7590\n",
      "Epoch 505/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.5619e-05 - val_loss: 0.7597\n",
      "Epoch 506/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.5339e-05 - val_loss: 0.7600\n",
      "Epoch 507/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.4929e-05 - val_loss: 0.7604\n",
      "Epoch 508/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.4693e-05 - val_loss: 0.7611\n",
      "Epoch 509/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.4362e-05 - val_loss: 0.7618\n",
      "Epoch 510/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.4047e-05 - val_loss: 0.7619\n",
      "Epoch 511/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.3734e-05 - val_loss: 0.7619\n",
      "Epoch 512/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.3387e-05 - val_loss: 0.7626\n",
      "Epoch 513/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.3135e-05 - val_loss: 0.7636\n",
      "Epoch 514/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.2792e-05 - val_loss: 0.7637\n",
      "Epoch 515/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.2505e-05 - val_loss: 0.7638\n",
      "Epoch 516/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.2434e-05 - val_loss: 0.7637\n",
      "Epoch 517/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.1858e-05 - val_loss: 0.7648\n",
      "Epoch 518/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.1564e-05 - val_loss: 0.7654\n",
      "Epoch 519/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.1299e-05 - val_loss: 0.7658\n",
      "Epoch 520/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.1008e-05 - val_loss: 0.7662\n",
      "Epoch 521/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.0784e-05 - val_loss: 0.7662\n",
      "Epoch 522/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.0325e-05 - val_loss: 0.7671\n",
      "Epoch 523/600\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.0083e-05 - val_loss: 0.7680\n",
      "Epoch 524/600\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.9774e-05 - val_loss: 0.7680\n",
      "Epoch 525/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.9454e-05 - val_loss: 0.7682\n",
      "Epoch 526/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.9219e-05 - val_loss: 0.7691\n",
      "Epoch 527/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.8989e-05 - val_loss: 0.7697\n",
      "Epoch 528/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.8722e-05 - val_loss: 0.7696\n",
      "Epoch 529/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.8352e-05 - val_loss: 0.7701\n",
      "Epoch 530/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.8072e-05 - val_loss: 0.7705\n",
      "Epoch 531/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.8007e-05 - val_loss: 0.7708\n",
      "Epoch 532/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.7545e-05 - val_loss: 0.7718\n",
      "Epoch 533/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.7249e-05 - val_loss: 0.7719\n",
      "Epoch 534/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.7026e-05 - val_loss: 0.7724\n",
      "Epoch 535/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.6679e-05 - val_loss: 0.7727\n",
      "Epoch 536/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.6405e-05 - val_loss: 0.7732\n",
      "Epoch 537/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.6223e-05 - val_loss: 0.7733\n",
      "Epoch 538/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.5900e-05 - val_loss: 0.7740\n",
      "Epoch 539/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.5645e-05 - val_loss: 0.7746\n",
      "Epoch 540/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.5380e-05 - val_loss: 0.7752\n",
      "Epoch 541/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.5148e-05 - val_loss: 0.7752\n",
      "Epoch 542/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.4903e-05 - val_loss: 0.7754\n",
      "Epoch 543/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.4587e-05 - val_loss: 0.7763\n",
      "Epoch 544/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.4444e-05 - val_loss: 0.7757\n",
      "Epoch 545/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.4172e-05 - val_loss: 0.7762\n",
      "Epoch 546/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.3880e-05 - val_loss: 0.7772\n",
      "Epoch 547/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.3615e-05 - val_loss: 0.7773\n",
      "Epoch 548/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.3321e-05 - val_loss: 0.7777\n",
      "Epoch 549/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.3115e-05 - val_loss: 0.7780\n",
      "Epoch 550/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.3010e-05 - val_loss: 0.7795\n",
      "Epoch 551/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.2660e-05 - val_loss: 0.7792\n",
      "Epoch 552/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.2407e-05 - val_loss: 0.7792\n",
      "Epoch 553/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.2196e-05 - val_loss: 0.7802\n",
      "Epoch 554/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.1890e-05 - val_loss: 0.7803\n",
      "Epoch 555/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.1654e-05 - val_loss: 0.7807\n",
      "Epoch 556/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.1510e-05 - val_loss: 0.7810\n",
      "Epoch 557/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.1197e-05 - val_loss: 0.7816\n",
      "Epoch 558/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.1094e-05 - val_loss: 0.7826\n",
      "Epoch 559/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.0764e-05 - val_loss: 0.7822\n",
      "Epoch 560/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.0536e-05 - val_loss: 0.7827\n",
      "Epoch 561/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.0273e-05 - val_loss: 0.7831\n",
      "Epoch 562/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.0064e-05 - val_loss: 0.7840\n",
      "Epoch 563/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 3.9864e-05 - val_loss: 0.7838\n",
      "Epoch 564/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.9712e-05 - val_loss: 0.7849\n",
      "Epoch 565/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.9442e-05 - val_loss: 0.7853\n",
      "Epoch 566/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.9188e-05 - val_loss: 0.7853\n",
      "Epoch 567/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.8950e-05 - val_loss: 0.7856\n",
      "Epoch 568/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.8934e-05 - val_loss: 0.7869\n",
      "Epoch 569/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.8545e-05 - val_loss: 0.7863\n",
      "Epoch 570/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.8386e-05 - val_loss: 0.7860\n",
      "Epoch 571/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.8166e-05 - val_loss: 0.7877\n",
      "Epoch 572/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.7946e-05 - val_loss: 0.7884\n",
      "Epoch 573/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.7727e-05 - val_loss: 0.7880\n",
      "Epoch 574/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.7480e-05 - val_loss: 0.7883\n",
      "Epoch 575/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.7428e-05 - val_loss: 0.7898\n",
      "Epoch 576/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.7062e-05 - val_loss: 0.7898\n",
      "Epoch 577/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.7046e-05 - val_loss: 0.7888\n",
      "Epoch 578/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.6805e-05 - val_loss: 0.7892\n",
      "Epoch 579/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.6513e-05 - val_loss: 0.7906\n",
      "Epoch 580/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.6394e-05 - val_loss: 0.7918\n",
      "Epoch 581/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.6125e-05 - val_loss: 0.7914\n",
      "Epoch 582/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5933e-05 - val_loss: 0.7917\n",
      "Epoch 583/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5721e-05 - val_loss: 0.7921\n",
      "Epoch 584/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5512e-05 - val_loss: 0.7927\n",
      "Epoch 585/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5342e-05 - val_loss: 0.7933\n",
      "Epoch 586/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5175e-05 - val_loss: 0.7932\n",
      "Epoch 587/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4963e-05 - val_loss: 0.7937\n",
      "Epoch 588/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4719e-05 - val_loss: 0.7941\n",
      "Epoch 589/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4545e-05 - val_loss: 0.7945\n",
      "Epoch 590/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4368e-05 - val_loss: 0.7944\n",
      "Epoch 591/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4170e-05 - val_loss: 0.7950\n",
      "Epoch 592/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3985e-05 - val_loss: 0.7956\n",
      "Epoch 593/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3841e-05 - val_loss: 0.7964\n",
      "Epoch 594/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3654e-05 - val_loss: 0.7966\n",
      "Epoch 595/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3456e-05 - val_loss: 0.7967\n",
      "Epoch 596/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3279e-05 - val_loss: 0.7971\n",
      "Epoch 597/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3137e-05 - val_loss: 0.7976\n",
      "Epoch 598/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.2894e-05 - val_loss: 0.7981\n",
      "Epoch 599/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.2754e-05 - val_loss: 0.7986\n",
      "Epoch 600/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.2739e-05 - val_loss: 0.7987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xde67777d08>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = X_train, y = y_train, epochs = 600, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xde6b0c3ec8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "\n",
    "# BINARY CLASSIFICATION\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class EarlyStopping in module tensorflow.python.keras.callbacks:\n",
      "\n",
      "class EarlyStopping(Callback)\n",
      " |  EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
      " |  \n",
      " |  Stop training when a monitored metric has stopped improving.\n",
      " |  \n",
      " |  Assuming the goal of a training is to minimize the loss. With this, the\n",
      " |  metric to be monitored would be `'loss'`, and mode would be `'min'`. A\n",
      " |  `model.fit()` training loop will check at end of every epoch whether\n",
      " |  the loss is no longer decreasing, considering the `min_delta` and\n",
      " |  `patience` if applicable. Once it's found no longer decreasing,\n",
      " |  `model.stop_training` is marked True and the training terminates.\n",
      " |  \n",
      " |  The quantity to be monitored needs to be available in `logs` dict.\n",
      " |  To make it so, pass the loss or metrics at `model.compile()`.\n",
      " |  \n",
      " |  Arguments:\n",
      " |    monitor: Quantity to be monitored.\n",
      " |    min_delta: Minimum change in the monitored quantity\n",
      " |        to qualify as an improvement, i.e. an absolute\n",
      " |        change of less than min_delta, will count as no\n",
      " |        improvement.\n",
      " |    patience: Number of epochs with no improvement\n",
      " |        after which training will be stopped.\n",
      " |    verbose: verbosity mode.\n",
      " |    mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode,\n",
      " |        training will stop when the quantity\n",
      " |        monitored has stopped decreasing; in `\"max\"`\n",
      " |        mode it will stop when the quantity\n",
      " |        monitored has stopped increasing; in `\"auto\"`\n",
      " |        mode, the direction is automatically inferred\n",
      " |        from the name of the monitored quantity.\n",
      " |    baseline: Baseline value for the monitored quantity.\n",
      " |        Training will stop if the model doesn't show improvement over the\n",
      " |        baseline.\n",
      " |    restore_best_weights: Whether to restore model weights from\n",
      " |        the epoch with the best value of the monitored quantity.\n",
      " |        If False, the model weights obtained at the last step of\n",
      " |        training are used.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  >>> callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
      " |  >>> # This callback will stop the training when there is no improvement in\n",
      " |  >>> # the validation loss for three consecutive epochs.\n",
      " |  >>> model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n",
      " |  >>> model.compile(tf.keras.optimizers.SGD(), loss='mse')\n",
      " |  >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n",
      " |  ...                     epochs=10, batch_size=1, callbacks=[callback],\n",
      " |  ...                     verbose=0)\n",
      " |  >>> len(history.history['loss'])  # Only 4 epochs are run.\n",
      " |  4\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      EarlyStopping\n",
      " |      Callback\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_monitor_value(self, logs)\n",
      " |  \n",
      " |  on_epoch_end(self, epoch, logs=None)\n",
      " |      Called at the end of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict, metric results for this training epoch, and for the\n",
      " |            validation epoch if validation is performed. Validation result keys\n",
      " |            are prefixed with `val_`.\n",
      " |  \n",
      " |  on_train_begin(self, logs=None)\n",
      " |      Called at the beginning of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_end(self, logs=None)\n",
      " |      Called at the end of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently the output of the last call to `on_epoch_end()`\n",
      " |            is passed to this argument for this method but that may change in\n",
      " |            the future.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Callback:\n",
      " |  \n",
      " |  on_batch_begin(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_begin`.\n",
      " |  \n",
      " |  on_batch_end(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_end`.\n",
      " |  \n",
      " |  on_epoch_begin(self, epoch, logs=None)\n",
      " |      Called at the start of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.predict_step`,\n",
      " |            it typically returns a dict with a key 'outputs' containing\n",
      " |            the model's outputs.\n",
      " |  \n",
      " |  on_predict_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_predict_begin(self, logs=None)\n",
      " |      Called at the beginning of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_end(self, logs=None)\n",
      " |      Called at the end of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the beginning of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.test_step`. Typically,\n",
      " |            the values of the `Model`'s metrics are returned.  Example:\n",
      " |            `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  on_test_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the end of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_test_begin(self, logs=None)\n",
      " |      Called at the beginning of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_end(self, logs=None)\n",
      " |      Called at the end of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently the output of the last call to\n",
      " |            `on_test_batch_end()` is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.train_step`. Typically,\n",
      " |            the values of the `Model`'s metrics are returned.  Example:\n",
      " |            `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  on_train_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  set_model(self, model)\n",
      " |  \n",
      " |  set_params(self, params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Callback:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(EarlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6661 - val_loss: 0.6320\n",
      "Epoch 2/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6069 - val_loss: 0.6034\n",
      "Epoch 3/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5763 - val_loss: 0.5802\n",
      "Epoch 4/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5491 - val_loss: 0.5543\n",
      "Epoch 5/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5182 - val_loss: 0.5272\n",
      "Epoch 6/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4824 - val_loss: 0.5007\n",
      "Epoch 7/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4509 - val_loss: 0.4769\n",
      "Epoch 8/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4190 - val_loss: 0.4556\n",
      "Epoch 9/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3896 - val_loss: 0.4357\n",
      "Epoch 10/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3611 - val_loss: 0.4207\n",
      "Epoch 11/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3348 - val_loss: 0.4076\n",
      "Epoch 12/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3127 - val_loss: 0.3939\n",
      "Epoch 13/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2933 - val_loss: 0.3864\n",
      "Epoch 14/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2757 - val_loss: 0.3805\n",
      "Epoch 15/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2607 - val_loss: 0.3747\n",
      "Epoch 16/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2482 - val_loss: 0.3716\n",
      "Epoch 17/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2369 - val_loss: 0.3683\n",
      "Epoch 18/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2270 - val_loss: 0.3664\n",
      "Epoch 19/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2165 - val_loss: 0.3655\n",
      "Epoch 20/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2061 - val_loss: 0.3656\n",
      "Epoch 21/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1980 - val_loss: 0.3663\n",
      "Epoch 22/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1915 - val_loss: 0.3643\n",
      "Epoch 23/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1819 - val_loss: 0.3638\n",
      "Epoch 24/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1746 - val_loss: 0.3646\n",
      "Epoch 25/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1685 - val_loss: 0.3659\n",
      "Epoch 26/600\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1604 - val_loss: 0.3660\n",
      "Epoch 27/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1551 - val_loss: 0.3669\n",
      "Epoch 28/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1486 - val_loss: 0.3688\n",
      "Epoch 29/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1451 - val_loss: 0.3712\n",
      "Epoch 30/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1376 - val_loss: 0.3718\n",
      "Epoch 31/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1338 - val_loss: 0.3706\n",
      "Epoch 32/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1289 - val_loss: 0.3738\n",
      "Epoch 33/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1255 - val_loss: 0.3791\n",
      "Epoch 34/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1188 - val_loss: 0.3792\n",
      "Epoch 35/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1116 - val_loss: 0.3801\n",
      "Epoch 36/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1072 - val_loss: 0.3828\n",
      "Epoch 37/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1033 - val_loss: 0.3843\n",
      "Epoch 38/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0989 - val_loss: 0.3871\n",
      "Epoch 39/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0957 - val_loss: 0.3896\n",
      "Epoch 40/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0916 - val_loss: 0.3914\n",
      "Epoch 41/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0888 - val_loss: 0.3936\n",
      "Epoch 42/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0848 - val_loss: 0.3947\n",
      "Epoch 43/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0813 - val_loss: 0.3969\n",
      "Epoch 44/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0793 - val_loss: 0.4003\n",
      "Epoch 45/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0759 - val_loss: 0.4038\n",
      "Epoch 46/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0747 - val_loss: 0.4042\n",
      "Epoch 47/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0696 - val_loss: 0.4085\n",
      "Epoch 48/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0689 - val_loss: 0.4101\n",
      "Epoch 00048: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xde6b79dbc8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=600, validation_data=(X_test, y_test), callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xde6baf4c88>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss = pd.DataFrame(model.history.history)\n",
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# BINARY CLSSIFICATION\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.7955WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0005s). Check your callbacks.\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.7372 - val_loss: 0.6918\n",
      "Epoch 2/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.7141 - val_loss: 0.6685\n",
      "Epoch 3/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6936 - val_loss: 0.6497\n",
      "Epoch 4/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6594 - val_loss: 0.6322\n",
      "Epoch 5/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6442 - val_loss: 0.6190\n",
      "Epoch 6/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6269 - val_loss: 0.6040\n",
      "Epoch 7/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6363 - val_loss: 0.5906\n",
      "Epoch 8/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6383 - val_loss: 0.5784\n",
      "Epoch 9/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6306 - val_loss: 0.5700\n",
      "Epoch 10/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5836 - val_loss: 0.5608\n",
      "Epoch 11/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5815 - val_loss: 0.5514\n",
      "Epoch 12/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5599 - val_loss: 0.5430\n",
      "Epoch 13/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.5334\n",
      "Epoch 14/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 0.5208\n",
      "Epoch 15/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5536 - val_loss: 0.5068\n",
      "Epoch 16/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 0.4977\n",
      "Epoch 17/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5172 - val_loss: 0.4860\n",
      "Epoch 18/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5034 - val_loss: 0.4739\n",
      "Epoch 19/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4800 - val_loss: 0.4622\n",
      "Epoch 20/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4839 - val_loss: 0.4499\n",
      "Epoch 21/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4836 - val_loss: 0.4360\n",
      "Epoch 22/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4420 - val_loss: 0.4257\n",
      "Epoch 23/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4876 - val_loss: 0.4207\n",
      "Epoch 24/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4609 - val_loss: 0.4231\n",
      "Epoch 25/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4531 - val_loss: 0.4203\n",
      "Epoch 26/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4327 - val_loss: 0.4076\n",
      "Epoch 27/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3932 - val_loss: 0.3955\n",
      "Epoch 28/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4107 - val_loss: 0.3868\n",
      "Epoch 29/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3621 - val_loss: 0.3757\n",
      "Epoch 30/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3978 - val_loss: 0.3677\n",
      "Epoch 31/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3619 - val_loss: 0.3620\n",
      "Epoch 32/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4409 - val_loss: 0.3608\n",
      "Epoch 33/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3723 - val_loss: 0.3613\n",
      "Epoch 34/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4011 - val_loss: 0.3611\n",
      "Epoch 35/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3117 - val_loss: 0.3524\n",
      "Epoch 36/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3797 - val_loss: 0.3427\n",
      "Epoch 37/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3644 - val_loss: 0.3390\n",
      "Epoch 38/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3709 - val_loss: 0.3379\n",
      "Epoch 39/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3336 - val_loss: 0.3358\n",
      "Epoch 40/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3132 - val_loss: 0.3341\n",
      "Epoch 41/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3351 - val_loss: 0.3273\n",
      "Epoch 42/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3322 - val_loss: 0.3266\n",
      "Epoch 43/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3053 - val_loss: 0.3221\n",
      "Epoch 44/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3416 - val_loss: 0.3189\n",
      "Epoch 45/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2952 - val_loss: 0.3149\n",
      "Epoch 46/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3057 - val_loss: 0.3116\n",
      "Epoch 47/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3110 - val_loss: 0.3187\n",
      "Epoch 48/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3195 - val_loss: 0.3298\n",
      "Epoch 49/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3175 - val_loss: 0.3256\n",
      "Epoch 50/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2958 - val_loss: 0.3215\n",
      "Epoch 51/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3084 - val_loss: 0.3178\n",
      "Epoch 52/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2679 - val_loss: 0.3206\n",
      "Epoch 53/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2624 - val_loss: 0.3158\n",
      "Epoch 54/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2646 - val_loss: 0.3128\n",
      "Epoch 55/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2586 - val_loss: 0.3147\n",
      "Epoch 56/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2490 - val_loss: 0.3257\n",
      "Epoch 57/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2721 - val_loss: 0.3276\n",
      "Epoch 58/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2490 - val_loss: 0.3239\n",
      "Epoch 59/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2223 - val_loss: 0.3205\n",
      "Epoch 60/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2325 - val_loss: 0.3126\n",
      "Epoch 61/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2565 - val_loss: 0.3103\n",
      "Epoch 62/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2586 - val_loss: 0.3059\n",
      "Epoch 63/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2102 - val_loss: 0.3103\n",
      "Epoch 64/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2176 - val_loss: 0.3195\n",
      "Epoch 65/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2383 - val_loss: 0.3186\n",
      "Epoch 66/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1960 - val_loss: 0.3182\n",
      "Epoch 67/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2037 - val_loss: 0.3135\n",
      "Epoch 68/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2268 - val_loss: 0.3103\n",
      "Epoch 69/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2104 - val_loss: 0.3117\n",
      "Epoch 70/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2246 - val_loss: 0.3186\n",
      "Epoch 71/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1967 - val_loss: 0.3241\n",
      "Epoch 72/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2084 - val_loss: 0.3173\n",
      "Epoch 73/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2025 - val_loss: 0.3088\n",
      "Epoch 74/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1732 - val_loss: 0.3081\n",
      "Epoch 75/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1993 - val_loss: 0.3070\n",
      "Epoch 76/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1803 - val_loss: 0.3057\n",
      "Epoch 77/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1935 - val_loss: 0.3059\n",
      "Epoch 78/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1859 - val_loss: 0.3033\n",
      "Epoch 79/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1829 - val_loss: 0.3033\n",
      "Epoch 80/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1552 - val_loss: 0.3119\n",
      "Epoch 81/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1448 - val_loss: 0.3252\n",
      "Epoch 82/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1873 - val_loss: 0.3349\n",
      "Epoch 83/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1385 - val_loss: 0.3270\n",
      "Epoch 84/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2043 - val_loss: 0.3207\n",
      "Epoch 85/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1431 - val_loss: 0.3106\n",
      "Epoch 86/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1582 - val_loss: 0.3074\n",
      "Epoch 87/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1480 - val_loss: 0.3145\n",
      "Epoch 88/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1564 - val_loss: 0.3190\n",
      "Epoch 89/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1474 - val_loss: 0.3192\n",
      "Epoch 90/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1537 - val_loss: 0.3181\n",
      "Epoch 91/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1324 - val_loss: 0.3220\n",
      "Epoch 92/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1485 - val_loss: 0.3248\n",
      "Epoch 93/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1372 - val_loss: 0.3254\n",
      "Epoch 94/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1292 - val_loss: 0.3273\n",
      "Epoch 95/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1452 - val_loss: 0.3311\n",
      "Epoch 96/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1246 - val_loss: 0.3298\n",
      "Epoch 97/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1597 - val_loss: 0.3295\n",
      "Epoch 98/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1063 - val_loss: 0.3332\n",
      "Epoch 99/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1697 - val_loss: 0.3300\n",
      "Epoch 100/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1069 - val_loss: 0.3361\n",
      "Epoch 101/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1329 - val_loss: 0.3426\n",
      "Epoch 102/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1121 - val_loss: 0.3463\n",
      "Epoch 103/600\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1152 - val_loss: 0.3453\n",
      "Epoch 00103: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xde6bb87f88>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=600, validation_data=(X_test, y_test), callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xde6d3955c8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-45-bc83193b8b59>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        35\n",
      "           1       0.92      0.92      0.92        37\n",
      "\n",
      "    accuracy                           0.92        72\n",
      "   macro avg       0.92      0.92      0.92        72\n",
      "weighted avg       0.92      0.92      0.92        72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32  3]\n",
      " [ 3 34]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating the exact same model, including its weights and the optimizer\n",
    "model = tf.keras.models.load_model('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpexooyw_6\\assets\n",
      "Size of h5 model: 69.78515625 KB\n",
      "Size of tflite model: 14.19921875 KB\n",
      "Decreased for factor: 4.914718019257221\n"
     ]
    }
   ],
   "source": [
    "# Converting the model without quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Saving the model to the disk\n",
    "open(\"final_quant.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "# size of the .h5 model\n",
    "h5_in_kb = os.path.getsize(\"final_model.h5\") / 1024\n",
    "print(\"Size of h5 model: {} KB\".format(h5_in_kb))\n",
    "\n",
    "# size of the .tflite model\n",
    "tflite_in_kb = os.path.getsize(\"final_quant.tflite\") / 1024\n",
    "print(\"Size of tflite model: {} KB\".format(tflite_in_kb))\n",
    "print(\"Decreased for factor: {}\".format(h5_in_kb/tflite_in_kb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpxmscptwp\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpxmscptwp\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8960"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the model to the TensorFlow Lite format with float16 quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()\n",
    "# Save to disk\n",
    "open(\"float16_quant.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpenr66hxa\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpenr66hxa\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6624"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the model to the TensorFlow Lite format with dynamic range quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "# Save to disk\n",
    "open(\"final_dynamic_range_quantization.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1488: set_learning_phase (from tensorflow.python.keras.backend) is deprecated and will be removed after 2020-10-11.\n",
      "Instructions for updating:\n",
      "Simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1488: set_learning_phase (from tensorflow.python.keras.backend) is deprecated and will be removed after 2020-10-11.\n",
      "Instructions for updating:\n",
      "Simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpikqzt1yv\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpikqzt1yv\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\admin\\AppData\\Local\\Temp\\tmpikqzt1yv\\variables\\variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\admin\\AppData\\Local\\Temp\\tmpikqzt1yv\\variables\\variables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default', '__saved_model_init_op'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default', '__saved_model_init_op'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input tensors info: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input tensors info: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_6_input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_6_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: serving_default_dense_6_input:0, shape: (-1, 90), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: serving_default_dense_6_input:0, shape: (-1, 90), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:output tensors info: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:output tensors info: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: StatefulPartitionedCall:0, shape: (-1, 1), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: StatefulPartitionedCall:0, shape: (-1, 1), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\admin\\AppData\\Local\\Temp\\tmpikqzt1yv\\variables\\variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\admin\\AppData\\Local\\Temp\\tmpikqzt1yv\\variables\\variables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\util.py:275: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\util.py:275: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\convert_to_constants.py:854: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\convert_to_constants.py:854: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5872"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the model to the tensorFlow lite format with full integer quantization\n",
    "scans = tf.cast(X_test, tf.float32)\n",
    "scans_data = tf.data.Dataset.from_tensor_slices(scans).batch(1)\n",
    "def representative_dataset_gen():\n",
    "  for input in scans_data.take(70):\n",
    "    # Get sample input data as a numpy array in a method of your choosing.\n",
    "    yield [input]\n",
    "    \n",
    "converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(\"final_model.h5\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.float32\n",
    "tflite_model = converter.convert()\n",
    "# Save to disk\n",
    "open(\"final_full_integer_quant.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = pd.read_csv(r\"valitation_set_3_n_5_p.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Distance0</th>\n",
       "      <th>Distance1</th>\n",
       "      <th>Distance2</th>\n",
       "      <th>Distance3</th>\n",
       "      <th>Distance4</th>\n",
       "      <th>Distance5</th>\n",
       "      <th>Distance6</th>\n",
       "      <th>Distance7</th>\n",
       "      <th>Distance8</th>\n",
       "      <th>...</th>\n",
       "      <th>Distance80</th>\n",
       "      <th>Distance81</th>\n",
       "      <th>Distance82</th>\n",
       "      <th>Distance83</th>\n",
       "      <th>Distance84</th>\n",
       "      <th>Distance85</th>\n",
       "      <th>Distance86</th>\n",
       "      <th>Distance87</th>\n",
       "      <th>Distance88</th>\n",
       "      <th>Distance89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>106</td>\n",
       "      <td>105</td>\n",
       "      <td>74</td>\n",
       "      <td>48</td>\n",
       "      <td>102</td>\n",
       "      <td>45</td>\n",
       "      <td>108</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>93</td>\n",
       "      <td>72</td>\n",
       "      <td>93</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>75</td>\n",
       "      <td>7</td>\n",
       "      <td>91</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>75</td>\n",
       "      <td>72</td>\n",
       "      <td>64</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>87</td>\n",
       "      <td>...</td>\n",
       "      <td>89</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>82</td>\n",
       "      <td>8</td>\n",
       "      <td>104</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>102</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>77</td>\n",
       "      <td>193</td>\n",
       "      <td>79</td>\n",
       "      <td>7</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>50</td>\n",
       "      <td>77</td>\n",
       "      <td>80</td>\n",
       "      <td>65</td>\n",
       "      <td>122</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>133</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>54</td>\n",
       "      <td>55</td>\n",
       "      <td>38</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>64</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "      <td>63</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>57</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>8</td>\n",
       "      <td>59</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>61</td>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "      <td>61</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>66</td>\n",
       "      <td>68</td>\n",
       "      <td>7</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>60</td>\n",
       "      <td>48</td>\n",
       "      <td>61</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>82</td>\n",
       "      <td>80</td>\n",
       "      <td>70</td>\n",
       "      <td>61</td>\n",
       "      <td>82</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>54</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>47</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>82</td>\n",
       "      <td>81</td>\n",
       "      <td>70</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>7</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>70</td>\n",
       "      <td>46</td>\n",
       "      <td>6</td>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Distance0  Distance1  Distance2  Distance3  Distance4  Distance5  \\\n",
       "0     0        105        106        105         74         48        102   \n",
       "1     0         76         76         75         72         64         75   \n",
       "2     0         78         78         77        193         79          7   \n",
       "3     1         55         54         55         38         55         55   \n",
       "4     1         60         60         59         57         59         59   \n",
       "5     1         67         66         67         67         66         68   \n",
       "6     1         83         82         80         70         61         82   \n",
       "7     1         81         82         81         70         82         82   \n",
       "\n",
       "   Distance6  Distance7  Distance8  ...  Distance80  Distance81  Distance82  \\\n",
       "0         45        108         46  ...          82          93          72   \n",
       "1         75         75         87  ...          89         102         102   \n",
       "2         78         78          6  ...          76          50          77   \n",
       "3         54         54         54  ...          64          62          63   \n",
       "4          8         59         57  ...          61           7          47   \n",
       "5          7         67         67  ...           9          49          60   \n",
       "6         81          7         69  ...          61          61          64   \n",
       "7          7         81         81  ...          70          46           6   \n",
       "\n",
       "   Distance83  Distance84  Distance85  Distance86  Distance87  Distance88  \\\n",
       "0          93          30          33          75           7          91   \n",
       "1          82           8         104           8          45         102   \n",
       "2          80          65         122           7           7         133   \n",
       "3          62          63          63          63           7          63   \n",
       "4          61          60          60           6          60          60   \n",
       "5          48          61          64           6          54          64   \n",
       "6          60          54          46           7          48          47   \n",
       "7          69          70          70           7           7          72   \n",
       "\n",
       "   Distance89  \n",
       "0          65  \n",
       "1          59  \n",
       "2          58  \n",
       "3           8  \n",
       "4          49  \n",
       "5           7  \n",
       "6          45  \n",
       "7          72  \n",
       "\n",
       "[8 rows x 91 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validate = validation_set.drop('Type', axis = 1).values\n",
    "y_validate = validation_set['Type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[105, 106, 105,  74,  48, 102,  45, 108,  46,  49,  35,   7,   9,\n",
       "         93, 101,  99,  35,  42,  63,  96,   6,  38, 100,  36, 108,  92,\n",
       "          6,  47,  54,  74,  68,  81,  67,  56,  50,   7, 112, 111,  68,\n",
       "        118, 117,  80,  51, 107, 111, 110,  84,   7, 107,  71,  49,  74,\n",
       "        108, 109,  60,   8,  33,  75, 127, 124, 130,  60,  94,  61,   8,\n",
       "        138,   8, 138, 112, 169,   8,  62,  92, 110,  64,   6,  71,  94,\n",
       "        113,  56,  82,  93,  72,  93,  30,  33,  75,   7,  91,  65],\n",
       "       [ 76,  76,  75,  72,  64,  75,  75,  75,  87,  86,  85,   8,  85,\n",
       "         87,  54,  84,  85,  53,   8,  94,   7,  56, 132, 134,  56, 129,\n",
       "         61,  71,   6,  46,  82, 102, 114,  70,   8, 113, 112, 113,  97,\n",
       "          8,  95, 106,   6, 111, 111, 105, 104,  90,  63, 103,  53,  56,\n",
       "        103,  94, 102,   7, 104,  99, 104,   5, 103,  52, 101,  58,   8,\n",
       "        102,   6, 101,  97, 102,   8, 102,  81, 102,  80,  58,  84, 102,\n",
       "        102,   7,  89, 102, 102,  82,   8, 104,   8,  45, 102,  59],\n",
       "       [ 78,  78,  77, 193,  79,   7,  78,  78,   6,  64,  77,   7,  76,\n",
       "         56,  74,  75,  67,   8,  76,  75,  66,  75,  75,   8,  75,  74,\n",
       "         75,  47,  75,  74,  74,  65,  75,   8,  72,  74,  73,  75,  69,\n",
       "         74,  67,   6,  74,  49,  75,  75,  74,   8,  75,  75,  75,  46,\n",
       "         75,  75,  75,  47,  75,  74,  47,   6,  74,  53,   8,  75,   6,\n",
       "         75,  73,  75,  76,  75,   7,  75,  69,  76,  76,   7,   6,  77,\n",
       "         77,   8,  76,  50,  77,  80,  65, 122,   7,   7, 133,  58],\n",
       "       [ 55,  54,  55,  38,  55,  55,  54,  54,  54,  54,   9,  53,  54,\n",
       "         54,   0,  53,  53,   8,  54,  54,  53,  53,  54,  53,  54,  54,\n",
       "         53,  53,  54,  53,  53,  53,  53,  53,  53,  50,  53,  53,  54,\n",
       "         53,  53,   8,  53,  53,  53,   8,  54,  54,  54,  54,  54,   8,\n",
       "         54,  51,  54,  56,  55,  55,  55,   8,  56,  53,  57,  58,   8,\n",
       "         58,  50,  66,  64,  64,  64,  54,  66,  66,  64,  65,  64,   8,\n",
       "         63,  50,  64,  62,  63,  62,  63,  63,  63,   7,  63,   8],\n",
       "       [ 60,  60,  59,  57,  59,  59,   8,  59,  57,  58,   7,  58,  58,\n",
       "         56,  57,  58,   8,   6,  48,  58,  56,  58,  57,  52,  57,  58,\n",
       "         59,  59,  57,  57,  55,  58,  56,  59,  58,  58,  59,  49,  60,\n",
       "          8,  60,   7,   8,   8,  67,   8,  96,  72,  93,   8,  81,   7,\n",
       "         93,   8,  96,  60,   8,  96,  97,   7,  53,  57,  89, 106,  56,\n",
       "        106,   8,  64,  62,  63,  58,  63,  58,  62,  62,  61,  60,  60,\n",
       "         45,  46,  61,   7,  47,  61,  60,  60,   6,  60,  60,  49],\n",
       "       [ 67,  66,  67,  67,  66,  68,   7,  67,  67,  66,   7,  65,  67,\n",
       "         65,  59,  65,  66,  67,  67,  66,  52,  52,  66,  69,  68,  68,\n",
       "          8,  48,  58,  61,  68,  53,  52,  53,   7,   8,  52,  51,  51,\n",
       "         49,  51,  51,  52,  50,  51,  49,  49,  52,  50,  51,  52,   7,\n",
       "         52,  52,  51,   7,  50,  51,  49,   7,  51,  51,   7,  52,  50,\n",
       "         51,  51,  52,  51,  52,  52,  52,   7,   6,  53,  63,   6,  63,\n",
       "         64,  64,   9,  49,  60,  48,  61,  64,   6,  54,  64,   7],\n",
       "       [ 83,  82,  80,  70,  61,  82,  81,   7,  69,   7,  68,  54,  67,\n",
       "          8,   6,  60,   6,  62,  62,  60,  58,   6,  59,  60,  60,  59,\n",
       "          8,  59,  60,  52,  54,  58,  59,  59,  59,  60,  59,  61,  61,\n",
       "         55,  62,   8,  60,   8,  59,  49,  61,  60,  59,  59,  59,  51,\n",
       "         53,  50,  53,   6,   8,  52,  82,  48, 106,  52,  52,  51,  50,\n",
       "        115,  48, 114,  61, 115,  50, 114,  49, 115, 101,  82,  50, 114,\n",
       "        114,  61,  61,  61,  64,  60,  54,  46,   7,  48,  47,  45],\n",
       "       [ 81,  82,  81,  70,  82,  82,   7,  81,  81,  84,  82,  69,  81,\n",
       "          8,  46,  78,  79,  71,  50,  77,  59,  60,  59,  77,  76,  76,\n",
       "         62,  75,   6,  76,  76,  76,  57,  65,  75,  76,  57,  76,  75,\n",
       "         76,  76,  59,  76,   7,  75,   6,  49,  72,  51,  71,  65,  49,\n",
       "         75,  75,  53,   9,  76,  74,  73,   7,  73,  73,  50,  73,  59,\n",
       "         73,   7,  73,  72,  72,  73,  72,   9,  69,  72,  60,  56,   7,\n",
       "         72,  49,  70,  46,   6,  69,  70,  70,   7,   7,  72,  72]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29411765, 0.29691877, 0.32915361, 0.62184874, 0.26373626,\n",
       "        0.60714286, 0.35714286, 0.36486486, 0.19246862, 0.36842105,\n",
       "        0.14522822, 0.03977273, 0.03846154, 0.42081448, 0.50248756,\n",
       "        0.50253807, 0.17326733, 0.2745098 , 0.27155172, 0.39183673,\n",
       "        0.03225806, 0.17511521, 0.5988024 , 0.17307692, 0.77142857,\n",
       "        0.54117647, 0.02764977, 0.31972789, 0.30508475, 0.3627451 ,\n",
       "        0.40963855, 0.28222997, 0.21202532, 0.28571429, 0.32467532,\n",
       "        0.03825137, 0.54634146, 0.54411765, 0.34170854, 0.56730769,\n",
       "        0.58208955, 0.46242775, 0.33552632, 0.41472868, 0.3447205 ,\n",
       "        0.46413502, 0.48837209, 0.04166667, 0.59776536, 0.29957806,\n",
       "        0.21030043, 0.5323741 , 0.53465347, 0.6300578 , 0.23715415,\n",
       "        0.04907975, 0.21019108, 0.35377358, 0.51209677, 0.62944162,\n",
       "        0.55084746, 0.36809816, 0.70149254, 0.33888889, 0.06557377,\n",
       "        0.75      , 0.05755396, 0.88461538, 0.57435897, 0.63059701,\n",
       "        0.04519774, 0.35227273, 0.4972973 , 0.58510638, 0.28699552,\n",
       "        0.02158273, 0.60169492, 0.55621302, 0.4501992 , 0.29946524,\n",
       "        0.47674419, 0.69924812, 0.2962963 , 0.53757225, 0.16483516,\n",
       "        0.04247104, 0.54744526, 0.05223881, 0.3625498 , 0.45138889],\n",
       "       [0.21288515, 0.21288515, 0.23510972, 0.60504202, 0.35164835,\n",
       "        0.44642857, 0.5952381 , 0.25337838, 0.36401674, 0.64661654,\n",
       "        0.3526971 , 0.04545455, 0.36324786, 0.39366516, 0.26865672,\n",
       "        0.42639594, 0.42079208, 0.34640523, 0.03448276, 0.38367347,\n",
       "        0.03763441, 0.25806452, 0.79041916, 0.64423077, 0.4       ,\n",
       "        0.75882353, 0.28110599, 0.4829932 , 0.03389831, 0.2254902 ,\n",
       "        0.4939759 , 0.3554007 , 0.36075949, 0.35714286, 0.05194805,\n",
       "        0.61748634, 0.54634146, 0.55392157, 0.48743719, 0.03846154,\n",
       "        0.47263682, 0.61271676, 0.03947368, 0.43023256, 0.3447205 ,\n",
       "        0.44303797, 0.60465116, 0.53571429, 0.35195531, 0.43459916,\n",
       "        0.22746781, 0.4028777 , 0.50990099, 0.5433526 , 0.40316206,\n",
       "        0.04294479, 0.66242038, 0.46698113, 0.41935484, 0.02538071,\n",
       "        0.43644068, 0.3190184 , 0.75373134, 0.32222222, 0.06557377,\n",
       "        0.55434783, 0.04316547, 0.6474359 , 0.4974359 , 0.38059701,\n",
       "        0.04519774, 0.57954545, 0.43783784, 0.54255319, 0.35874439,\n",
       "        0.20863309, 0.71186441, 0.6035503 , 0.4063745 , 0.03743316,\n",
       "        0.51744186, 0.76691729, 0.41975309, 0.47398844, 0.04395604,\n",
       "        0.13384813, 0.05839416, 0.3358209 , 0.4063745 , 0.40972222],\n",
       "       [0.21848739, 0.21848739, 0.24137931, 1.62184874, 0.43406593,\n",
       "        0.04166667, 0.61904762, 0.26351351, 0.0251046 , 0.48120301,\n",
       "        0.31950207, 0.03977273, 0.32478632, 0.25339367, 0.3681592 ,\n",
       "        0.38071066, 0.33168317, 0.05228758, 0.32758621, 0.30612245,\n",
       "        0.35483871, 0.34562212, 0.4491018 , 0.03846154, 0.53571429,\n",
       "        0.43529412, 0.34562212, 0.31972789, 0.42372881, 0.3627451 ,\n",
       "        0.44578313, 0.22648084, 0.23734177, 0.04081633, 0.46753247,\n",
       "        0.40437158, 0.35609756, 0.36764706, 0.34673367, 0.35576923,\n",
       "        0.33333333, 0.03468208, 0.48684211, 0.18992248, 0.23291925,\n",
       "        0.3164557 , 0.43023256, 0.04761905, 0.41899441, 0.3164557 ,\n",
       "        0.32188841, 0.33093525, 0.37128713, 0.43352601, 0.29644269,\n",
       "        0.28834356, 0.47770701, 0.3490566 , 0.18951613, 0.03045685,\n",
       "        0.31355932, 0.32515337, 0.05970149, 0.41666667, 0.04918033,\n",
       "        0.4076087 , 0.52517986, 0.48076923, 0.38974359, 0.27985075,\n",
       "        0.03954802, 0.42613636, 0.37297297, 0.40425532, 0.34080717,\n",
       "        0.02517986, 0.05084746, 0.4556213 , 0.30677291, 0.04278075,\n",
       "        0.44186047, 0.37593985, 0.31687243, 0.46242775, 0.35714286,\n",
       "        0.15701416, 0.05109489, 0.05223881, 0.52988048, 0.40277778],\n",
       "       [0.15406162, 0.1512605 , 0.17241379, 0.31932773, 0.3021978 ,\n",
       "        0.32738095, 0.42857143, 0.18243243, 0.22594142, 0.40601504,\n",
       "        0.0373444 , 0.30113636, 0.23076923, 0.24434389, 0.        ,\n",
       "        0.26903553, 0.26237624, 0.05228758, 0.23275862, 0.22040816,\n",
       "        0.28494624, 0.24423963, 0.32335329, 0.25480769, 0.38571429,\n",
       "        0.31764706, 0.24423963, 0.36054422, 0.30508475, 0.25980392,\n",
       "        0.31927711, 0.18466899, 0.16772152, 0.27040816, 0.34415584,\n",
       "        0.27322404, 0.25853659, 0.25980392, 0.27135678, 0.25480769,\n",
       "        0.26368159, 0.04624277, 0.34868421, 0.20542636, 0.16459627,\n",
       "        0.03375527, 0.31395349, 0.32142857, 0.30167598, 0.2278481 ,\n",
       "        0.23175966, 0.05755396, 0.26732673, 0.29479769, 0.21343874,\n",
       "        0.34355828, 0.35031847, 0.25943396, 0.22177419, 0.04060914,\n",
       "        0.23728814, 0.32515337, 0.42537313, 0.32222222, 0.06557377,\n",
       "        0.31521739, 0.35971223, 0.42307692, 0.32820513, 0.23880597,\n",
       "        0.36158192, 0.30681818, 0.35675676, 0.35106383, 0.28699552,\n",
       "        0.23381295, 0.54237288, 0.04733728, 0.25099602, 0.26737968,\n",
       "        0.37209302, 0.46616541, 0.25925926, 0.3583815 , 0.34615385,\n",
       "        0.08108108, 0.45985401, 0.05223881, 0.25099602, 0.05555556],\n",
       "       [0.16806723, 0.16806723, 0.18495298, 0.4789916 , 0.32417582,\n",
       "        0.35119048, 0.06349206, 0.19932432, 0.23849372, 0.43609023,\n",
       "        0.02904564, 0.32954545, 0.24786325, 0.25339367, 0.28358209,\n",
       "        0.29441624, 0.03960396, 0.03921569, 0.20689655, 0.23673469,\n",
       "        0.30107527, 0.26728111, 0.34131737, 0.25      , 0.40714286,\n",
       "        0.34117647, 0.2718894 , 0.40136054, 0.3220339 , 0.27941176,\n",
       "        0.3313253 , 0.20209059, 0.17721519, 0.30102041, 0.37662338,\n",
       "        0.31693989, 0.28780488, 0.24019608, 0.30150754, 0.03846154,\n",
       "        0.29850746, 0.04046243, 0.05263158, 0.03100775, 0.20807453,\n",
       "        0.03375527, 0.55813953, 0.42857143, 0.51955307, 0.03375527,\n",
       "        0.34763948, 0.05035971, 0.46039604, 0.04624277, 0.37944664,\n",
       "        0.36809816, 0.05095541, 0.45283019, 0.39112903, 0.03553299,\n",
       "        0.22457627, 0.34969325, 0.6641791 , 0.58888889, 0.45901639,\n",
       "        0.57608696, 0.05755396, 0.41025641, 0.31794872, 0.23507463,\n",
       "        0.32768362, 0.35795455, 0.31351351, 0.32978723, 0.27802691,\n",
       "        0.21942446, 0.50847458, 0.35502959, 0.17928287, 0.2459893 ,\n",
       "        0.35465116, 0.05263158, 0.19341564, 0.35260116, 0.32967033,\n",
       "        0.07722008, 0.04379562, 0.44776119, 0.23904382, 0.34027778],\n",
       "       [0.18767507, 0.18487395, 0.21003135, 0.56302521, 0.36263736,\n",
       "        0.4047619 , 0.05555556, 0.22635135, 0.28033473, 0.4962406 ,\n",
       "        0.02904564, 0.36931818, 0.28632479, 0.29411765, 0.29353234,\n",
       "        0.32994924, 0.32673267, 0.4379085 , 0.2887931 , 0.26938776,\n",
       "        0.27956989, 0.23963134, 0.39520958, 0.33173077, 0.48571429,\n",
       "        0.4       , 0.03686636, 0.32653061, 0.32768362, 0.29901961,\n",
       "        0.40963855, 0.18466899, 0.16455696, 0.27040816, 0.04545455,\n",
       "        0.04371585, 0.25365854, 0.25      , 0.25628141, 0.23557692,\n",
       "        0.25373134, 0.29479769, 0.34210526, 0.19379845, 0.15838509,\n",
       "        0.20675105, 0.28488372, 0.30952381, 0.27932961, 0.21518987,\n",
       "        0.22317597, 0.05035971, 0.25742574, 0.30057803, 0.20158103,\n",
       "        0.04294479, 0.31847134, 0.24056604, 0.19758065, 0.03553299,\n",
       "        0.21610169, 0.31288344, 0.05223881, 0.28888889, 0.40983607,\n",
       "        0.27717391, 0.36690647, 0.33333333, 0.26153846, 0.19402985,\n",
       "        0.29378531, 0.29545455, 0.03783784, 0.03191489, 0.23766816,\n",
       "        0.22661871, 0.05084746, 0.37278107, 0.25498008, 0.34224599,\n",
       "        0.05232558, 0.36842105, 0.24691358, 0.27745665, 0.33516484,\n",
       "        0.08236808, 0.04379562, 0.40298507, 0.25498008, 0.04861111],\n",
       "       [0.232493  , 0.22969188, 0.2507837 , 0.58823529, 0.33516484,\n",
       "        0.48809524, 0.64285714, 0.02364865, 0.28870293, 0.05263158,\n",
       "        0.28215768, 0.30681818, 0.28632479, 0.0361991 , 0.02985075,\n",
       "        0.30456853, 0.02970297, 0.40522876, 0.26724138, 0.24489796,\n",
       "        0.31182796, 0.02764977, 0.35329341, 0.28846154, 0.42857143,\n",
       "        0.34705882, 0.03686636, 0.40136054, 0.33898305, 0.25490196,\n",
       "        0.3253012 , 0.20209059, 0.18670886, 0.30102041, 0.38311688,\n",
       "        0.32786885, 0.28780488, 0.29901961, 0.30653266, 0.26442308,\n",
       "        0.30845771, 0.04624277, 0.39473684, 0.03100775, 0.18322981,\n",
       "        0.20675105, 0.35465116, 0.35714286, 0.32960894, 0.24894515,\n",
       "        0.25321888, 0.36690647, 0.26237624, 0.28901734, 0.20948617,\n",
       "        0.03680982, 0.05095541, 0.24528302, 0.33064516, 0.24365482,\n",
       "        0.44915254, 0.3190184 , 0.3880597 , 0.28333333, 0.40983607,\n",
       "        0.625     , 0.34532374, 0.73076923, 0.31282051, 0.42910448,\n",
       "        0.28248588, 0.64772727, 0.26486486, 0.61170213, 0.4529148 ,\n",
       "        0.29496403, 0.42372881, 0.67455621, 0.45418327, 0.32620321,\n",
       "        0.35465116, 0.45864662, 0.26337449, 0.34682081, 0.2967033 ,\n",
       "        0.05920206, 0.05109489, 0.35820896, 0.187251  , 0.3125    ],\n",
       "       [0.22689076, 0.22969188, 0.2539185 , 0.58823529, 0.45054945,\n",
       "        0.48809524, 0.05555556, 0.27364865, 0.33891213, 0.63157895,\n",
       "        0.34024896, 0.39204545, 0.34615385, 0.0361991 , 0.22885572,\n",
       "        0.39593909, 0.39108911, 0.46405229, 0.21551724, 0.31428571,\n",
       "        0.3172043 , 0.2764977 , 0.35329341, 0.37019231, 0.54285714,\n",
       "        0.44705882, 0.28571429, 0.51020408, 0.03389831, 0.37254902,\n",
       "        0.45783133, 0.26480836, 0.18037975, 0.33163265, 0.48701299,\n",
       "        0.41530055, 0.27804878, 0.37254902, 0.37688442, 0.36538462,\n",
       "        0.37810945, 0.34104046, 0.5       , 0.02713178, 0.23291925,\n",
       "        0.02531646, 0.28488372, 0.42857143, 0.2849162 , 0.29957806,\n",
       "        0.27896996, 0.35251799, 0.37128713, 0.43352601, 0.20948617,\n",
       "        0.05521472, 0.48407643, 0.3490566 , 0.29435484, 0.03553299,\n",
       "        0.30932203, 0.44785276, 0.37313433, 0.40555556, 0.48360656,\n",
       "        0.39673913, 0.05035971, 0.46794872, 0.36923077, 0.26865672,\n",
       "        0.41242938, 0.40909091, 0.04864865, 0.36702128, 0.32286996,\n",
       "        0.21582734, 0.47457627, 0.04142012, 0.28685259, 0.26203209,\n",
       "        0.40697674, 0.34586466, 0.02469136, 0.39884393, 0.38461538,\n",
       "        0.09009009, 0.05109489, 0.05223881, 0.28685259, 0.5       ]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = model.predict_classes(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}