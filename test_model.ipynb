{
 "cells": [
  {
   "source": [
    "## Data Analysis/Visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
=======
   "execution_count": 1,
>>>>>>> f5a04238fb6132e73ebdd262e3fb2621a4cbb6c1
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Type  Distance0  Distance1  Distance2  Distance3  Distance4  Distance5  \\\n",
       "0     1        147        277        173        233        268        226   \n",
       "1     1         96        265        184        175        107        116   \n",
       "2     1        244         12        204        299          9        173   \n",
       "3     0         42        208        182        121        227         17   \n",
       "4     0         45        271        144         36        292        119   \n",
       "5     0         92        296        220        275         46        200   \n",
       "6     1        267        212        205          9        256        292   \n",
       "7     0        248        122        108        111        190        202   \n",
       "8     1        228        217        159          3         99          1   \n",
       "9     1         49         98         43        215         73        200   \n",
       "\n",
       "   Distance6  Distance7  Distance8  ...  Distance80  Distance81  Distance82  \\\n",
       "0        231        103        118  ...         264         197         185   \n",
       "1         49         97        144  ...         278         298         236   \n",
       "2        140         20        133  ...         271         239          92   \n",
       "3        115         38         26  ...         284         115         138   \n",
       "4        266         56         66  ...         278          18          92   \n",
       "5         29        218         61  ...          65         194         251   \n",
       "6        292        179        115  ...         208         199          44   \n",
       "7        111        186        122  ...          48         227         227   \n",
       "8        229        123        292  ...          15         177         272   \n",
       "9        237         81         39  ...          31           9         152   \n",
       "\n",
       "   Distance83  Distance84  Distance85  Distance86  Distance87  Distance88  \\\n",
       "0         254         243         234         205         299         201   \n",
       "1          70          53         270         125         272         235   \n",
       "2         198         136         189         131         164         140   \n",
       "3         102         206          69         258         156          74   \n",
       "4          10          47         192         162         200         115   \n",
       "5         250         126         166         125          67          12   \n",
       "6          43          84          99         299         116         210   \n",
       "7          36         187          49         291         238          58   \n",
       "8          75          47         156         160         133          86   \n",
       "9          80         241          19         205          13          97   \n",
       "\n",
       "   Distance89  \n",
       "0         264  \n",
       "1         224  \n",
       "2          80  \n",
       "3         200  \n",
       "4         106  \n",
       "5         159  \n",
       "6         104  \n",
       "7         193  \n",
       "8          54  \n",
       "9         266  \n",
       "\n",
       "[10 rows x 91 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Type</th>\n      <th>Distance0</th>\n      <th>Distance1</th>\n      <th>Distance2</th>\n      <th>Distance3</th>\n      <th>Distance4</th>\n      <th>Distance5</th>\n      <th>Distance6</th>\n      <th>Distance7</th>\n      <th>Distance8</th>\n      <th>...</th>\n      <th>Distance80</th>\n      <th>Distance81</th>\n      <th>Distance82</th>\n      <th>Distance83</th>\n      <th>Distance84</th>\n      <th>Distance85</th>\n      <th>Distance86</th>\n      <th>Distance87</th>\n      <th>Distance88</th>\n      <th>Distance89</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>147</td>\n      <td>277</td>\n      <td>173</td>\n      <td>233</td>\n      <td>268</td>\n      <td>226</td>\n      <td>231</td>\n      <td>103</td>\n      <td>118</td>\n      <td>...</td>\n      <td>264</td>\n      <td>197</td>\n      <td>185</td>\n      <td>254</td>\n      <td>243</td>\n      <td>234</td>\n      <td>205</td>\n      <td>299</td>\n      <td>201</td>\n      <td>264</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>96</td>\n      <td>265</td>\n      <td>184</td>\n      <td>175</td>\n      <td>107</td>\n      <td>116</td>\n      <td>49</td>\n      <td>97</td>\n      <td>144</td>\n      <td>...</td>\n      <td>278</td>\n      <td>298</td>\n      <td>236</td>\n      <td>70</td>\n      <td>53</td>\n      <td>270</td>\n      <td>125</td>\n      <td>272</td>\n      <td>235</td>\n      <td>224</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>244</td>\n      <td>12</td>\n      <td>204</td>\n      <td>299</td>\n      <td>9</td>\n      <td>173</td>\n      <td>140</td>\n      <td>20</td>\n      <td>133</td>\n      <td>...</td>\n      <td>271</td>\n      <td>239</td>\n      <td>92</td>\n      <td>198</td>\n      <td>136</td>\n      <td>189</td>\n      <td>131</td>\n      <td>164</td>\n      <td>140</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>42</td>\n      <td>208</td>\n      <td>182</td>\n      <td>121</td>\n      <td>227</td>\n      <td>17</td>\n      <td>115</td>\n      <td>38</td>\n      <td>26</td>\n      <td>...</td>\n      <td>284</td>\n      <td>115</td>\n      <td>138</td>\n      <td>102</td>\n      <td>206</td>\n      <td>69</td>\n      <td>258</td>\n      <td>156</td>\n      <td>74</td>\n      <td>200</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>45</td>\n      <td>271</td>\n      <td>144</td>\n      <td>36</td>\n      <td>292</td>\n      <td>119</td>\n      <td>266</td>\n      <td>56</td>\n      <td>66</td>\n      <td>...</td>\n      <td>278</td>\n      <td>18</td>\n      <td>92</td>\n      <td>10</td>\n      <td>47</td>\n      <td>192</td>\n      <td>162</td>\n      <td>200</td>\n      <td>115</td>\n      <td>106</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>92</td>\n      <td>296</td>\n      <td>220</td>\n      <td>275</td>\n      <td>46</td>\n      <td>200</td>\n      <td>29</td>\n      <td>218</td>\n      <td>61</td>\n      <td>...</td>\n      <td>65</td>\n      <td>194</td>\n      <td>251</td>\n      <td>250</td>\n      <td>126</td>\n      <td>166</td>\n      <td>125</td>\n      <td>67</td>\n      <td>12</td>\n      <td>159</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>267</td>\n      <td>212</td>\n      <td>205</td>\n      <td>9</td>\n      <td>256</td>\n      <td>292</td>\n      <td>292</td>\n      <td>179</td>\n      <td>115</td>\n      <td>...</td>\n      <td>208</td>\n      <td>199</td>\n      <td>44</td>\n      <td>43</td>\n      <td>84</td>\n      <td>99</td>\n      <td>299</td>\n      <td>116</td>\n      <td>210</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>248</td>\n      <td>122</td>\n      <td>108</td>\n      <td>111</td>\n      <td>190</td>\n      <td>202</td>\n      <td>111</td>\n      <td>186</td>\n      <td>122</td>\n      <td>...</td>\n      <td>48</td>\n      <td>227</td>\n      <td>227</td>\n      <td>36</td>\n      <td>187</td>\n      <td>49</td>\n      <td>291</td>\n      <td>238</td>\n      <td>58</td>\n      <td>193</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>228</td>\n      <td>217</td>\n      <td>159</td>\n      <td>3</td>\n      <td>99</td>\n      <td>1</td>\n      <td>229</td>\n      <td>123</td>\n      <td>292</td>\n      <td>...</td>\n      <td>15</td>\n      <td>177</td>\n      <td>272</td>\n      <td>75</td>\n      <td>47</td>\n      <td>156</td>\n      <td>160</td>\n      <td>133</td>\n      <td>86</td>\n      <td>54</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>49</td>\n      <td>98</td>\n      <td>43</td>\n      <td>215</td>\n      <td>73</td>\n      <td>200</td>\n      <td>237</td>\n      <td>81</td>\n      <td>39</td>\n      <td>...</td>\n      <td>31</td>\n      <td>9</td>\n      <td>152</td>\n      <td>80</td>\n      <td>241</td>\n      <td>19</td>\n      <td>205</td>\n      <td>13</td>\n      <td>97</td>\n      <td>266</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 91 columns</p>\n</div>"
     },
<<<<<<< HEAD
     "execution_count": 3,
=======
>>>>>>> f5a04238fb6132e73ebdd262e3fb2621a4cbb6c1
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            Type  Distance0   Distance1   Distance2   Distance3   Distance4  \\\n",
       "count  10.000000   10.00000   10.000000   10.000000   10.000000   10.000000   \n",
       "mean    0.600000  145.80000  197.800000  162.200000  147.700000  156.700000   \n",
       "std     0.516398   92.62565   92.310828   53.115806  108.709654  101.915052   \n",
       "min     0.000000   42.00000   12.000000   43.000000    3.000000    9.000000   \n",
       "25%     0.000000   59.75000  143.500000  147.750000   54.750000   79.500000   \n",
       "50%     1.000000  121.50000  214.500000  177.500000  148.000000  148.500000   \n",
       "75%     1.000000  240.00000  269.500000  199.000000  228.500000  248.750000   \n",
       "max     1.000000  267.00000  296.000000  220.000000  299.000000  292.000000   \n",
       "\n",
       "        Distance5   Distance6   Distance7   Distance8  ...  Distance80  \\\n",
       "count   10.000000   10.000000   10.000000   10.000000  ...    10.00000   \n",
       "mean   154.600000  169.900000  110.100000  111.600000  ...   174.20000   \n",
       "std     91.874552   92.927032   66.356696   75.608935  ...   118.26693   \n",
       "min      1.000000   29.000000   20.000000   26.000000  ...    15.00000   \n",
       "25%    116.750000  112.000000   62.250000   62.250000  ...    52.25000   \n",
       "50%    186.500000  184.500000  100.000000  116.500000  ...   236.00000   \n",
       "75%    201.500000  235.500000  165.000000  130.250000  ...   276.25000   \n",
       "max    292.000000  292.000000  218.000000  292.000000  ...   284.00000   \n",
       "\n",
       "       Distance81  Distance82  Distance83  Distance84  Distance85  Distance86  \\\n",
       "count    10.00000   10.000000   10.000000   10.000000   10.000000   10.000000   \n",
       "mean    167.30000  168.900000  111.800000  137.000000  144.300000  196.100000   \n",
       "std      93.43215   77.585007   89.319402   78.528127   82.483736   66.998259   \n",
       "min       9.00000   44.000000   10.000000   47.000000   19.000000  125.000000   \n",
       "25%     130.50000  103.500000   49.750000   60.750000   76.500000  138.250000   \n",
       "50%     195.50000  168.500000   77.500000  131.000000  161.000000  183.500000   \n",
       "75%     220.00000  233.750000  174.000000  201.250000  191.250000  244.750000   \n",
       "max     298.00000  272.000000  254.000000  243.000000  270.000000  299.000000   \n",
       "\n",
       "       Distance87  Distance88  Distance89  \n",
       "count    10.00000   10.000000   10.000000  \n",
       "mean    165.80000  122.800000  165.000000  \n",
       "std      89.54676   72.680121   76.213151  \n",
       "min      13.00000   12.000000   54.000000  \n",
       "25%     120.25000   77.000000  104.500000  \n",
       "50%     160.00000  106.000000  176.000000  \n",
       "75%     228.50000  185.750000  218.000000  \n",
       "max     299.00000  235.000000  266.000000  \n",
       "\n",
       "[8 rows x 91 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Type</th>\n      <th>Distance0</th>\n      <th>Distance1</th>\n      <th>Distance2</th>\n      <th>Distance3</th>\n      <th>Distance4</th>\n      <th>Distance5</th>\n      <th>Distance6</th>\n      <th>Distance7</th>\n      <th>Distance8</th>\n      <th>...</th>\n      <th>Distance80</th>\n      <th>Distance81</th>\n      <th>Distance82</th>\n      <th>Distance83</th>\n      <th>Distance84</th>\n      <th>Distance85</th>\n      <th>Distance86</th>\n      <th>Distance87</th>\n      <th>Distance88</th>\n      <th>Distance89</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>10.000000</td>\n      <td>10.00000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>...</td>\n      <td>10.00000</td>\n      <td>10.00000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>10.00000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.600000</td>\n      <td>145.80000</td>\n      <td>197.800000</td>\n      <td>162.200000</td>\n      <td>147.700000</td>\n      <td>156.700000</td>\n      <td>154.600000</td>\n      <td>169.900000</td>\n      <td>110.100000</td>\n      <td>111.600000</td>\n      <td>...</td>\n      <td>174.20000</td>\n      <td>167.30000</td>\n      <td>168.900000</td>\n      <td>111.800000</td>\n      <td>137.000000</td>\n      <td>144.300000</td>\n      <td>196.100000</td>\n      <td>165.80000</td>\n      <td>122.800000</td>\n      <td>165.000000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.516398</td>\n      <td>92.62565</td>\n      <td>92.310828</td>\n      <td>53.115806</td>\n      <td>108.709654</td>\n      <td>101.915052</td>\n      <td>91.874552</td>\n      <td>92.927032</td>\n      <td>66.356696</td>\n      <td>75.608935</td>\n      <td>...</td>\n      <td>118.26693</td>\n      <td>93.43215</td>\n      <td>77.585007</td>\n      <td>89.319402</td>\n      <td>78.528127</td>\n      <td>82.483736</td>\n      <td>66.998259</td>\n      <td>89.54676</td>\n      <td>72.680121</td>\n      <td>76.213151</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>42.00000</td>\n      <td>12.000000</td>\n      <td>43.000000</td>\n      <td>3.000000</td>\n      <td>9.000000</td>\n      <td>1.000000</td>\n      <td>29.000000</td>\n      <td>20.000000</td>\n      <td>26.000000</td>\n      <td>...</td>\n      <td>15.00000</td>\n      <td>9.00000</td>\n      <td>44.000000</td>\n      <td>10.000000</td>\n      <td>47.000000</td>\n      <td>19.000000</td>\n      <td>125.000000</td>\n      <td>13.00000</td>\n      <td>12.000000</td>\n      <td>54.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>59.75000</td>\n      <td>143.500000</td>\n      <td>147.750000</td>\n      <td>54.750000</td>\n      <td>79.500000</td>\n      <td>116.750000</td>\n      <td>112.000000</td>\n      <td>62.250000</td>\n      <td>62.250000</td>\n      <td>...</td>\n      <td>52.25000</td>\n      <td>130.50000</td>\n      <td>103.500000</td>\n      <td>49.750000</td>\n      <td>60.750000</td>\n      <td>76.500000</td>\n      <td>138.250000</td>\n      <td>120.25000</td>\n      <td>77.000000</td>\n      <td>104.500000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n      <td>121.50000</td>\n      <td>214.500000</td>\n      <td>177.500000</td>\n      <td>148.000000</td>\n      <td>148.500000</td>\n      <td>186.500000</td>\n      <td>184.500000</td>\n      <td>100.000000</td>\n      <td>116.500000</td>\n      <td>...</td>\n      <td>236.00000</td>\n      <td>195.50000</td>\n      <td>168.500000</td>\n      <td>77.500000</td>\n      <td>131.000000</td>\n      <td>161.000000</td>\n      <td>183.500000</td>\n      <td>160.00000</td>\n      <td>106.000000</td>\n      <td>176.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n      <td>240.00000</td>\n      <td>269.500000</td>\n      <td>199.000000</td>\n      <td>228.500000</td>\n      <td>248.750000</td>\n      <td>201.500000</td>\n      <td>235.500000</td>\n      <td>165.000000</td>\n      <td>130.250000</td>\n      <td>...</td>\n      <td>276.25000</td>\n      <td>220.00000</td>\n      <td>233.750000</td>\n      <td>174.000000</td>\n      <td>201.250000</td>\n      <td>191.250000</td>\n      <td>244.750000</td>\n      <td>228.50000</td>\n      <td>185.750000</td>\n      <td>218.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>267.00000</td>\n      <td>296.000000</td>\n      <td>220.000000</td>\n      <td>299.000000</td>\n      <td>292.000000</td>\n      <td>292.000000</td>\n      <td>292.000000</td>\n      <td>218.000000</td>\n      <td>292.000000</td>\n      <td>...</td>\n      <td>284.00000</td>\n      <td>298.00000</td>\n      <td>272.000000</td>\n      <td>254.000000</td>\n      <td>243.000000</td>\n      <td>270.000000</td>\n      <td>299.000000</td>\n      <td>299.00000</td>\n      <td>235.000000</td>\n      <td>266.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 91 columns</p>\n</div>"
     },
<<<<<<< HEAD
     "execution_count": 4,
=======
>>>>>>> f5a04238fb6132e73ebdd262e3fb2621a4cbb6c1
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
=======
>>>>>>> f5a04238fb6132e73ebdd262e3fb2621a4cbb6c1
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "columns = df.columns\n",
    "\n",
    "X = df[columns[1:]]\n",
    "y = df['Type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Distance0  Distance1  Distance2  Distance3  Distance4  Distance5  \\\n",
       "0        147        277        173        233        268        226   \n",
       "1         96        265        184        175        107        116   \n",
       "2        244         12        204        299          9        173   \n",
       "3         42        208        182        121        227         17   \n",
       "4         45        271        144         36        292        119   \n",
       "\n",
       "   Distance6  Distance7  Distance8  Distance9  ...  Distance80  Distance81  \\\n",
       "0        231        103        118         45  ...         264         197   \n",
       "1         49         97        144        276  ...         278         298   \n",
       "2        140         20        133        198  ...         271         239   \n",
       "3        115         38         26        216  ...         284         115   \n",
       "4        266         56         66        183  ...         278          18   \n",
       "\n",
       "   Distance82  Distance83  Distance84  Distance85  Distance86  Distance87  \\\n",
       "0         185         254         243         234         205         299   \n",
       "1         236          70          53         270         125         272   \n",
       "2          92         198         136         189         131         164   \n",
       "3         138         102         206          69         258         156   \n",
       "4          92          10          47         192         162         200   \n",
       "\n",
       "   Distance88  Distance89  \n",
       "0         201         264  \n",
       "1         235         224  \n",
       "2         140          80  \n",
       "3          74         200  \n",
       "4         115         106  \n",
       "\n",
       "[5 rows x 90 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Distance0</th>\n      <th>Distance1</th>\n      <th>Distance2</th>\n      <th>Distance3</th>\n      <th>Distance4</th>\n      <th>Distance5</th>\n      <th>Distance6</th>\n      <th>Distance7</th>\n      <th>Distance8</th>\n      <th>Distance9</th>\n      <th>...</th>\n      <th>Distance80</th>\n      <th>Distance81</th>\n      <th>Distance82</th>\n      <th>Distance83</th>\n      <th>Distance84</th>\n      <th>Distance85</th>\n      <th>Distance86</th>\n      <th>Distance87</th>\n      <th>Distance88</th>\n      <th>Distance89</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>147</td>\n      <td>277</td>\n      <td>173</td>\n      <td>233</td>\n      <td>268</td>\n      <td>226</td>\n      <td>231</td>\n      <td>103</td>\n      <td>118</td>\n      <td>45</td>\n      <td>...</td>\n      <td>264</td>\n      <td>197</td>\n      <td>185</td>\n      <td>254</td>\n      <td>243</td>\n      <td>234</td>\n      <td>205</td>\n      <td>299</td>\n      <td>201</td>\n      <td>264</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>96</td>\n      <td>265</td>\n      <td>184</td>\n      <td>175</td>\n      <td>107</td>\n      <td>116</td>\n      <td>49</td>\n      <td>97</td>\n      <td>144</td>\n      <td>276</td>\n      <td>...</td>\n      <td>278</td>\n      <td>298</td>\n      <td>236</td>\n      <td>70</td>\n      <td>53</td>\n      <td>270</td>\n      <td>125</td>\n      <td>272</td>\n      <td>235</td>\n      <td>224</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>244</td>\n      <td>12</td>\n      <td>204</td>\n      <td>299</td>\n      <td>9</td>\n      <td>173</td>\n      <td>140</td>\n      <td>20</td>\n      <td>133</td>\n      <td>198</td>\n      <td>...</td>\n      <td>271</td>\n      <td>239</td>\n      <td>92</td>\n      <td>198</td>\n      <td>136</td>\n      <td>189</td>\n      <td>131</td>\n      <td>164</td>\n      <td>140</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>42</td>\n      <td>208</td>\n      <td>182</td>\n      <td>121</td>\n      <td>227</td>\n      <td>17</td>\n      <td>115</td>\n      <td>38</td>\n      <td>26</td>\n      <td>216</td>\n      <td>...</td>\n      <td>284</td>\n      <td>115</td>\n      <td>138</td>\n      <td>102</td>\n      <td>206</td>\n      <td>69</td>\n      <td>258</td>\n      <td>156</td>\n      <td>74</td>\n      <td>200</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>45</td>\n      <td>271</td>\n      <td>144</td>\n      <td>36</td>\n      <td>292</td>\n      <td>119</td>\n      <td>266</td>\n      <td>56</td>\n      <td>66</td>\n      <td>183</td>\n      <td>...</td>\n      <td>278</td>\n      <td>18</td>\n      <td>92</td>\n      <td>10</td>\n      <td>47</td>\n      <td>192</td>\n      <td>162</td>\n      <td>200</td>\n      <td>115</td>\n      <td>106</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 90 columns</p>\n</div>"
     },
<<<<<<< HEAD
     "execution_count": 7,
=======
>>>>>>> f5a04238fb6132e73ebdd262e3fb2621a4cbb6c1
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    0\n",
       "4    0\n",
       "Name: Type, dtype: int64"
      ]
     },
<<<<<<< HEAD
     "execution_count": 8,
=======
>>>>>>> f5a04238fb6132e73ebdd262e3fb2621a4cbb6c1
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
<<<<<<< HEAD
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)"
=======
    "y.head()"
>>>>>>> f5a04238fb6132e73ebdd262e3fb2621a4cbb6c1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 9,
>>>>>>> f5a04238fb6132e73ebdd262e3fb2621a4cbb6c1
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": null,
>>>>>>> f5a04238fb6132e73ebdd262e3fb2621a4cbb6c1
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 90)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30, activation = 'relu'))\n",
    "model.add(Dense(15, activation = 'relu'))\n",
    "\n",
    "# BINARY CLASSIFICATION\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.7914 - val_loss: 0.4744\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7557 - val_loss: 0.5110\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7242 - val_loss: 0.5457\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6967 - val_loss: 0.5757\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6731 - val_loss: 0.6022\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6508 - val_loss: 0.6267\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6300 - val_loss: 0.6490\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6125 - val_loss: 0.6682\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5965 - val_loss: 0.6828\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5806 - val_loss: 0.6910\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5648 - val_loss: 0.6968\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5500 - val_loss: 0.6997\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5368 - val_loss: 0.7009\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5238 - val_loss: 0.6987\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5107 - val_loss: 0.6952\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4979 - val_loss: 0.6922\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4863 - val_loss: 0.6894\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4744 - val_loss: 0.6867\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.4626 - val_loss: 0.6857\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4522 - val_loss: 0.6869\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4421 - val_loss: 0.6908\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4316 - val_loss: 0.6970\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4207 - val_loss: 0.7029\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4103 - val_loss: 0.7076\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4001 - val_loss: 0.7114\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3899 - val_loss: 0.7144\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3795 - val_loss: 0.7173\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3693 - val_loss: 0.7201\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3593 - val_loss: 0.7221\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3494 - val_loss: 0.7241\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3397 - val_loss: 0.7275\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3301 - val_loss: 0.7325\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3205 - val_loss: 0.7402\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3111 - val_loss: 0.7487\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3018 - val_loss: 0.7573\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2928 - val_loss: 0.7666\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2836 - val_loss: 0.7775\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2743 - val_loss: 0.7901\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2649 - val_loss: 0.8043\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2554 - val_loss: 0.8197\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2464 - val_loss: 0.8357\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2375 - val_loss: 0.8502\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2288 - val_loss: 0.8641\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2202 - val_loss: 0.8770\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2120 - val_loss: 0.8905\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2039 - val_loss: 0.9050\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1961 - val_loss: 0.9200\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1885 - val_loss: 0.9381\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1811 - val_loss: 0.9572\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1737 - val_loss: 0.9771\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1664 - val_loss: 0.9980\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1591 - val_loss: 1.0189\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1520 - val_loss: 1.0414\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1452 - val_loss: 1.0626\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1385 - val_loss: 1.0824\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1320 - val_loss: 1.1014\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1257 - val_loss: 1.1195\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1196 - val_loss: 1.1371\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1139 - val_loss: 1.1556\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1084 - val_loss: 1.1743\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1031 - val_loss: 1.1945\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0981 - val_loss: 1.2145\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0934 - val_loss: 1.2347\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0888 - val_loss: 1.2539\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0845 - val_loss: 1.2712\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0804 - val_loss: 1.2864\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0766 - val_loss: 1.2993\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0731 - val_loss: 1.3106\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0697 - val_loss: 1.3208\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0665 - val_loss: 1.3300\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0635 - val_loss: 1.3393\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0607 - val_loss: 1.3505\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0580 - val_loss: 1.3636\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0553 - val_loss: 1.3779\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0528 - val_loss: 1.3933\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0505 - val_loss: 1.4098\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0482 - val_loss: 1.4274\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0461 - val_loss: 1.4461\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0441 - val_loss: 1.4652\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0421 - val_loss: 1.4853\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0404 - val_loss: 1.5038\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0387 - val_loss: 1.5209\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0371 - val_loss: 1.5356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0357 - val_loss: 1.5489\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0343 - val_loss: 1.5624\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0329 - val_loss: 1.5740\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0316 - val_loss: 1.5841\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0304 - val_loss: 1.5927\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0293 - val_loss: 1.6011\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0282 - val_loss: 1.6095\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0271 - val_loss: 1.6189\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0262 - val_loss: 1.6288\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0252 - val_loss: 1.6389\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0243 - val_loss: 1.6495\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0235 - val_loss: 1.6606\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0227 - val_loss: 1.6727\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0219 - val_loss: 1.6844\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0212 - val_loss: 1.6958\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0205 - val_loss: 1.7073\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0198 - val_loss: 1.7182\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0192 - val_loss: 1.7279\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0186 - val_loss: 1.7364\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0181 - val_loss: 1.7452\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0175 - val_loss: 1.7544\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0170 - val_loss: 1.7634\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0165 - val_loss: 1.7732\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0160 - val_loss: 1.7843\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0156 - val_loss: 1.7963\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0152 - val_loss: 1.8091\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0147 - val_loss: 1.8216\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0143 - val_loss: 1.8322\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0140 - val_loss: 1.8407\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0136 - val_loss: 1.8477\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0133 - val_loss: 1.8533\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0129 - val_loss: 1.8588\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0126 - val_loss: 1.8644\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0123 - val_loss: 1.8714\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0120 - val_loss: 1.8798\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0117 - val_loss: 1.8890\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0114 - val_loss: 1.8977\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0112 - val_loss: 1.9059\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0109 - val_loss: 1.9142\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0107 - val_loss: 1.9226\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0104 - val_loss: 1.9307\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0102 - val_loss: 1.9384\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0100 - val_loss: 1.9465\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0097 - val_loss: 1.9552\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0095 - val_loss: 1.9640\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0093 - val_loss: 1.9728\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0091 - val_loss: 1.9810\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0089 - val_loss: 1.9891\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0087 - val_loss: 1.9976\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0086 - val_loss: 2.0074\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0084 - val_loss: 2.0188\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0082 - val_loss: 2.0305\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0080 - val_loss: 2.0420\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0078 - val_loss: 2.0531\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0077 - val_loss: 2.0639\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0075 - val_loss: 2.0753\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0074 - val_loss: 2.0870\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0072 - val_loss: 2.0979\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0071 - val_loss: 2.1090\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0069 - val_loss: 2.1203\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0068 - val_loss: 2.1316\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0067 - val_loss: 2.1419\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0065 - val_loss: 2.1514\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0064 - val_loss: 2.1602\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0063 - val_loss: 2.1687\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0062 - val_loss: 2.1772\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0061 - val_loss: 2.1855\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0060 - val_loss: 2.1938\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0058 - val_loss: 2.2020\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0057 - val_loss: 2.2103\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0056 - val_loss: 2.2183\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0055 - val_loss: 2.2262\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0054 - val_loss: 2.2341\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0053 - val_loss: 2.2419\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0052 - val_loss: 2.2497\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0051 - val_loss: 2.2576\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0051 - val_loss: 2.2659\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0050 - val_loss: 2.2738\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0049 - val_loss: 2.2814\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0048 - val_loss: 2.2885\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0047 - val_loss: 2.2954\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0046 - val_loss: 2.3028\n",
      "Epoch 166/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0046 - val_loss: 2.3105\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0045 - val_loss: 2.3179\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0044 - val_loss: 2.3256\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0043 - val_loss: 2.3331\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0043 - val_loss: 2.3403\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0042 - val_loss: 2.3470\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0041 - val_loss: 2.3536\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0041 - val_loss: 2.3606\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0040 - val_loss: 2.3675\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0039 - val_loss: 2.3739\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0039 - val_loss: 2.3796\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0038 - val_loss: 2.3858\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0038 - val_loss: 2.3921\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0037 - val_loss: 2.3984\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0037 - val_loss: 2.4052\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0036 - val_loss: 2.4115\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0036 - val_loss: 2.4176\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0035 - val_loss: 2.4241\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0035 - val_loss: 2.4314\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0034 - val_loss: 2.4379\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0034 - val_loss: 2.4437\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0033 - val_loss: 2.4499\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0033 - val_loss: 2.4564\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0032 - val_loss: 2.4625\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0032 - val_loss: 2.4681\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0031 - val_loss: 2.4740\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 2.4797\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0031 - val_loss: 2.4852\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0030 - val_loss: 2.4903\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0030 - val_loss: 2.4956\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0029 - val_loss: 2.5011\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0029 - val_loss: 2.5067\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0029 - val_loss: 2.5123\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0028 - val_loss: 2.5183\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0028 - val_loss: 2.5238\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0028 - val_loss: 2.5284\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 2.5332\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 2.5386\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 2.5441\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 2.5495\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0026 - val_loss: 2.5548\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0026 - val_loss: 2.5598\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0025 - val_loss: 2.5646\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0025 - val_loss: 2.5692\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0025 - val_loss: 2.5734\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0024 - val_loss: 2.5781\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0024 - val_loss: 2.5830\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 2.5880\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0024 - val_loss: 2.5928\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0023 - val_loss: 2.5972\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0023 - val_loss: 2.6016\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0023 - val_loss: 2.6066\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0023 - val_loss: 2.6119\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0022 - val_loss: 2.6166\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0022 - val_loss: 2.6208\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0022 - val_loss: 2.6248\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0022 - val_loss: 2.6289\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0021 - val_loss: 2.6335\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 2.6382\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 2.6425\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 2.6464\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 2.6507\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 2.6550\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 2.6592\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 2.6631\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 2.6676\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 2.6722\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 2.6763\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 2.6799\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 2.6839\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 2.6881\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 2.6923\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 2.6964\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 2.7005\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 2.7042\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 2.7074\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 2.7112\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0018 - val_loss: 2.7154\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 2.7196\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 2.7235\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 2.7271\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 2.7307\n",
      "Epoch 248/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 2.7341\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 2.7377\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 2.7414\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 2.7454\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 2.7488\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 2.7523\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 2.7560\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 2.7598\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 2.7633\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 2.7661\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 2.7695\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 2.7729\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 2.7765\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 2.7806\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 2.7843\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 2.7876\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 2.7907\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 2.7939\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 2.7972\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 2.8007\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 2.8042\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 2.8079\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 2.8107\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 2.8139\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 2.8171\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 2.8204\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 2.8240\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 2.8276\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 2.8307\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 2.8334\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0013 - val_loss: 2.8362\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 2.8394\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 2.8433\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 2.8471\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0012 - val_loss: 2.8503\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.8529\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.8555\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.8587\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.8619\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.8652\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.8687\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.8718\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.8748\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.8778\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.8812\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 2.8849\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 2.8882\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 2.8910\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 2.8939\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 2.8969\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 2.9003\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 2.9042\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 2.9075\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 2.9104\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 2.9133\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 2.9166\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 2.9201\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 2.9233\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 2.9266\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 2.9297\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 2.9326\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 2.9355\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 2.9391\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.9934e-04 - val_loss: 2.9422\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.9210e-04 - val_loss: 2.9449\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.8495e-04 - val_loss: 2.9478\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.7786e-04 - val_loss: 2.9509\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.7083e-04 - val_loss: 2.9539\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.6384e-04 - val_loss: 2.9570\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.5697e-04 - val_loss: 2.9597\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.5018e-04 - val_loss: 2.9625\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.4349e-04 - val_loss: 2.9654\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.3683e-04 - val_loss: 2.9684\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.3021e-04 - val_loss: 2.9714\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.2369e-04 - val_loss: 2.9746\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.1716e-04 - val_loss: 2.9781\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.1082e-04 - val_loss: 2.9810\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.0453e-04 - val_loss: 2.9835\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.9826e-04 - val_loss: 2.9859\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.9205e-04 - val_loss: 2.9888\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.8591e-04 - val_loss: 2.9920\n",
      "Epoch 329/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step - loss: 8.7979e-04 - val_loss: 2.9952\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.7387e-04 - val_loss: 2.9978\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - ETA: 0s - loss: 8.6790e-0 - 0s 11ms/step - loss: 8.6790e-04 - val_loss: 3.0000\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.6201e-04 - val_loss: 3.0025\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 8.5624e-04 - val_loss: 3.0052\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.5046e-04 - val_loss: 3.0081\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.4472e-04 - val_loss: 3.0110\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.3908e-04 - val_loss: 3.0140\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.3352e-04 - val_loss: 3.0163\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.2799e-04 - val_loss: 3.0187\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.2252e-04 - val_loss: 3.0215\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.1710e-04 - val_loss: 3.0244\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.1171e-04 - val_loss: 3.0270\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.0642e-04 - val_loss: 3.0294\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.0116e-04 - val_loss: 3.0319\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.9595e-04 - val_loss: 3.0347\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.9081e-04 - val_loss: 3.0375\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.8573e-04 - val_loss: 3.0401\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.8069e-04 - val_loss: 3.0425\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.7565e-04 - val_loss: 3.0450\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.7072e-04 - val_loss: 3.0474\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.6584e-04 - val_loss: 3.0498\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.6099e-04 - val_loss: 3.0523\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.5617e-04 - val_loss: 3.0549\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.5138e-04 - val_loss: 3.0575\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.4669e-04 - val_loss: 3.0602\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.4206e-04 - val_loss: 3.0630\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.3739e-04 - val_loss: 3.0655\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.3280e-04 - val_loss: 3.0677\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.2827e-04 - val_loss: 3.0698\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.2380e-04 - val_loss: 3.0720\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.1935e-04 - val_loss: 3.0744\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.1496e-04 - val_loss: 3.0773\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.1055e-04 - val_loss: 3.0801\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.0624e-04 - val_loss: 3.0824\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.0195e-04 - val_loss: 3.0847\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.9770e-04 - val_loss: 3.0868\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.9349e-04 - val_loss: 3.0891\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.8931e-04 - val_loss: 3.0912\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.8520e-04 - val_loss: 3.0935\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.8110e-04 - val_loss: 3.0959\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.7704e-04 - val_loss: 3.0985\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.7301e-04 - val_loss: 3.1013\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.6905e-04 - val_loss: 3.1035\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.6510e-04 - val_loss: 3.1057\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 6.6118e-04 - val_loss: 3.1074\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.5734e-04 - val_loss: 3.1099\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.5352e-04 - val_loss: 3.1126\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.4974e-04 - val_loss: 3.1151\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.4596e-04 - val_loss: 3.1174\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.4221e-04 - val_loss: 3.1196\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.3851e-04 - val_loss: 3.1220\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.3484e-04 - val_loss: 3.1247\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.3121e-04 - val_loss: 3.1271\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.2760e-04 - val_loss: 3.1295\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.2403e-04 - val_loss: 3.1314\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 6.2052e-04 - val_loss: 3.1338\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.1698e-04 - val_loss: 3.1362\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.1350e-04 - val_loss: 3.1386\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.1006e-04 - val_loss: 3.1409\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 6.0663e-04 - val_loss: 3.1431\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.0323e-04 - val_loss: 3.1450\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.9993e-04 - val_loss: 3.1475\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.9656e-04 - val_loss: 3.1502\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.9325e-04 - val_loss: 3.1529\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.8998e-04 - val_loss: 3.1552\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.8674e-04 - val_loss: 3.1572\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.8352e-04 - val_loss: 3.1594\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.8032e-04 - val_loss: 3.1617\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.7716e-04 - val_loss: 3.1645\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.7401e-04 - val_loss: 3.1669\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.7086e-04 - val_loss: 3.1689\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.6772e-04 - val_loss: 3.1707\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.6459e-04 - val_loss: 3.1725\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.6153e-04 - val_loss: 3.1747\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.5847e-04 - val_loss: 3.1776\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.5539e-04 - val_loss: 3.1802\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.5236e-04 - val_loss: 3.1822\n",
      "Epoch 407/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step - loss: 5.4935e-04 - val_loss: 3.1841\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.4638e-04 - val_loss: 3.1865\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.4342e-04 - val_loss: 3.1890\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.4049e-04 - val_loss: 3.1913\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.3758e-04 - val_loss: 3.1934\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.3469e-04 - val_loss: 3.1955\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.3182e-04 - val_loss: 3.1979\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.2899e-04 - val_loss: 3.2001\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.2617e-04 - val_loss: 3.2022\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.2337e-04 - val_loss: 3.2045\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.2060e-04 - val_loss: 3.2069\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.1785e-04 - val_loss: 3.2090\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.1511e-04 - val_loss: 3.2113\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.1240e-04 - val_loss: 3.2133\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.0970e-04 - val_loss: 3.2152\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.0707e-04 - val_loss: 3.2176\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.0443e-04 - val_loss: 3.2200\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.0180e-04 - val_loss: 3.2219\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.9921e-04 - val_loss: 3.2238\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.9664e-04 - val_loss: 3.2260\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.9408e-04 - val_loss: 3.2283\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.9155e-04 - val_loss: 3.2306\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.8903e-04 - val_loss: 3.2327\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.8651e-04 - val_loss: 3.2348\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.8403e-04 - val_loss: 3.2375\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.8155e-04 - val_loss: 3.2400\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.7911e-04 - val_loss: 3.2420\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.7665e-04 - val_loss: 3.2440\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.7421e-04 - val_loss: 3.2461\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.7178e-04 - val_loss: 3.2484\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.6935e-04 - val_loss: 3.2508\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.6693e-04 - val_loss: 3.2529\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.6453e-04 - val_loss: 3.2550\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.6214e-04 - val_loss: 3.2573\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.5975e-04 - val_loss: 3.2595\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.5740e-04 - val_loss: 3.2617\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.5505e-04 - val_loss: 3.2638\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.5272e-04 - val_loss: 3.2664\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.5038e-04 - val_loss: 3.2689\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.4808e-04 - val_loss: 3.2711\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.4578e-04 - val_loss: 3.2732\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.4351e-04 - val_loss: 3.2756\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.4125e-04 - val_loss: 3.2778\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.3901e-04 - val_loss: 3.2799\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.3679e-04 - val_loss: 3.2818\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.3457e-04 - val_loss: 3.2838\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.3238e-04 - val_loss: 3.2862\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.3020e-04 - val_loss: 3.2885\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.2804e-04 - val_loss: 3.2908\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.2590e-04 - val_loss: 3.2931\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.2377e-04 - val_loss: 3.2950\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.2164e-04 - val_loss: 3.2970\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.1954e-04 - val_loss: 3.2992\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.1747e-04 - val_loss: 3.3014\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.1538e-04 - val_loss: 3.3034\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.1333e-04 - val_loss: 3.3054\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.1129e-04 - val_loss: 3.3074\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.0927e-04 - val_loss: 3.3094\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.0726e-04 - val_loss: 3.3115\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.0526e-04 - val_loss: 3.3136\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.0327e-04 - val_loss: 3.3157\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.0131e-04 - val_loss: 3.3179\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.9935e-04 - val_loss: 3.3202\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.9742e-04 - val_loss: 3.3223\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.9549e-04 - val_loss: 3.3242\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.9358e-04 - val_loss: 3.3261\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.9169e-04 - val_loss: 3.3283\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.8980e-04 - val_loss: 3.3307\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.8794e-04 - val_loss: 3.3327\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.8607e-04 - val_loss: 3.3347\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.8423e-04 - val_loss: 3.3366\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.8240e-04 - val_loss: 3.3386\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.8059e-04 - val_loss: 3.3408\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.7878e-04 - val_loss: 3.3429\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.7699e-04 - val_loss: 3.3448\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.7522e-04 - val_loss: 3.3469\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.7346e-04 - val_loss: 3.3490\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.7171e-04 - val_loss: 3.3508\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.6997e-04 - val_loss: 3.3526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.6824e-04 - val_loss: 3.3545\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.6653e-04 - val_loss: 3.3565\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.6484e-04 - val_loss: 3.3588\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.6314e-04 - val_loss: 3.3609\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.6147e-04 - val_loss: 3.3627\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.5980e-04 - val_loss: 3.3646\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.5814e-04 - val_loss: 3.3667\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.5650e-04 - val_loss: 3.3687\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.5484e-04 - val_loss: 3.3705\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.5318e-04 - val_loss: 3.3724\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.5154e-04 - val_loss: 3.3747\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.4989e-04 - val_loss: 3.3769\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.4826e-04 - val_loss: 3.3786\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.4663e-04 - val_loss: 3.3803\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.4501e-04 - val_loss: 3.3820\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.4341e-04 - val_loss: 3.3840\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.4181e-04 - val_loss: 3.3861\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.4023e-04 - val_loss: 3.3881\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.3865e-04 - val_loss: 3.3899\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.3708e-04 - val_loss: 3.3919\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.3552e-04 - val_loss: 3.3939\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.3398e-04 - val_loss: 3.3957\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.3245e-04 - val_loss: 3.3976\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.3092e-04 - val_loss: 3.3994\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.2941e-04 - val_loss: 3.4015\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.2791e-04 - val_loss: 3.4035\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.2641e-04 - val_loss: 3.4050\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.2493e-04 - val_loss: 3.4067\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.2346e-04 - val_loss: 3.4085\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.2200e-04 - val_loss: 3.4104\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.2055e-04 - val_loss: 3.4121\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.1911e-04 - val_loss: 3.4141\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.1768e-04 - val_loss: 3.4158\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.1627e-04 - val_loss: 3.4177\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.1486e-04 - val_loss: 3.4194\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.1346e-04 - val_loss: 3.4211\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.1207e-04 - val_loss: 3.4228\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.1069e-04 - val_loss: 3.4247\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0931e-04 - val_loss: 3.4267\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0796e-04 - val_loss: 3.4284\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0661e-04 - val_loss: 3.4301\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0526e-04 - val_loss: 3.4319\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0393e-04 - val_loss: 3.4339\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0262e-04 - val_loss: 3.4356\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.0132e-04 - val_loss: 3.4372\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.0002e-04 - val_loss: 3.4386\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9875e-04 - val_loss: 3.4405\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9746e-04 - val_loss: 3.4427\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9620e-04 - val_loss: 3.4448\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9495e-04 - val_loss: 3.4465\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9370e-04 - val_loss: 3.4479\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9245e-04 - val_loss: 3.4493\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9122e-04 - val_loss: 3.4508\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.8999e-04 - val_loss: 3.4525\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.8878e-04 - val_loss: 3.4545\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.8756e-04 - val_loss: 3.4565\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.8637e-04 - val_loss: 3.4585\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.8518e-04 - val_loss: 3.4601\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.8399e-04 - val_loss: 3.4615\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.8280e-04 - val_loss: 3.4627\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.8164e-04 - val_loss: 3.4643\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.8048e-04 - val_loss: 3.4663\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.7932e-04 - val_loss: 3.4682\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.7818e-04 - val_loss: 3.4699\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.7704e-04 - val_loss: 3.4713\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.7590e-04 - val_loss: 3.4727\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.7478e-04 - val_loss: 3.4743\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.7367e-04 - val_loss: 3.4762\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.7255e-04 - val_loss: 3.4781\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.7145e-04 - val_loss: 3.4799\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.7036e-04 - val_loss: 3.4815\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6926e-04 - val_loss: 3.4829\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6816e-04 - val_loss: 3.4844\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6706e-04 - val_loss: 3.4859\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.6597e-04 - val_loss: 3.4874\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6490e-04 - val_loss: 3.4892\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6380e-04 - val_loss: 3.4909\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.6273e-04 - val_loss: 3.4925\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6166e-04 - val_loss: 3.4940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6059e-04 - val_loss: 3.4955\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.5953e-04 - val_loss: 3.4972\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.5848e-04 - val_loss: 3.4988\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.5743e-04 - val_loss: 3.5006\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.5638e-04 - val_loss: 3.5022\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.5535e-04 - val_loss: 3.5039\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.5432e-04 - val_loss: 3.5055\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.5329e-04 - val_loss: 3.5070\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.5227e-04 - val_loss: 3.5085\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.5126e-04 - val_loss: 3.5099\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.5025e-04 - val_loss: 3.5114\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4925e-04 - val_loss: 3.5131\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4826e-04 - val_loss: 3.5146\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.4727e-04 - val_loss: 3.5162\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4630e-04 - val_loss: 3.5181\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4533e-04 - val_loss: 3.5198\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.4436e-04 - val_loss: 3.5212\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4339e-04 - val_loss: 3.5227\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4244e-04 - val_loss: 3.5243\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4149e-04 - val_loss: 3.5259\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4054e-04 - val_loss: 3.5275\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3960e-04 - val_loss: 3.5290\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.3867e-04 - val_loss: 3.5307\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3774e-04 - val_loss: 3.5324\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3682e-04 - val_loss: 3.5340\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3590e-04 - val_loss: 3.5354\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3499e-04 - val_loss: 3.5369\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3409e-04 - val_loss: 3.5388\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3320e-04 - val_loss: 3.5402\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.3231e-04 - val_loss: 3.5415\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.3142e-04 - val_loss: 3.5429\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3054e-04 - val_loss: 3.5446\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.2967e-04 - val_loss: 3.5463\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.2880e-04 - val_loss: 3.5480\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.2794e-04 - val_loss: 3.5494\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.2708e-04 - val_loss: 3.5507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x51de4f21c8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = X_train, y = y_train, epochs = 600, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x51de5db3c8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('test_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating the exact same model, including its weights and the optimizer\n",
    "model = tf.keras.models.load_model('test_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp9yddax8l\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp9yddax8l\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of h5 model: 68.12109375 KB\n",
      "Size of tflite model: 14.1640625 KB\n",
      "Decreased for factor: 4.809431880860452\n"
     ]
    }
   ],
   "source": [
    "# Converting the model without quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Saving the model to the disk\n",
    "open(\"test_quant.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "# size of the .h5 model\n",
    "h5_in_kb = os.path.getsize(\"test_model.h5\") / 1024\n",
    "print(\"Size of h5 model: {} KB\".format(h5_in_kb))\n",
    "\n",
    "# size of the .tflite model\n",
    "tflite_in_kb = os.path.getsize(\"test_quant.tflite\") / 1024\n",
    "print(\"Size of tflite model: {} KB\".format(tflite_in_kb))\n",
    "print(\"Decreased for factor: {}\".format(h5_in_kb/tflite_in_kb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "d213bab112ee6222d3c7ae0b13bad4fa123e684288282252b000db52d3893657"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}