{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\\\Users\\admin\\Desktop\\Door-Detection\\Door-Detection\\test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Distance0</th>\n",
       "      <th>Distance1</th>\n",
       "      <th>Distance2</th>\n",
       "      <th>Distance3</th>\n",
       "      <th>Distance4</th>\n",
       "      <th>Distance5</th>\n",
       "      <th>Distance6</th>\n",
       "      <th>Distance7</th>\n",
       "      <th>Distance8</th>\n",
       "      <th>...</th>\n",
       "      <th>Distance80</th>\n",
       "      <th>Distance81</th>\n",
       "      <th>Distance82</th>\n",
       "      <th>Distance83</th>\n",
       "      <th>Distance84</th>\n",
       "      <th>Distance85</th>\n",
       "      <th>Distance86</th>\n",
       "      <th>Distance87</th>\n",
       "      <th>Distance88</th>\n",
       "      <th>Distance89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>147</td>\n",
       "      <td>277</td>\n",
       "      <td>173</td>\n",
       "      <td>233</td>\n",
       "      <td>268</td>\n",
       "      <td>226</td>\n",
       "      <td>231</td>\n",
       "      <td>103</td>\n",
       "      <td>118</td>\n",
       "      <td>...</td>\n",
       "      <td>264</td>\n",
       "      <td>197</td>\n",
       "      <td>185</td>\n",
       "      <td>254</td>\n",
       "      <td>243</td>\n",
       "      <td>234</td>\n",
       "      <td>205</td>\n",
       "      <td>299</td>\n",
       "      <td>201</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>265</td>\n",
       "      <td>184</td>\n",
       "      <td>175</td>\n",
       "      <td>107</td>\n",
       "      <td>116</td>\n",
       "      <td>49</td>\n",
       "      <td>97</td>\n",
       "      <td>144</td>\n",
       "      <td>...</td>\n",
       "      <td>278</td>\n",
       "      <td>298</td>\n",
       "      <td>236</td>\n",
       "      <td>70</td>\n",
       "      <td>53</td>\n",
       "      <td>270</td>\n",
       "      <td>125</td>\n",
       "      <td>272</td>\n",
       "      <td>235</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>244</td>\n",
       "      <td>12</td>\n",
       "      <td>204</td>\n",
       "      <td>299</td>\n",
       "      <td>9</td>\n",
       "      <td>173</td>\n",
       "      <td>140</td>\n",
       "      <td>20</td>\n",
       "      <td>133</td>\n",
       "      <td>...</td>\n",
       "      <td>271</td>\n",
       "      <td>239</td>\n",
       "      <td>92</td>\n",
       "      <td>198</td>\n",
       "      <td>136</td>\n",
       "      <td>189</td>\n",
       "      <td>131</td>\n",
       "      <td>164</td>\n",
       "      <td>140</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>208</td>\n",
       "      <td>182</td>\n",
       "      <td>121</td>\n",
       "      <td>227</td>\n",
       "      <td>17</td>\n",
       "      <td>115</td>\n",
       "      <td>38</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>284</td>\n",
       "      <td>115</td>\n",
       "      <td>138</td>\n",
       "      <td>102</td>\n",
       "      <td>206</td>\n",
       "      <td>69</td>\n",
       "      <td>258</td>\n",
       "      <td>156</td>\n",
       "      <td>74</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>271</td>\n",
       "      <td>144</td>\n",
       "      <td>36</td>\n",
       "      <td>292</td>\n",
       "      <td>119</td>\n",
       "      <td>266</td>\n",
       "      <td>56</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>278</td>\n",
       "      <td>18</td>\n",
       "      <td>92</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>192</td>\n",
       "      <td>162</td>\n",
       "      <td>200</td>\n",
       "      <td>115</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>296</td>\n",
       "      <td>220</td>\n",
       "      <td>275</td>\n",
       "      <td>46</td>\n",
       "      <td>200</td>\n",
       "      <td>29</td>\n",
       "      <td>218</td>\n",
       "      <td>61</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>194</td>\n",
       "      <td>251</td>\n",
       "      <td>250</td>\n",
       "      <td>126</td>\n",
       "      <td>166</td>\n",
       "      <td>125</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>267</td>\n",
       "      <td>212</td>\n",
       "      <td>205</td>\n",
       "      <td>9</td>\n",
       "      <td>256</td>\n",
       "      <td>292</td>\n",
       "      <td>292</td>\n",
       "      <td>179</td>\n",
       "      <td>115</td>\n",
       "      <td>...</td>\n",
       "      <td>208</td>\n",
       "      <td>199</td>\n",
       "      <td>44</td>\n",
       "      <td>43</td>\n",
       "      <td>84</td>\n",
       "      <td>99</td>\n",
       "      <td>299</td>\n",
       "      <td>116</td>\n",
       "      <td>210</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>248</td>\n",
       "      <td>122</td>\n",
       "      <td>108</td>\n",
       "      <td>111</td>\n",
       "      <td>190</td>\n",
       "      <td>202</td>\n",
       "      <td>111</td>\n",
       "      <td>186</td>\n",
       "      <td>122</td>\n",
       "      <td>...</td>\n",
       "      <td>48</td>\n",
       "      <td>227</td>\n",
       "      <td>227</td>\n",
       "      <td>36</td>\n",
       "      <td>187</td>\n",
       "      <td>49</td>\n",
       "      <td>291</td>\n",
       "      <td>238</td>\n",
       "      <td>58</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>228</td>\n",
       "      <td>217</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>229</td>\n",
       "      <td>123</td>\n",
       "      <td>292</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>177</td>\n",
       "      <td>272</td>\n",
       "      <td>75</td>\n",
       "      <td>47</td>\n",
       "      <td>156</td>\n",
       "      <td>160</td>\n",
       "      <td>133</td>\n",
       "      <td>86</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>215</td>\n",
       "      <td>73</td>\n",
       "      <td>200</td>\n",
       "      <td>237</td>\n",
       "      <td>81</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "      <td>152</td>\n",
       "      <td>80</td>\n",
       "      <td>241</td>\n",
       "      <td>19</td>\n",
       "      <td>205</td>\n",
       "      <td>13</td>\n",
       "      <td>97</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Distance0  Distance1  Distance2  Distance3  Distance4  Distance5  \\\n",
       "0     1        147        277        173        233        268        226   \n",
       "1     1         96        265        184        175        107        116   \n",
       "2     1        244         12        204        299          9        173   \n",
       "3     0         42        208        182        121        227         17   \n",
       "4     0         45        271        144         36        292        119   \n",
       "5     0         92        296        220        275         46        200   \n",
       "6     1        267        212        205          9        256        292   \n",
       "7     0        248        122        108        111        190        202   \n",
       "8     1        228        217        159          3         99          1   \n",
       "9     1         49         98         43        215         73        200   \n",
       "\n",
       "   Distance6  Distance7  Distance8  ...  Distance80  Distance81  Distance82  \\\n",
       "0        231        103        118  ...         264         197         185   \n",
       "1         49         97        144  ...         278         298         236   \n",
       "2        140         20        133  ...         271         239          92   \n",
       "3        115         38         26  ...         284         115         138   \n",
       "4        266         56         66  ...         278          18          92   \n",
       "5         29        218         61  ...          65         194         251   \n",
       "6        292        179        115  ...         208         199          44   \n",
       "7        111        186        122  ...          48         227         227   \n",
       "8        229        123        292  ...          15         177         272   \n",
       "9        237         81         39  ...          31           9         152   \n",
       "\n",
       "   Distance83  Distance84  Distance85  Distance86  Distance87  Distance88  \\\n",
       "0         254         243         234         205         299         201   \n",
       "1          70          53         270         125         272         235   \n",
       "2         198         136         189         131         164         140   \n",
       "3         102         206          69         258         156          74   \n",
       "4          10          47         192         162         200         115   \n",
       "5         250         126         166         125          67          12   \n",
       "6          43          84          99         299         116         210   \n",
       "7          36         187          49         291         238          58   \n",
       "8          75          47         156         160         133          86   \n",
       "9          80         241          19         205          13          97   \n",
       "\n",
       "   Distance89  \n",
       "0         264  \n",
       "1         224  \n",
       "2          80  \n",
       "3         200  \n",
       "4         106  \n",
       "5         159  \n",
       "6         104  \n",
       "7         193  \n",
       "8          54  \n",
       "9         266  \n",
       "\n",
       "[10 rows x 91 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Distance0</th>\n",
       "      <th>Distance1</th>\n",
       "      <th>Distance2</th>\n",
       "      <th>Distance3</th>\n",
       "      <th>Distance4</th>\n",
       "      <th>Distance5</th>\n",
       "      <th>Distance6</th>\n",
       "      <th>Distance7</th>\n",
       "      <th>Distance8</th>\n",
       "      <th>...</th>\n",
       "      <th>Distance80</th>\n",
       "      <th>Distance81</th>\n",
       "      <th>Distance82</th>\n",
       "      <th>Distance83</th>\n",
       "      <th>Distance84</th>\n",
       "      <th>Distance85</th>\n",
       "      <th>Distance86</th>\n",
       "      <th>Distance87</th>\n",
       "      <th>Distance88</th>\n",
       "      <th>Distance89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>145.80000</td>\n",
       "      <td>197.800000</td>\n",
       "      <td>162.200000</td>\n",
       "      <td>147.700000</td>\n",
       "      <td>156.700000</td>\n",
       "      <td>154.600000</td>\n",
       "      <td>169.900000</td>\n",
       "      <td>110.100000</td>\n",
       "      <td>111.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>174.20000</td>\n",
       "      <td>167.30000</td>\n",
       "      <td>168.900000</td>\n",
       "      <td>111.800000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>144.300000</td>\n",
       "      <td>196.100000</td>\n",
       "      <td>165.80000</td>\n",
       "      <td>122.800000</td>\n",
       "      <td>165.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.516398</td>\n",
       "      <td>92.62565</td>\n",
       "      <td>92.310828</td>\n",
       "      <td>53.115806</td>\n",
       "      <td>108.709654</td>\n",
       "      <td>101.915052</td>\n",
       "      <td>91.874552</td>\n",
       "      <td>92.927032</td>\n",
       "      <td>66.356696</td>\n",
       "      <td>75.608935</td>\n",
       "      <td>...</td>\n",
       "      <td>118.26693</td>\n",
       "      <td>93.43215</td>\n",
       "      <td>77.585007</td>\n",
       "      <td>89.319402</td>\n",
       "      <td>78.528127</td>\n",
       "      <td>82.483736</td>\n",
       "      <td>66.998259</td>\n",
       "      <td>89.54676</td>\n",
       "      <td>72.680121</td>\n",
       "      <td>76.213151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.00000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>59.75000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>147.750000</td>\n",
       "      <td>54.750000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>116.750000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>62.250000</td>\n",
       "      <td>62.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>52.25000</td>\n",
       "      <td>130.50000</td>\n",
       "      <td>103.500000</td>\n",
       "      <td>49.750000</td>\n",
       "      <td>60.750000</td>\n",
       "      <td>76.500000</td>\n",
       "      <td>138.250000</td>\n",
       "      <td>120.25000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>104.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>121.50000</td>\n",
       "      <td>214.500000</td>\n",
       "      <td>177.500000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>148.500000</td>\n",
       "      <td>186.500000</td>\n",
       "      <td>184.500000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>116.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>236.00000</td>\n",
       "      <td>195.50000</td>\n",
       "      <td>168.500000</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>183.500000</td>\n",
       "      <td>160.00000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>176.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>240.00000</td>\n",
       "      <td>269.500000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>228.500000</td>\n",
       "      <td>248.750000</td>\n",
       "      <td>201.500000</td>\n",
       "      <td>235.500000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>130.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>276.25000</td>\n",
       "      <td>220.00000</td>\n",
       "      <td>233.750000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>201.250000</td>\n",
       "      <td>191.250000</td>\n",
       "      <td>244.750000</td>\n",
       "      <td>228.50000</td>\n",
       "      <td>185.750000</td>\n",
       "      <td>218.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>267.00000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>284.00000</td>\n",
       "      <td>298.00000</td>\n",
       "      <td>272.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.00000</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>266.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Type  Distance0   Distance1   Distance2   Distance3   Distance4  \\\n",
       "count  10.000000   10.00000   10.000000   10.000000   10.000000   10.000000   \n",
       "mean    0.600000  145.80000  197.800000  162.200000  147.700000  156.700000   \n",
       "std     0.516398   92.62565   92.310828   53.115806  108.709654  101.915052   \n",
       "min     0.000000   42.00000   12.000000   43.000000    3.000000    9.000000   \n",
       "25%     0.000000   59.75000  143.500000  147.750000   54.750000   79.500000   \n",
       "50%     1.000000  121.50000  214.500000  177.500000  148.000000  148.500000   \n",
       "75%     1.000000  240.00000  269.500000  199.000000  228.500000  248.750000   \n",
       "max     1.000000  267.00000  296.000000  220.000000  299.000000  292.000000   \n",
       "\n",
       "        Distance5   Distance6   Distance7   Distance8  ...  Distance80  \\\n",
       "count   10.000000   10.000000   10.000000   10.000000  ...    10.00000   \n",
       "mean   154.600000  169.900000  110.100000  111.600000  ...   174.20000   \n",
       "std     91.874552   92.927032   66.356696   75.608935  ...   118.26693   \n",
       "min      1.000000   29.000000   20.000000   26.000000  ...    15.00000   \n",
       "25%    116.750000  112.000000   62.250000   62.250000  ...    52.25000   \n",
       "50%    186.500000  184.500000  100.000000  116.500000  ...   236.00000   \n",
       "75%    201.500000  235.500000  165.000000  130.250000  ...   276.25000   \n",
       "max    292.000000  292.000000  218.000000  292.000000  ...   284.00000   \n",
       "\n",
       "       Distance81  Distance82  Distance83  Distance84  Distance85  Distance86  \\\n",
       "count    10.00000   10.000000   10.000000   10.000000   10.000000   10.000000   \n",
       "mean    167.30000  168.900000  111.800000  137.000000  144.300000  196.100000   \n",
       "std      93.43215   77.585007   89.319402   78.528127   82.483736   66.998259   \n",
       "min       9.00000   44.000000   10.000000   47.000000   19.000000  125.000000   \n",
       "25%     130.50000  103.500000   49.750000   60.750000   76.500000  138.250000   \n",
       "50%     195.50000  168.500000   77.500000  131.000000  161.000000  183.500000   \n",
       "75%     220.00000  233.750000  174.000000  201.250000  191.250000  244.750000   \n",
       "max     298.00000  272.000000  254.000000  243.000000  270.000000  299.000000   \n",
       "\n",
       "       Distance87  Distance88  Distance89  \n",
       "count    10.00000   10.000000   10.000000  \n",
       "mean    165.80000  122.800000  165.000000  \n",
       "std      89.54676   72.680121   76.213151  \n",
       "min      13.00000   12.000000   54.000000  \n",
       "25%     120.25000   77.000000  104.500000  \n",
       "50%     160.00000  106.000000  176.000000  \n",
       "75%     228.50000  185.750000  218.000000  \n",
       "max     299.00000  235.000000  266.000000  \n",
       "\n",
       "[8 rows x 91 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Type', axis = 1).values\n",
    "y = df['Type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[147, 277, 173, 233, 268, 226, 231, 103, 118,  45, 153, 145, 220,\n",
       "         97, 148,  52,  58, 132, 109, 135,  79, 267, 269,  23, 232, 218,\n",
       "        248, 284,  92, 296, 129, 213,  76,  36, 109, 288, 277, 186, 267,\n",
       "        189,  28,  56, 270,  87, 277,  93, 136, 283, 265, 141,  37, 195,\n",
       "        108,  90,  59, 175,  79, 283, 243, 167, 196, 140, 101, 161, 129,\n",
       "        151, 175, 103,  69,  78, 227, 234,  59, 269,  97, 279,  95, 114,\n",
       "        265,  94, 264, 197, 185, 254, 243, 234, 205, 299, 201, 264],\n",
       "       [ 96, 265, 184, 175, 107, 116,  49,  97, 144, 276, 156,   7, 167,\n",
       "        131,  59, 203, 201, 294,  89, 168, 269,  93, 179, 296, 274,  53,\n",
       "          7,  61, 121, 189, 299,  36,  77, 166, 202, 129, 255, 180, 185,\n",
       "        283, 146, 179, 112,  22,  32, 207, 218,  32,  56, 247,  46,  40,\n",
       "        290, 175, 299, 102, 133, 228, 216, 240, 141, 155, 174, 243,  49,\n",
       "        286, 158, 273, 250, 122, 131, 134, 243,  30, 219, 286, 175, 157,\n",
       "         62,  64, 278, 298, 236,  70,  53, 270, 125, 272, 235, 224],\n",
       "       [244,  12, 204, 299,   9, 173, 140,  20, 133, 198,   2,  85,  55,\n",
       "          7, 300, 267,  80,  66, 298, 250,  53,  65, 283, 251,  41, 211,\n",
       "        294, 218,  76, 186, 223, 295, 124, 290,  57, 177, 166, 153, 121,\n",
       "        223, 139,  86, 212,  88, 286, 107, 283, 166, 276, 236,  46, 208,\n",
       "        119,  68, 137, 182, 111, 232,  70,  57, 163,  35,  18, 101, 133,\n",
       "        279, 220, 220, 124, 281, 279, 139, 136, 112, 218,  98, 165, 238,\n",
       "        259,   9, 271, 239,  92, 198, 136, 189, 131, 164, 140,  80],\n",
       "       [ 42, 208, 182, 121, 227,  17, 115,  38,  26, 216,  76,  94,  98,\n",
       "         66,  60,  57, 200, 252, 113, 196, 269, 156, 249, 149, 237, 101,\n",
       "        103,   2, 192, 206, 110,  81,  86, 122,  86, 149, 202, 140, 169,\n",
       "        162,  80, 237, 248, 242, 288, 216,  89,  46, 205, 188,  94, 143,\n",
       "         75, 230,  29,  18, 156, 171, 124,  81, 231, 262, 248,   1, 132,\n",
       "        211, 191,  56, 203, 110, 137, 147, 246,  32,  98, 289,  22,  97,\n",
       "        240, 182, 284, 115, 138, 102, 206,  69, 258, 156,  74, 200],\n",
       "       [ 45, 271, 144,  36, 292, 119, 266,  56,  66, 183, 127, 291,  40,\n",
       "        286,  54,  36,  30,  17, 105,  67,  82, 286, 234, 112,  77, 287,\n",
       "        181, 287, 241, 218, 187, 236, 163, 164,  54, 129, 117, 265, 213,\n",
       "        253, 161, 242, 240, 242,  51, 111, 238, 238, 189,  58, 272,  55,\n",
       "        294, 259, 173,  15, 150, 107, 170, 171, 248,  80, 274,  39, 296,\n",
       "         96,  25, 277, 210,  84, 196, 224, 190, 163, 280,  11, 211, 238,\n",
       "         81,  45, 278,  18,  92,  10,  47, 192, 162, 200, 115, 106],\n",
       "       [ 92, 296, 220, 275,  46, 200,  29, 218,  61, 207,  28,  68, 175,\n",
       "        275, 147, 268, 101,  99,  40, 275, 148, 282, 112, 175, 121,  83,\n",
       "        148,  54, 111, 134, 146, 266,  18, 108, 205, 114, 131, 166, 192,\n",
       "         45, 137, 218,  63, 213, 148,  78,  11,  50, 293, 223, 181, 101,\n",
       "        249, 120,  84, 204, 200, 228,  95, 193,  43, 287, 201, 292,   1,\n",
       "        189, 291, 252, 241, 247, 132, 147,  20,  72,  29, 122, 209, 223,\n",
       "        113, 204,  65, 194, 251, 250, 126, 166, 125,  67,  12, 159],\n",
       "       [267, 212, 205,   9, 256, 292, 292, 179, 115,  51,  34,  15, 154,\n",
       "        177, 257,  57,  35, 272,  89, 216, 181, 134,  25, 250,  70,  12,\n",
       "         82,  49,  75, 149,  47, 192, 165, 218, 241, 120, 296, 176, 137,\n",
       "         44, 172, 141, 291, 202, 162, 239, 152, 213, 287, 256, 112, 173,\n",
       "        247, 109, 244, 108,   9,  19, 264,   2, 244, 168, 145, 220, 226,\n",
       "        111, 263,  79, 178, 116,  46, 169,  78,  43, 166,  92, 130,  72,\n",
       "         17,  39, 208, 199,  44,  43,  84,  99, 299, 116, 210, 104],\n",
       "       [248, 122, 108, 111, 190, 202, 111, 186, 122, 243, 224, 159, 203,\n",
       "         50, 159, 259, 276, 240, 278, 168,  15,  12,  66, 106, 294,  39,\n",
       "        237, 160,  32,  49,  12, 203, 239, 139, 242,  94,  53, 262, 285,\n",
       "        299, 119, 185,  65, 152, 244, 286, 292, 293, 294, 141,  77,  41,\n",
       "         24, 240, 154, 237, 237, 184,  21, 114, 112, 278,  25, 148, 125,\n",
       "         11,  65, 219, 159, 143, 122, 281, 189, 136, 123, 269,  17,  93,\n",
       "        133, 282,  48, 227, 227,  36, 187,  49, 291, 238,  58, 193],\n",
       "       [228, 217, 159,   3,  99,   1, 229, 123, 292, 124, 217,  89, 269,\n",
       "        151,  21, 104,  16,  64, 228, 197, 202, 145, 233,  94, 163, 279,\n",
       "         29, 146, 180, 226, 168,  86,  67,  13, 215, 236, 100,  98, 149,\n",
       "        292,  97, 156,  80, 198, 149, 118, 216,  37,  65, 178,  53, 103,\n",
       "         99, 112, 260,  43, 131, 105, 170, 249,  66, 151, 154,  20, 175,\n",
       "         70,  82, 201,  16,  34, 131, 227, 151, 161, 257, 232, 179, 140,\n",
       "        179, 249,  15, 177, 272,  75,  47, 156, 160, 133,  86,  54],\n",
       "       [ 49,  98,  43, 215,  73, 200, 237,  81,  39,  48, 215,  28, 285,\n",
       "         15, 112, 191, 242, 209,  82, 255, 141, 148, 268, 245, 240, 232,\n",
       "         38, 168, 243, 203,  86, 166, 283, 254, 233, 216,  92,   9,  34,\n",
       "         19, 251, 213,  60, 204, 197,  61,  67,  19, 231, 203, 125,  34,\n",
       "        256, 247,  94, 154, 157, 255, 265, 230, 119, 109, 220, 154, 181,\n",
       "        119, 150,  21, 181, 263, 195,  14, 263,  61, 296, 210, 278, 284,\n",
       "         29,  66,  31,   9, 152,  80, 241,  19, 205,  13,  97, 266]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0, 1, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01333333, 0.87373737, 0.57062147, 0.10150376, 1.        ,\n",
       "        0.37090909, 0.90114068, 0.1       , 0.33898305, 0.59210526,\n",
       "        0.50510204, 1.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.27310924, 0.        ,\n",
       "        0.26377953, 1.        , 0.8600823 , 0.03157895, 0.03125   ,\n",
       "        1.        , 0.75652174, 1.        , 0.99052133, 1.        ,\n",
       "        0.6097561 , 0.86956522, 0.54716981, 0.38356164, 0.        ,\n",
       "        0.28688525, 0.26337449, 1.        , 0.71314741, 0.83571429,\n",
       "        0.47368421, 1.        , 0.77922078, 1.        , 0.07421875,\n",
       "        0.22222222, 0.80782918, 0.79927007, 0.55882353, 0.        ,\n",
       "        1.        , 0.15107914, 1.        , 1.        , 0.53333333,\n",
       "        0.        , 0.61842105, 0.37288136, 0.61065574, 0.71008403,\n",
       "        1.        , 0.        , 1.        , 0.13058419, 1.        ,\n",
       "        0.30909091, 0.        , 1.        , 0.56043956, 0.        ,\n",
       "        1.        , 0.78651685, 0.69958848, 1.        , 0.94007491,\n",
       "        0.        , 0.74329502, 0.78301887, 0.28699552, 0.02469136,\n",
       "        0.97628458, 0.03114187, 0.23188406, 0.        , 0.        ,\n",
       "        0.68924303, 0.21264368, 0.72200772, 0.46188341, 0.01234568],\n",
       "       [0.03111111, 0.        , 0.        , 0.77443609, 0.1097561 ,\n",
       "        0.66545455, 0.79087452, 0.23888889, 0.11016949, 0.        ,\n",
       "        0.95408163, 0.07394366, 1.        , 0.        , 0.28571429,\n",
       "        0.66810345, 0.86178862, 0.69314079, 0.17647059, 0.90384615,\n",
       "        0.49606299, 0.49635036, 1.        , 0.73157895, 0.75892857,\n",
       "        0.8       , 0.13478261, 0.58245614, 1.        , 0.9112426 ,\n",
       "        0.25783972, 0.56521739, 1.        , 1.        , 0.95212766,\n",
       "        1.        , 0.16049383, 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.71287129, 0.        , 0.82727273, 0.64453125,\n",
       "        0.        , 0.19928826, 0.        , 0.73529412, 0.73232323,\n",
       "        0.34955752, 0.        , 0.85925926, 0.92      , 0.24074074,\n",
       "        0.62612613, 0.64912281, 1.        , 1.        , 0.95798319,\n",
       "        0.37073171, 0.14009662, 0.78313253, 0.5257732 , 0.61016949,\n",
       "        0.39272727, 0.46992481, 0.        , 0.24175824, 1.        ,\n",
       "        0.99333333, 0.        , 1.        , 0.23308271, 1.        ,\n",
       "        0.71582734, 1.        , 1.        , 0.05381166, 0.11111111,\n",
       "        0.        , 0.        , 0.52173913, 0.29166667, 1.        ,\n",
       "        0.        , 0.45977011, 0.        , 0.38116592, 1.        ],\n",
       "       [0.        , 0.55555556, 0.78531073, 0.42105263, 0.73577236,\n",
       "        0.        , 0.3269962 , 0.        , 0.        , 0.73684211,\n",
       "        0.24489796, 0.30633803, 0.23673469, 0.18819188, 0.02955665,\n",
       "        0.09051724, 0.69105691, 0.84837545, 0.30672269, 0.62019231,\n",
       "        1.        , 0.52554745, 0.9218107 , 0.22631579, 0.74553571,\n",
       "        0.32363636, 0.4173913 , 0.        , 0.75829384, 0.92899408,\n",
       "        0.34146341, 0.19565217, 0.25660377, 0.09589041, 0.17021277,\n",
       "        0.45081967, 0.61316872, 0.51171875, 0.53784861, 0.51071429,\n",
       "        0.        , 0.95049505, 0.81385281, 1.        , 1.        ,\n",
       "        0.68888889, 0.27758007, 0.09854015, 0.62605042, 0.65656566,\n",
       "        0.21238938, 0.78417266, 0.18888889, 0.80666667, 0.        ,\n",
       "        0.01351351, 0.64473684, 0.6440678 , 0.42213115, 0.33193277,\n",
       "        0.91707317, 0.87922705, 0.89558233, 0.        , 0.4440678 ,\n",
       "        0.72727273, 0.62406015, 0.13671875, 0.48351648, 0.1452514 ,\n",
       "        0.60666667, 0.49812734, 0.93004115, 0.01503759, 0.25842697,\n",
       "        1.        , 0.01915709, 0.11792453, 1.        , 0.58847737,\n",
       "        1.        , 0.36678201, 0.45410628, 0.38333333, 0.81958763,\n",
       "        0.19920319, 0.76436782, 0.55212355, 0.27802691, 0.59259259],\n",
       "       [0.22222222, 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.66545455, 0.        , 1.        , 0.29661017, 0.69736842,\n",
       "        0.        , 0.21478873, 0.55102041, 0.95940959, 0.45812808,\n",
       "        1.        , 0.28861789, 0.29602888, 0.        , 1.        ,\n",
       "        0.52362205, 0.98540146, 0.35802469, 0.36315789, 0.22767857,\n",
       "        0.25818182, 0.61304348, 0.18245614, 0.37440758, 0.50295858,\n",
       "        0.46689895, 1.        , 0.        , 0.        , 0.80319149,\n",
       "        0.16393443, 0.32098765, 0.61328125, 0.62948207, 0.09285714,\n",
       "        0.33333333, 0.76237624, 0.01298701, 0.86818182, 0.453125  ,\n",
       "        0.07555556, 0.        , 0.11313869, 0.99579832, 0.83333333,\n",
       "        0.59734513, 0.48201439, 0.83333333, 0.07333333, 0.2037037 ,\n",
       "        0.85135135, 0.8377193 , 0.88559322, 0.30327869, 0.80252101,\n",
       "        0.        , 1.        , 0.70682731, 1.        , 0.        ,\n",
       "        0.64727273, 1.        , 0.90234375, 0.9010989 , 0.91061453,\n",
       "        0.57333333, 0.49812734, 0.        , 0.31578947, 0.        ,\n",
       "        0.39928058, 0.73563218, 0.71226415, 0.43049327, 0.67901235,\n",
       "        0.13438735, 0.64013841, 1.        , 1.        , 0.40721649,\n",
       "        0.58565737, 0.        , 0.20849421, 0.        , 0.33950617],\n",
       "       [0.91555556, 0.12121212, 0.36723164, 0.38345865, 0.58536585,\n",
       "        0.67272727, 0.31178707, 0.82222222, 0.81355932, 0.85526316,\n",
       "        1.        , 0.53521127, 0.66530612, 0.12915129, 0.51724138,\n",
       "        0.9612069 , 1.        , 0.80505415, 1.        , 0.48557692,\n",
       "        0.        , 0.        , 0.16872428, 0.        , 1.        ,\n",
       "        0.09818182, 1.        , 0.55438596, 0.        , 0.        ,\n",
       "        0.        , 0.72608696, 0.83396226, 0.21232877, 1.        ,\n",
       "        0.        , 0.        , 0.98828125, 1.        , 1.        ,\n",
       "        0.22807018, 0.43564356, 0.02164502, 0.59090909, 0.828125  ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.41919192,\n",
       "        0.13716814, 0.05035971, 0.        , 0.87333333, 0.46296296,\n",
       "        1.        , 1.        , 0.69915254, 0.        , 0.47058824,\n",
       "        0.33658537, 0.95652174, 0.        , 0.50515464, 0.42033898,\n",
       "        0.        , 0.15037594, 0.7734375 , 0.        , 0.32960894,\n",
       "        0.50666667, 1.        , 0.69547325, 0.79699248, 0.35205993,\n",
       "        0.92805755, 0.        , 0.0990566 , 0.52017937, 1.        ,\n",
       "        0.06719368, 0.75432526, 0.88405797, 0.10833333, 0.72164948,\n",
       "        0.11952191, 0.95402299, 0.86872587, 0.20627803, 0.54938272],\n",
       "       [1.        , 0.57575758, 0.91525424, 0.        , 0.85365854,\n",
       "        1.        , 1.        , 0.78333333, 0.75423729, 0.01315789,\n",
       "        0.03061224, 0.02816901, 0.46530612, 0.59778598, 1.        ,\n",
       "        0.09051724, 0.0203252 , 0.92057762, 0.20588235, 0.71634615,\n",
       "        0.65354331, 0.44525547, 0.        , 0.75789474, 0.        ,\n",
       "        0.        , 0.32608696, 0.16491228, 0.20379147, 0.59171598,\n",
       "        0.12195122, 0.67826087, 0.55471698, 0.75342466, 0.99468085,\n",
       "        0.21311475, 1.        , 0.65234375, 0.41035857, 0.08928571,\n",
       "        0.5380117 , 0.        , 1.        , 0.81818182, 0.5078125 ,\n",
       "        0.79111111, 0.50177936, 0.7080292 , 0.97058824, 1.        ,\n",
       "        0.2920354 , 1.        , 0.82592593, 0.        , 0.7962963 ,\n",
       "        0.41891892, 0.        , 0.        , 0.99590164, 0.        ,\n",
       "        0.9804878 , 0.42512077, 0.48192771, 0.75257732, 0.76271186,\n",
       "        0.36363636, 0.89473684, 0.2265625 , 0.20879121, 0.17877095,\n",
       "        0.        , 0.58052434, 0.23868313, 0.09774436, 0.51310861,\n",
       "        0.29136691, 0.43295019, 0.        , 0.        , 0.        ,\n",
       "        0.69960474, 0.65743945, 0.        , 0.1375    , 0.19072165,\n",
       "        0.3187251 , 1.        , 0.3976834 , 0.88789238, 0.        ],\n",
       "       [0.24      , 0.84343434, 0.79661017, 0.62406015, 0.24796748,\n",
       "        0.36      , 0.07604563, 0.32777778, 1.        , 1.        ,\n",
       "        0.65306122, 0.        , 0.51836735, 0.42804428, 0.02463054,\n",
       "        0.71982759, 0.69512195, 1.        , 0.20588235, 0.48557692,\n",
       "        1.        , 0.29562044, 0.63374486, 1.        , 0.91071429,\n",
       "        0.14909091, 0.        , 0.20701754, 0.42180095, 0.82840237,\n",
       "        1.        , 0.        , 0.22264151, 0.39726027, 0.78723404,\n",
       "        0.28688525, 0.83127572, 0.66796875, 0.60159363, 0.94285714,\n",
       "        0.38596491, 0.37623762, 0.22510823, 0.        , 0.        ,\n",
       "        0.64888889, 0.7366548 , 0.04744526, 0.        , 0.95454545,\n",
       "        0.        , 0.04316547, 0.98518519, 0.44      , 1.        ,\n",
       "        0.39189189, 0.54385965, 0.88559322, 0.79918033, 1.        ,\n",
       "        0.47804878, 0.36231884, 0.59839357, 0.83161512, 0.16271186,\n",
       "        1.        , 0.5       , 0.984375  , 1.        , 0.2122905 ,\n",
       "        0.56666667, 0.4494382 , 0.91769547, 0.        , 0.71161049,\n",
       "        0.98920863, 0.60536398, 0.4009434 , 0.20179372, 0.10288066,\n",
       "        0.97628458, 1.        , 0.92753623, 0.25      , 0.03092784,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.74074074]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.82666667,  0.6010101 ,  0.65536723, -0.02255639,  0.21544715,\n",
       "        -0.05818182,  0.76045627,  0.47222222,  2.25423729,  0.33333333,\n",
       "         0.96428571,  0.28873239,  0.93469388,  0.50184502, -0.16256158,\n",
       "         0.29310345, -0.05691057,  0.16967509,  0.78991597,  0.625     ,\n",
       "         0.73622047,  0.48540146,  0.85596708, -0.06315789,  0.41517857,\n",
       "         0.97090909,  0.09565217,  0.50526316,  0.7014218 ,  1.04733728,\n",
       "         0.54355401,  0.2173913 ,  0.18490566, -0.65068493,  0.85638298,\n",
       "         1.16393443,  0.19341564,  0.34765625,  0.45816733,  0.975     ,\n",
       "         0.0994152 ,  0.14851485,  0.08658009,  0.8       ,  0.45703125,\n",
       "         0.25333333,  0.72953737,  0.06569343,  0.03781513,  0.60606061,\n",
       "         0.03097345,  0.49640288,  0.27777778,  0.02      ,  0.85555556,\n",
       "         0.12612613,  0.53508772,  0.36440678,  0.61065574,  1.03781513,\n",
       "         0.11219512,  0.34299517,  0.51807229,  0.0652921 ,  0.58983051,\n",
       "         0.21454545,  0.21428571,  0.703125  , -1.57142857, -0.27932961,\n",
       "         0.56666667,  0.79775281,  0.53909465,  0.98496241,  0.85393258,\n",
       "         0.79496403,  0.62068966,  0.32075472,  0.7264574 ,  0.86419753,\n",
       "        -0.06324111,  0.58131488,  1.10144928,  0.27083333,  0.        ,\n",
       "         0.54581673,  0.20114943,  0.46332046,  0.33183857, -0.30864198],\n",
       "       [ 0.89777778, -0.43434343,  0.90960452,  1.09022556, -0.1504065 ,\n",
       "         0.56727273,  0.42205323, -0.1       ,  0.90677966,  0.65789474,\n",
       "        -0.13265306,  0.27464789,  0.06122449, -0.0295203 ,  1.21182266,\n",
       "         0.99568966,  0.20325203,  0.17689531,  1.08403361,  0.87980769,\n",
       "         0.1496063 ,  0.19343066,  1.0617284 ,  0.76315789, -0.12946429,\n",
       "         0.72363636,  1.24782609,  0.75789474,  0.20853081,  0.81065089,\n",
       "         0.73519164,  1.12608696,  0.4       ,  1.24657534,  0.01595745,\n",
       "         0.68032787,  0.46502058,  0.5625    ,  0.34661355,  0.72857143,\n",
       "         0.34502924, -0.54455446,  0.65800866,  0.3       ,  0.9921875 ,\n",
       "         0.20444444,  0.96797153,  0.53649635,  0.92436975,  0.8989899 ,\n",
       "         0.        ,  1.25179856,  0.35185185, -0.27333333,  0.4       ,\n",
       "         0.75225225,  0.44736842,  0.90254237,  0.20081967,  0.23109244,\n",
       "         0.58536585, -0.2173913 , -0.02811245,  0.34364261,  0.44745763,\n",
       "         0.97454545,  0.73308271,  0.77734375, -0.38461538,  1.10055866,\n",
       "         1.55333333,  0.46816479,  0.47736626,  0.61654135,  0.70786517,\n",
       "         0.31294964,  0.56704981,  0.78301887,  1.08520179, -0.12345679,\n",
       "         0.9486166 ,  0.79584775,  0.23188406,  0.78333333,  0.45876289,\n",
       "         0.67729084,  0.03448276,  0.58301158,  0.57399103, -0.14814815],\n",
       "       [ 0.46666667,  0.9040404 ,  0.73446328,  0.84210526,  0.90243902,\n",
       "         0.76      ,  0.76806084,  0.36111111,  0.77966102, -0.01315789,\n",
       "         0.6377551 ,  0.48591549,  0.73469388,  0.30258303,  0.46305419,\n",
       "         0.06896552,  0.11382114,  0.41516245,  0.28991597,  0.32692308,\n",
       "         0.2519685 ,  0.93065693,  1.00411523, -0.43684211,  0.72321429,\n",
       "         0.74909091,  1.04782609,  0.98947368,  0.28436019,  1.46153846,\n",
       "         0.40766551,  0.76956522,  0.21886792, -0.49315068,  0.29255319,\n",
       "         1.59016393,  0.9218107 ,  0.69140625,  0.92828685,  0.60714286,\n",
       "        -0.30409357, -0.84158416,  0.90909091,  0.29545455,  0.95703125,\n",
       "         0.14222222,  0.44483986,  0.96350365,  0.87815126,  0.41919192,\n",
       "        -0.03982301,  1.15827338,  0.31111111, -0.12666667,  0.11111111,\n",
       "         0.72072072,  0.30701754,  1.11864407,  0.90983607,  0.69327731,\n",
       "         0.74634146,  0.28985507,  0.30522088,  0.54982818,  0.43389831,\n",
       "         0.50909091,  0.56390977,  0.3203125 , -0.98901099, -0.03351955,\n",
       "         1.20666667,  0.82397004,  0.16049383,  1.79699248,  0.25468165,\n",
       "         0.96402878,  0.29885057,  0.19811321,  1.11210762,  0.22633745,\n",
       "         0.92094862,  0.65051903,  0.68115942,  1.01666667,  1.01030928,\n",
       "         0.85657371,  0.45977011,  1.1042471 ,  0.84753363,  0.98765432]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 90)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30, activation = 'relu'))\n",
    "model.add(Dense(15, activation = 'relu'))\n",
    "\n",
    "# BINARY CLASSIFICATION\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6300 - val_loss: 0.9197\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6014 - val_loss: 0.9559\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5738 - val_loss: 0.9895\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5497 - val_loss: 1.0167\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5273 - val_loss: 1.0363\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5063 - val_loss: 1.0485\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4857 - val_loss: 1.0565\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4657 - val_loss: 1.0642\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4483 - val_loss: 1.0712\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4316 - val_loss: 1.0778\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4154 - val_loss: 1.0824\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3992 - val_loss: 1.0855\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3836 - val_loss: 1.0878\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3684 - val_loss: 1.0905\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3554 - val_loss: 1.0910\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3424 - val_loss: 1.0898\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3300 - val_loss: 1.0905\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3180 - val_loss: 1.0915\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3059 - val_loss: 1.0928\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2943 - val_loss: 1.0932\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2828 - val_loss: 1.0929\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2716 - val_loss: 1.0931\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2606 - val_loss: 1.0950\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2496 - val_loss: 1.0984\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2392 - val_loss: 1.1028\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2289 - val_loss: 1.1096\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2191 - val_loss: 1.1150\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2094 - val_loss: 1.1180\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2001 - val_loss: 1.1224\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1912 - val_loss: 1.1322\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1823 - val_loss: 1.1444\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1740 - val_loss: 1.1539\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1664 - val_loss: 1.1544\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1583 - val_loss: 1.1483\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1502 - val_loss: 1.1390\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1428 - val_loss: 1.1291\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1356 - val_loss: 1.1194\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1291 - val_loss: 1.1131\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1228 - val_loss: 1.1102\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1166 - val_loss: 1.1099\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1108 - val_loss: 1.1102\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1050 - val_loss: 1.1105\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0995 - val_loss: 1.1103\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0943 - val_loss: 1.1106\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0891 - val_loss: 1.1112\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0843 - val_loss: 1.1143\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0797 - val_loss: 1.1176\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0754 - val_loss: 1.1210\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0713 - val_loss: 1.1268\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0673 - val_loss: 1.1335\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0637 - val_loss: 1.1406\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0603 - val_loss: 1.1473\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0572 - val_loss: 1.1526\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0542 - val_loss: 1.1565\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0513 - val_loss: 1.1598\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0486 - val_loss: 1.1639\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0460 - val_loss: 1.1667\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0436 - val_loss: 1.1709\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0414 - val_loss: 1.1773\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0393 - val_loss: 1.1838\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0374 - val_loss: 1.1921\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0355 - val_loss: 1.2007\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0337 - val_loss: 1.2098\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0321 - val_loss: 1.2203\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0306 - val_loss: 1.2305\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0291 - val_loss: 1.2397\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0277 - val_loss: 1.2474\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0264 - val_loss: 1.2550\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0252 - val_loss: 1.2623\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0240 - val_loss: 1.2698\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0229 - val_loss: 1.2782\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0219 - val_loss: 1.2861\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0210 - val_loss: 1.2953\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0201 - val_loss: 1.3037\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0192 - val_loss: 1.3108\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0184 - val_loss: 1.3183\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0177 - val_loss: 1.3268\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0169 - val_loss: 1.3355\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0163 - val_loss: 1.3425\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0156 - val_loss: 1.3478\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0150 - val_loss: 1.3526\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0144 - val_loss: 1.3579\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0139 - val_loss: 1.3646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0134 - val_loss: 1.3723\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0129 - val_loss: 1.3794\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0125 - val_loss: 1.3863\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0120 - val_loss: 1.3929\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0116 - val_loss: 1.3994\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0112 - val_loss: 1.4061\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0109 - val_loss: 1.4124\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0105 - val_loss: 1.4173\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0102 - val_loss: 1.4209\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0099 - val_loss: 1.4251\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0096 - val_loss: 1.4309\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0093 - val_loss: 1.4388\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0091 - val_loss: 1.4470\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0088 - val_loss: 1.4547\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0086 - val_loss: 1.4606\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0083 - val_loss: 1.4659\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0081 - val_loss: 1.4704\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0079 - val_loss: 1.4762\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0077 - val_loss: 1.4837\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0075 - val_loss: 1.4909\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0073 - val_loss: 1.4977\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0071 - val_loss: 1.5044\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0070 - val_loss: 1.5122\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0068 - val_loss: 1.5210\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0066 - val_loss: 1.5292\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0065 - val_loss: 1.5363\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0063 - val_loss: 1.5429\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0062 - val_loss: 1.5486\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0060 - val_loss: 1.5542\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0059 - val_loss: 1.5598\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0058 - val_loss: 1.5644\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0057 - val_loss: 1.5681\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0056 - val_loss: 1.5718\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0054 - val_loss: 1.5766\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0053 - val_loss: 1.5821\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0052 - val_loss: 1.5878\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0051 - val_loss: 1.5923\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0050 - val_loss: 1.5961\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0049 - val_loss: 1.6001\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0048 - val_loss: 1.6044\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0047 - val_loss: 1.6088\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0047 - val_loss: 1.6129\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0046 - val_loss: 1.6173\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0045 - val_loss: 1.6217\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 1.6255\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0043 - val_loss: 1.6291\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0043 - val_loss: 1.6329\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 1.6365\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0041 - val_loss: 1.6400\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0040 - val_loss: 1.6429\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0040 - val_loss: 1.6462\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0039 - val_loss: 1.6495\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0039 - val_loss: 1.6528\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0038 - val_loss: 1.6552\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0037 - val_loss: 1.6574\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0037 - val_loss: 1.6595\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0036 - val_loss: 1.6626\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0036 - val_loss: 1.6662\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0035 - val_loss: 1.6707\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0035 - val_loss: 1.6743\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0034 - val_loss: 1.6779\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0033 - val_loss: 1.6825\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0033 - val_loss: 1.6865\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0033 - val_loss: 1.6892\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0032 - val_loss: 1.6924\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0032 - val_loss: 1.6958\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0031 - val_loss: 1.6987\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0031 - val_loss: 1.7015\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0030 - val_loss: 1.7039\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0030 - val_loss: 1.7066\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0029 - val_loss: 1.7098\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0029 - val_loss: 1.7133\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0029 - val_loss: 1.7164\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0028 - val_loss: 1.7192\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0028 - val_loss: 1.7224\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0027 - val_loss: 1.7263\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 1.7306\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 1.7338\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 1.7356\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0026 - val_loss: 1.7385\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0026 - val_loss: 1.7429\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0025 - val_loss: 1.7464\n",
      "Epoch 166/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0025 - val_loss: 1.7481\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0025 - val_loss: 1.7506\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0024 - val_loss: 1.7537\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0024 - val_loss: 1.7574\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0024 - val_loss: 1.7609\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0024 - val_loss: 1.7641\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0023 - val_loss: 1.7672\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 1.7694\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 1.7714\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 1.7746\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 1.7785\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 1.7817\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0022 - val_loss: 1.7842\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0021 - val_loss: 1.7867\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 1.7900\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 1.7936\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 1.7971\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 1.7990\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0020 - val_loss: 1.8014\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0020 - val_loss: 1.8044\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 1.8079\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 1.8112\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 1.8139\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 1.8160\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 1.8185\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 1.8215\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 1.8242\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0018 - val_loss: 1.8266\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0018 - val_loss: 1.8288\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 1.8316\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 1.8343\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 1.8371\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 1.8401\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 1.8424\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 1.8455\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 1.8485\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 1.8507\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 1.8530\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 1.8558\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 1.8583\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 1.8611\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 1.8638\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 1.8665\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0016 - val_loss: 1.8687\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 1.8712\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 1.8739\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 1.8763\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 1.8785\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 1.8801\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0015 - val_loss: 1.8825\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 1.8861\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 1.8891\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 1.8914\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 1.8931\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 1.8954\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 1.8982\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 1.9006\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0014 - val_loss: 1.9030\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 1.9051\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 1.9074\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 1.9090\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 1.9111\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 1.9141\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 1.9164\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 1.9179\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 1.9197\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 1.9223\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 1.9253\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 1.9279\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 1.9291\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 1.9306\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 1.9332\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 1.9363\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 1.9382\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 1.9399\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 1.9421\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 1.9450\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 1.9472\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 1.9489\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 1.9505\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 1.9526\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 1.9550\n",
      "Epoch 248/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 1.9574\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 1.9594\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 1.9611\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 1.9631\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 1.9652\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 1.9671\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 1.9698\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 1.9721\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 1.9739\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 1.9754\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 1.9770\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 1.9792\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 1.9821\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.9465e-04 - val_loss: 1.9842\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.8703e-04 - val_loss: 1.9857\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.7950e-04 - val_loss: 1.9876\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.7212e-04 - val_loss: 1.9899\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.6478e-04 - val_loss: 1.9918\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.5756e-04 - val_loss: 1.9934\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.5034e-04 - val_loss: 1.9955\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.4326e-04 - val_loss: 1.9976\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.3624e-04 - val_loss: 1.9994\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.2927e-04 - val_loss: 2.0015\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.2245e-04 - val_loss: 2.0034\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.1566e-04 - val_loss: 2.0051\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.0895e-04 - val_loss: 2.0068\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.0231e-04 - val_loss: 2.0083\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.9573e-04 - val_loss: 2.0099\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.8926e-04 - val_loss: 2.0121\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.8289e-04 - val_loss: 2.0139\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.7657e-04 - val_loss: 2.0152\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.7030e-04 - val_loss: 2.0175\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 8.6402e-04 - val_loss: 2.0198\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.5794e-04 - val_loss: 2.0216\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.5188e-04 - val_loss: 2.0229\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.4585e-04 - val_loss: 2.0246\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.3988e-04 - val_loss: 2.0268\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.3403e-04 - val_loss: 2.0285\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.2819e-04 - val_loss: 2.0301\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.2246e-04 - val_loss: 2.0319\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.1674e-04 - val_loss: 2.0339\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.1108e-04 - val_loss: 2.0357\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.0549e-04 - val_loss: 2.0372\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 8.0000e-04 - val_loss: 2.0393\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.9458e-04 - val_loss: 2.0407\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.8918e-04 - val_loss: 2.0425\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.8380e-04 - val_loss: 2.0446\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.7852e-04 - val_loss: 2.0460\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.7325e-04 - val_loss: 2.0474\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.6807e-04 - val_loss: 2.0493\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.6294e-04 - val_loss: 2.0515\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 7.5786e-04 - val_loss: 2.0532\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.5284e-04 - val_loss: 2.0547\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.4786e-04 - val_loss: 2.0563\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.4289e-04 - val_loss: 2.0585\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.3800e-04 - val_loss: 2.0606\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.3324e-04 - val_loss: 2.0621\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.2840e-04 - val_loss: 2.0632\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.2366e-04 - val_loss: 2.0649\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.1898e-04 - val_loss: 2.0670\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.1435e-04 - val_loss: 2.0684\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.0971e-04 - val_loss: 2.0696\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.0519e-04 - val_loss: 2.0714\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.0066e-04 - val_loss: 2.0732\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.9619e-04 - val_loss: 2.0750\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.9176e-04 - val_loss: 2.0765\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.8737e-04 - val_loss: 2.0782\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.8300e-04 - val_loss: 2.0798\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.7873e-04 - val_loss: 2.0813\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.7446e-04 - val_loss: 2.0830\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.7025e-04 - val_loss: 2.0850\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.6611e-04 - val_loss: 2.0866\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.6194e-04 - val_loss: 2.0880\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.5783e-04 - val_loss: 2.0893\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.5379e-04 - val_loss: 2.0912\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.4976e-04 - val_loss: 2.0930\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.4579e-04 - val_loss: 2.0946\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.4183e-04 - val_loss: 2.0961\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.3790e-04 - val_loss: 2.0978\n",
      "Epoch 327/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step - loss: 6.3404e-04 - val_loss: 2.0991\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.3020e-04 - val_loss: 2.1005\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.2641e-04 - val_loss: 2.1019\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.2264e-04 - val_loss: 2.1038\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.1887e-04 - val_loss: 2.1059\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.1518e-04 - val_loss: 2.1076\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 6.1153e-04 - val_loss: 2.1089\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.0786e-04 - val_loss: 2.1099\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.0424e-04 - val_loss: 2.1110\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.0071e-04 - val_loss: 2.1126\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.9717e-04 - val_loss: 2.1149\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.9367e-04 - val_loss: 2.1167\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.9019e-04 - val_loss: 2.1181\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.8676e-04 - val_loss: 2.1196\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.8335e-04 - val_loss: 2.1213\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.7995e-04 - val_loss: 2.1227\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.7661e-04 - val_loss: 2.1242\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.7328e-04 - val_loss: 2.1256\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.6996e-04 - val_loss: 2.1273\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.6670e-04 - val_loss: 2.1290\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.6345e-04 - val_loss: 2.1304\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.6024e-04 - val_loss: 2.1318\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.5706e-04 - val_loss: 2.1336\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.5391e-04 - val_loss: 2.1354\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.5078e-04 - val_loss: 2.1366\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.4765e-04 - val_loss: 2.1376\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.4459e-04 - val_loss: 2.1392\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.4157e-04 - val_loss: 2.1409\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.3855e-04 - val_loss: 2.1422\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.3550e-04 - val_loss: 2.1440\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.3255e-04 - val_loss: 2.1455\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.2963e-04 - val_loss: 2.1469\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.2670e-04 - val_loss: 2.1482\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.2378e-04 - val_loss: 2.1495\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.2090e-04 - val_loss: 2.1511\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.1806e-04 - val_loss: 2.1524\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.1524e-04 - val_loss: 2.1538\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.1243e-04 - val_loss: 2.1556\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.0963e-04 - val_loss: 2.1574\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.0689e-04 - val_loss: 2.1589\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.0417e-04 - val_loss: 2.1602\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.0146e-04 - val_loss: 2.1615\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.9874e-04 - val_loss: 2.1631\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.9605e-04 - val_loss: 2.1645\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.9341e-04 - val_loss: 2.1658\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.9078e-04 - val_loss: 2.1672\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.8816e-04 - val_loss: 2.1690\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.8559e-04 - val_loss: 2.1702\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.8303e-04 - val_loss: 2.1714\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.8049e-04 - val_loss: 2.1726\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.7795e-04 - val_loss: 2.1741\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.7545e-04 - val_loss: 2.1753\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.7296e-04 - val_loss: 2.1765\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.7051e-04 - val_loss: 2.1781\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.6807e-04 - val_loss: 2.1797\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.6562e-04 - val_loss: 2.1811\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.6321e-04 - val_loss: 2.1825\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.6081e-04 - val_loss: 2.1838\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.5846e-04 - val_loss: 2.1852\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.5612e-04 - val_loss: 2.1863\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.5378e-04 - val_loss: 2.1879\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.5146e-04 - val_loss: 2.1895\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.4916e-04 - val_loss: 2.1907\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.4687e-04 - val_loss: 2.1921\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.4461e-04 - val_loss: 2.1933\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.4236e-04 - val_loss: 2.1946\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.4014e-04 - val_loss: 2.1959\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.3793e-04 - val_loss: 2.1972\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.3572e-04 - val_loss: 2.1987\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.3354e-04 - val_loss: 2.2001\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.3138e-04 - val_loss: 2.2014\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.2924e-04 - val_loss: 2.2028\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.2711e-04 - val_loss: 2.2038\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.2499e-04 - val_loss: 2.2051\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.2289e-04 - val_loss: 2.2066\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.2080e-04 - val_loss: 2.2079\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.1873e-04 - val_loss: 2.2093\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.1669e-04 - val_loss: 2.2104\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.1466e-04 - val_loss: 2.2117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.1262e-04 - val_loss: 2.2129\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.1061e-04 - val_loss: 2.2140\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.0862e-04 - val_loss: 2.2155\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.0663e-04 - val_loss: 2.2173\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.0468e-04 - val_loss: 2.2185\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.0272e-04 - val_loss: 2.2194\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.0079e-04 - val_loss: 2.2207\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.9887e-04 - val_loss: 2.2222\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.9695e-04 - val_loss: 2.2234\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.9508e-04 - val_loss: 2.2246\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.9320e-04 - val_loss: 2.2261\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.9131e-04 - val_loss: 2.2274\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.8947e-04 - val_loss: 2.2286\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.8763e-04 - val_loss: 2.2298\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.8579e-04 - val_loss: 2.2309\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.8398e-04 - val_loss: 2.2322\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.8218e-04 - val_loss: 2.2333\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.8040e-04 - val_loss: 2.2347\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.7861e-04 - val_loss: 2.2361\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.7684e-04 - val_loss: 2.2372\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.7510e-04 - val_loss: 2.2384\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.7336e-04 - val_loss: 2.2396\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.7164e-04 - val_loss: 2.2408\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.6991e-04 - val_loss: 2.2422\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.6821e-04 - val_loss: 2.2436\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.6652e-04 - val_loss: 2.2447\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.6484e-04 - val_loss: 2.2456\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.6318e-04 - val_loss: 2.2470\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.6150e-04 - val_loss: 2.2485\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.5987e-04 - val_loss: 2.2496\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.5823e-04 - val_loss: 2.2505\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.5661e-04 - val_loss: 2.2518\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.5500e-04 - val_loss: 2.2531\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.5339e-04 - val_loss: 2.2545\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.5180e-04 - val_loss: 2.2555\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.5022e-04 - val_loss: 2.2567\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.4865e-04 - val_loss: 2.2581\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.4710e-04 - val_loss: 2.2593\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.4554e-04 - val_loss: 2.2604\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.4400e-04 - val_loss: 2.2613\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.4248e-04 - val_loss: 2.2624\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.4097e-04 - val_loss: 2.2634\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.3947e-04 - val_loss: 2.2646\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.3796e-04 - val_loss: 2.2662\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.3646e-04 - val_loss: 2.2679\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.3500e-04 - val_loss: 2.2689\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.3352e-04 - val_loss: 2.2695\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.3206e-04 - val_loss: 2.2707\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.3062e-04 - val_loss: 2.2721\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.2918e-04 - val_loss: 2.2734\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.2775e-04 - val_loss: 2.2744\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.2632e-04 - val_loss: 2.2752\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.2491e-04 - val_loss: 2.2765\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.2350e-04 - val_loss: 2.2777\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.2211e-04 - val_loss: 2.2786\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.2073e-04 - val_loss: 2.2799\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.1935e-04 - val_loss: 2.2812\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.1798e-04 - val_loss: 2.2824\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.1663e-04 - val_loss: 2.2833\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.1530e-04 - val_loss: 2.2846\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.1395e-04 - val_loss: 2.2860\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.1261e-04 - val_loss: 2.2869\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.1129e-04 - val_loss: 2.2879\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.0998e-04 - val_loss: 2.2892\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0866e-04 - val_loss: 2.2906\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0738e-04 - val_loss: 2.2916\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0608e-04 - val_loss: 2.2926\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0480e-04 - val_loss: 2.2936\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.0352e-04 - val_loss: 2.2947\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0224e-04 - val_loss: 2.2957\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0099e-04 - val_loss: 2.2970\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9974e-04 - val_loss: 2.2983\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9850e-04 - val_loss: 2.2994\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9726e-04 - val_loss: 2.3001\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9604e-04 - val_loss: 2.3014\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9482e-04 - val_loss: 2.3025\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9360e-04 - val_loss: 2.3034\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9240e-04 - val_loss: 2.3046\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9120e-04 - val_loss: 2.3059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9002e-04 - val_loss: 2.3070\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.8883e-04 - val_loss: 2.3080\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.8765e-04 - val_loss: 2.3087\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.8649e-04 - val_loss: 2.3098\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.8534e-04 - val_loss: 2.3110\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.8417e-04 - val_loss: 2.3123\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.8303e-04 - val_loss: 2.3132\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.8190e-04 - val_loss: 2.3139\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.8077e-04 - val_loss: 2.3150\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.7963e-04 - val_loss: 2.3164\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.7851e-04 - val_loss: 2.3178\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.7739e-04 - val_loss: 2.3188\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.7629e-04 - val_loss: 2.3197\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.7519e-04 - val_loss: 2.3209\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.7409e-04 - val_loss: 2.3221\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.7300e-04 - val_loss: 2.3233\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.7192e-04 - val_loss: 2.3241\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.7085e-04 - val_loss: 2.3250\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6978e-04 - val_loss: 2.3260\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.6872e-04 - val_loss: 2.3271\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.6766e-04 - val_loss: 2.3284\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6661e-04 - val_loss: 2.3294\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6557e-04 - val_loss: 2.3303\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.6453e-04 - val_loss: 2.3311\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6350e-04 - val_loss: 2.3322\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6247e-04 - val_loss: 2.3335\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.6145e-04 - val_loss: 2.3344\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6045e-04 - val_loss: 2.3353\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.5944e-04 - val_loss: 2.3366\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.5843e-04 - val_loss: 2.3375\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.5744e-04 - val_loss: 2.3386\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.5645e-04 - val_loss: 2.3399\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.5548e-04 - val_loss: 2.3408\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.5449e-04 - val_loss: 2.3416\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.5352e-04 - val_loss: 2.3424\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.5255e-04 - val_loss: 2.3432\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.5159e-04 - val_loss: 2.3443\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.5064e-04 - val_loss: 2.3456\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4968e-04 - val_loss: 2.3469\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4873e-04 - val_loss: 2.3479\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4779e-04 - val_loss: 2.3489\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4686e-04 - val_loss: 2.3499\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.4593e-04 - val_loss: 2.3507\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4500e-04 - val_loss: 2.3515\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.4408e-04 - val_loss: 2.3524\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4317e-04 - val_loss: 2.3535\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4225e-04 - val_loss: 2.3546\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.4135e-04 - val_loss: 2.3557\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4045e-04 - val_loss: 2.3565\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.3956e-04 - val_loss: 2.3574\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.3868e-04 - val_loss: 2.3587\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3779e-04 - val_loss: 2.3596\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3691e-04 - val_loss: 2.3606\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3603e-04 - val_loss: 2.3614\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3516e-04 - val_loss: 2.3624\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3430e-04 - val_loss: 2.3636\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3344e-04 - val_loss: 2.3644\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.3258e-04 - val_loss: 2.3653\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.3173e-04 - val_loss: 2.3664\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.3089e-04 - val_loss: 2.3676\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3005e-04 - val_loss: 2.3686\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.2921e-04 - val_loss: 2.3694\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.2837e-04 - val_loss: 2.3701\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.2756e-04 - val_loss: 2.3710\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.2673e-04 - val_loss: 2.3723\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.2591e-04 - val_loss: 2.3734\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.2509e-04 - val_loss: 2.3742\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.2428e-04 - val_loss: 2.3753\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.2347e-04 - val_loss: 2.3765\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.2267e-04 - val_loss: 2.3772\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.2187e-04 - val_loss: 2.3781\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.2108e-04 - val_loss: 2.3790\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.2029e-04 - val_loss: 2.3799\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.1951e-04 - val_loss: 2.3808\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.1873e-04 - val_loss: 2.3818\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.1795e-04 - val_loss: 2.3828\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.1718e-04 - val_loss: 2.3836\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.1641e-04 - val_loss: 2.3847\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.1565e-04 - val_loss: 2.3857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.1489e-04 - val_loss: 2.3866\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.1413e-04 - val_loss: 2.3875\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.1338e-04 - val_loss: 2.3884\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.1263e-04 - val_loss: 2.3895\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.1189e-04 - val_loss: 2.3902\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.1114e-04 - val_loss: 2.3911\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.1041e-04 - val_loss: 2.3922\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.0968e-04 - val_loss: 2.3932\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.0895e-04 - val_loss: 2.3942\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.0822e-04 - val_loss: 2.3951\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.0750e-04 - val_loss: 2.3961\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.0678e-04 - val_loss: 2.3968\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.0607e-04 - val_loss: 2.3975\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.0536e-04 - val_loss: 2.3986\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.0465e-04 - val_loss: 2.3994\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.0395e-04 - val_loss: 2.4003\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.0325e-04 - val_loss: 2.4014\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.0255e-04 - val_loss: 2.4024\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.0186e-04 - val_loss: 2.4033\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.0117e-04 - val_loss: 2.4043\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.0049e-04 - val_loss: 2.4052\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.9980e-04 - val_loss: 2.4059\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.9912e-04 - val_loss: 2.4069\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.9845e-04 - val_loss: 2.4078\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.9778e-04 - val_loss: 2.4085\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.9711e-04 - val_loss: 2.4095\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.9645e-04 - val_loss: 2.4105\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.9578e-04 - val_loss: 2.4115\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.9513e-04 - val_loss: 2.4124\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.9448e-04 - val_loss: 2.4131\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.9382e-04 - val_loss: 2.4139\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.9317e-04 - val_loss: 2.4151\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.9253e-04 - val_loss: 2.4162\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.9189e-04 - val_loss: 2.4168\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.9124e-04 - val_loss: 2.4173\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.9061e-04 - val_loss: 2.4181\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8998e-04 - val_loss: 2.4193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x3945706e48>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = X_train, y = y_train, epochs = 600, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x394579ad08>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('test_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating the exact same model, including its weights and the optimizer\n",
    "model = tf.keras.models.load_model('test_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpo_twafja\\assets\n",
      "Size of h5 model: 68.12109375 KB\n",
      "Size of tflite model: 14.1640625 KB\n",
      "Decreased for factor: 4.809431880860452\n"
     ]
    }
   ],
   "source": [
    "# Converting the model without quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Saving the model to the disk\n",
    "open(\"test_quant.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "# size of the .h5 model\n",
    "h5_in_kb = os.path.getsize(\"test_model.h5\") / 1024\n",
    "print(\"Size of h5 model: {} KB\".format(h5_in_kb))\n",
    "\n",
    "# size of the .tflite model\n",
    "tflite_in_kb = os.path.getsize(\"test_quant.tflite\") / 1024\n",
    "print(\"Size of tflite model: {} KB\".format(tflite_in_kb))\n",
    "print(\"Decreased for factor: {}\".format(h5_in_kb/tflite_in_kb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1488: set_learning_phase (from tensorflow.python.keras.backend) is deprecated and will be removed after 2020-10-11.\n",
      "Instructions for updating:\n",
      "Simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1488: set_learning_phase (from tensorflow.python.keras.backend) is deprecated and will be removed after 2020-10-11.\n",
      "Instructions for updating:\n",
      "Simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpeqpk627c\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpeqpk627c\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\admin\\AppData\\Local\\Temp\\tmpeqpk627c\\variables\\variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\admin\\AppData\\Local\\Temp\\tmpeqpk627c\\variables\\variables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'__saved_model_init_op', 'serving_default'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'__saved_model_init_op', 'serving_default'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input tensors info: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input tensors info: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: serving_default_dense_input:0, shape: (-1, 90), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: serving_default_dense_input:0, shape: (-1, 90), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:output tensors info: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:output tensors info: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: StatefulPartitionedCall:0, shape: (-1, 1), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: StatefulPartitionedCall:0, shape: (-1, 1), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\admin\\AppData\\Local\\Temp\\tmpeqpk627c\\variables\\variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\admin\\AppData\\Local\\Temp\\tmpeqpk627c\\variables\\variables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\util.py:275: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\util.py:275: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\convert_to_constants.py:854: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\convert_to_constants.py:854: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5824"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the model to the tensorFlow lite format with full integer quantization\n",
    "scans = tf.cast(X_test, tf.float32)\n",
    "scans_data = tf.data.Dataset.from_tensor_slices(scans).batch(1)\n",
    "def representative_dataset_gen():\n",
    "  for input in scans_data.take(4):\n",
    "    # Get sample input data as a numpy array in a method of your choosing.\n",
    "    yield [input]\n",
    "    \n",
    "converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(\"test_model.h5\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.float32\n",
    "tflite_model = converter.convert()\n",
    "# Save to disk\n",
    "open(\"test_8bit.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
