{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\\\Users\\admin\\Desktop\\Door-Detection\\Door-Detection\\test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Distance0</th>\n",
       "      <th>Distance1</th>\n",
       "      <th>Distance2</th>\n",
       "      <th>Distance3</th>\n",
       "      <th>Distance4</th>\n",
       "      <th>Distance5</th>\n",
       "      <th>Distance6</th>\n",
       "      <th>Distance7</th>\n",
       "      <th>Distance8</th>\n",
       "      <th>...</th>\n",
       "      <th>Distance80</th>\n",
       "      <th>Distance81</th>\n",
       "      <th>Distance82</th>\n",
       "      <th>Distance83</th>\n",
       "      <th>Distance84</th>\n",
       "      <th>Distance85</th>\n",
       "      <th>Distance86</th>\n",
       "      <th>Distance87</th>\n",
       "      <th>Distance88</th>\n",
       "      <th>Distance89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>147</td>\n",
       "      <td>277</td>\n",
       "      <td>173</td>\n",
       "      <td>233</td>\n",
       "      <td>268</td>\n",
       "      <td>226</td>\n",
       "      <td>231</td>\n",
       "      <td>103</td>\n",
       "      <td>118</td>\n",
       "      <td>...</td>\n",
       "      <td>264</td>\n",
       "      <td>197</td>\n",
       "      <td>185</td>\n",
       "      <td>254</td>\n",
       "      <td>243</td>\n",
       "      <td>234</td>\n",
       "      <td>205</td>\n",
       "      <td>299</td>\n",
       "      <td>201</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>265</td>\n",
       "      <td>184</td>\n",
       "      <td>175</td>\n",
       "      <td>107</td>\n",
       "      <td>116</td>\n",
       "      <td>49</td>\n",
       "      <td>97</td>\n",
       "      <td>144</td>\n",
       "      <td>...</td>\n",
       "      <td>278</td>\n",
       "      <td>298</td>\n",
       "      <td>236</td>\n",
       "      <td>70</td>\n",
       "      <td>53</td>\n",
       "      <td>270</td>\n",
       "      <td>125</td>\n",
       "      <td>272</td>\n",
       "      <td>235</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>244</td>\n",
       "      <td>12</td>\n",
       "      <td>204</td>\n",
       "      <td>299</td>\n",
       "      <td>9</td>\n",
       "      <td>173</td>\n",
       "      <td>140</td>\n",
       "      <td>20</td>\n",
       "      <td>133</td>\n",
       "      <td>...</td>\n",
       "      <td>271</td>\n",
       "      <td>239</td>\n",
       "      <td>92</td>\n",
       "      <td>198</td>\n",
       "      <td>136</td>\n",
       "      <td>189</td>\n",
       "      <td>131</td>\n",
       "      <td>164</td>\n",
       "      <td>140</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>208</td>\n",
       "      <td>182</td>\n",
       "      <td>121</td>\n",
       "      <td>227</td>\n",
       "      <td>17</td>\n",
       "      <td>115</td>\n",
       "      <td>38</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>284</td>\n",
       "      <td>115</td>\n",
       "      <td>138</td>\n",
       "      <td>102</td>\n",
       "      <td>206</td>\n",
       "      <td>69</td>\n",
       "      <td>258</td>\n",
       "      <td>156</td>\n",
       "      <td>74</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>271</td>\n",
       "      <td>144</td>\n",
       "      <td>36</td>\n",
       "      <td>292</td>\n",
       "      <td>119</td>\n",
       "      <td>266</td>\n",
       "      <td>56</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>278</td>\n",
       "      <td>18</td>\n",
       "      <td>92</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>192</td>\n",
       "      <td>162</td>\n",
       "      <td>200</td>\n",
       "      <td>115</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>296</td>\n",
       "      <td>220</td>\n",
       "      <td>275</td>\n",
       "      <td>46</td>\n",
       "      <td>200</td>\n",
       "      <td>29</td>\n",
       "      <td>218</td>\n",
       "      <td>61</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>194</td>\n",
       "      <td>251</td>\n",
       "      <td>250</td>\n",
       "      <td>126</td>\n",
       "      <td>166</td>\n",
       "      <td>125</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>267</td>\n",
       "      <td>212</td>\n",
       "      <td>205</td>\n",
       "      <td>9</td>\n",
       "      <td>256</td>\n",
       "      <td>292</td>\n",
       "      <td>292</td>\n",
       "      <td>179</td>\n",
       "      <td>115</td>\n",
       "      <td>...</td>\n",
       "      <td>208</td>\n",
       "      <td>199</td>\n",
       "      <td>44</td>\n",
       "      <td>43</td>\n",
       "      <td>84</td>\n",
       "      <td>99</td>\n",
       "      <td>299</td>\n",
       "      <td>116</td>\n",
       "      <td>210</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>248</td>\n",
       "      <td>122</td>\n",
       "      <td>108</td>\n",
       "      <td>111</td>\n",
       "      <td>190</td>\n",
       "      <td>202</td>\n",
       "      <td>111</td>\n",
       "      <td>186</td>\n",
       "      <td>122</td>\n",
       "      <td>...</td>\n",
       "      <td>48</td>\n",
       "      <td>227</td>\n",
       "      <td>227</td>\n",
       "      <td>36</td>\n",
       "      <td>187</td>\n",
       "      <td>49</td>\n",
       "      <td>291</td>\n",
       "      <td>238</td>\n",
       "      <td>58</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>228</td>\n",
       "      <td>217</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>229</td>\n",
       "      <td>123</td>\n",
       "      <td>292</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>177</td>\n",
       "      <td>272</td>\n",
       "      <td>75</td>\n",
       "      <td>47</td>\n",
       "      <td>156</td>\n",
       "      <td>160</td>\n",
       "      <td>133</td>\n",
       "      <td>86</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>215</td>\n",
       "      <td>73</td>\n",
       "      <td>200</td>\n",
       "      <td>237</td>\n",
       "      <td>81</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "      <td>152</td>\n",
       "      <td>80</td>\n",
       "      <td>241</td>\n",
       "      <td>19</td>\n",
       "      <td>205</td>\n",
       "      <td>13</td>\n",
       "      <td>97</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Distance0  Distance1  Distance2  Distance3  Distance4  Distance5  \\\n",
       "0     1        147        277        173        233        268        226   \n",
       "1     1         96        265        184        175        107        116   \n",
       "2     1        244         12        204        299          9        173   \n",
       "3     0         42        208        182        121        227         17   \n",
       "4     0         45        271        144         36        292        119   \n",
       "5     0         92        296        220        275         46        200   \n",
       "6     1        267        212        205          9        256        292   \n",
       "7     0        248        122        108        111        190        202   \n",
       "8     1        228        217        159          3         99          1   \n",
       "9     1         49         98         43        215         73        200   \n",
       "\n",
       "   Distance6  Distance7  Distance8  ...  Distance80  Distance81  Distance82  \\\n",
       "0        231        103        118  ...         264         197         185   \n",
       "1         49         97        144  ...         278         298         236   \n",
       "2        140         20        133  ...         271         239          92   \n",
       "3        115         38         26  ...         284         115         138   \n",
       "4        266         56         66  ...         278          18          92   \n",
       "5         29        218         61  ...          65         194         251   \n",
       "6        292        179        115  ...         208         199          44   \n",
       "7        111        186        122  ...          48         227         227   \n",
       "8        229        123        292  ...          15         177         272   \n",
       "9        237         81         39  ...          31           9         152   \n",
       "\n",
       "   Distance83  Distance84  Distance85  Distance86  Distance87  Distance88  \\\n",
       "0         254         243         234         205         299         201   \n",
       "1          70          53         270         125         272         235   \n",
       "2         198         136         189         131         164         140   \n",
       "3         102         206          69         258         156          74   \n",
       "4          10          47         192         162         200         115   \n",
       "5         250         126         166         125          67          12   \n",
       "6          43          84          99         299         116         210   \n",
       "7          36         187          49         291         238          58   \n",
       "8          75          47         156         160         133          86   \n",
       "9          80         241          19         205          13          97   \n",
       "\n",
       "   Distance89  \n",
       "0         264  \n",
       "1         224  \n",
       "2          80  \n",
       "3         200  \n",
       "4         106  \n",
       "5         159  \n",
       "6         104  \n",
       "7         193  \n",
       "8          54  \n",
       "9         266  \n",
       "\n",
       "[10 rows x 91 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Distance0</th>\n",
       "      <th>Distance1</th>\n",
       "      <th>Distance2</th>\n",
       "      <th>Distance3</th>\n",
       "      <th>Distance4</th>\n",
       "      <th>Distance5</th>\n",
       "      <th>Distance6</th>\n",
       "      <th>Distance7</th>\n",
       "      <th>Distance8</th>\n",
       "      <th>...</th>\n",
       "      <th>Distance80</th>\n",
       "      <th>Distance81</th>\n",
       "      <th>Distance82</th>\n",
       "      <th>Distance83</th>\n",
       "      <th>Distance84</th>\n",
       "      <th>Distance85</th>\n",
       "      <th>Distance86</th>\n",
       "      <th>Distance87</th>\n",
       "      <th>Distance88</th>\n",
       "      <th>Distance89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>145.80000</td>\n",
       "      <td>197.800000</td>\n",
       "      <td>162.200000</td>\n",
       "      <td>147.700000</td>\n",
       "      <td>156.700000</td>\n",
       "      <td>154.600000</td>\n",
       "      <td>169.900000</td>\n",
       "      <td>110.100000</td>\n",
       "      <td>111.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>174.20000</td>\n",
       "      <td>167.30000</td>\n",
       "      <td>168.900000</td>\n",
       "      <td>111.800000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>144.300000</td>\n",
       "      <td>196.100000</td>\n",
       "      <td>165.80000</td>\n",
       "      <td>122.800000</td>\n",
       "      <td>165.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.516398</td>\n",
       "      <td>92.62565</td>\n",
       "      <td>92.310828</td>\n",
       "      <td>53.115806</td>\n",
       "      <td>108.709654</td>\n",
       "      <td>101.915052</td>\n",
       "      <td>91.874552</td>\n",
       "      <td>92.927032</td>\n",
       "      <td>66.356696</td>\n",
       "      <td>75.608935</td>\n",
       "      <td>...</td>\n",
       "      <td>118.26693</td>\n",
       "      <td>93.43215</td>\n",
       "      <td>77.585007</td>\n",
       "      <td>89.319402</td>\n",
       "      <td>78.528127</td>\n",
       "      <td>82.483736</td>\n",
       "      <td>66.998259</td>\n",
       "      <td>89.54676</td>\n",
       "      <td>72.680121</td>\n",
       "      <td>76.213151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.00000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>59.75000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>147.750000</td>\n",
       "      <td>54.750000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>116.750000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>62.250000</td>\n",
       "      <td>62.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>52.25000</td>\n",
       "      <td>130.50000</td>\n",
       "      <td>103.500000</td>\n",
       "      <td>49.750000</td>\n",
       "      <td>60.750000</td>\n",
       "      <td>76.500000</td>\n",
       "      <td>138.250000</td>\n",
       "      <td>120.25000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>104.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>121.50000</td>\n",
       "      <td>214.500000</td>\n",
       "      <td>177.500000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>148.500000</td>\n",
       "      <td>186.500000</td>\n",
       "      <td>184.500000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>116.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>236.00000</td>\n",
       "      <td>195.50000</td>\n",
       "      <td>168.500000</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>183.500000</td>\n",
       "      <td>160.00000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>176.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>240.00000</td>\n",
       "      <td>269.500000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>228.500000</td>\n",
       "      <td>248.750000</td>\n",
       "      <td>201.500000</td>\n",
       "      <td>235.500000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>130.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>276.25000</td>\n",
       "      <td>220.00000</td>\n",
       "      <td>233.750000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>201.250000</td>\n",
       "      <td>191.250000</td>\n",
       "      <td>244.750000</td>\n",
       "      <td>228.50000</td>\n",
       "      <td>185.750000</td>\n",
       "      <td>218.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>267.00000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>284.00000</td>\n",
       "      <td>298.00000</td>\n",
       "      <td>272.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>299.00000</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>266.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Type  Distance0   Distance1   Distance2   Distance3   Distance4  \\\n",
       "count  10.000000   10.00000   10.000000   10.000000   10.000000   10.000000   \n",
       "mean    0.600000  145.80000  197.800000  162.200000  147.700000  156.700000   \n",
       "std     0.516398   92.62565   92.310828   53.115806  108.709654  101.915052   \n",
       "min     0.000000   42.00000   12.000000   43.000000    3.000000    9.000000   \n",
       "25%     0.000000   59.75000  143.500000  147.750000   54.750000   79.500000   \n",
       "50%     1.000000  121.50000  214.500000  177.500000  148.000000  148.500000   \n",
       "75%     1.000000  240.00000  269.500000  199.000000  228.500000  248.750000   \n",
       "max     1.000000  267.00000  296.000000  220.000000  299.000000  292.000000   \n",
       "\n",
       "        Distance5   Distance6   Distance7   Distance8  ...  Distance80  \\\n",
       "count   10.000000   10.000000   10.000000   10.000000  ...    10.00000   \n",
       "mean   154.600000  169.900000  110.100000  111.600000  ...   174.20000   \n",
       "std     91.874552   92.927032   66.356696   75.608935  ...   118.26693   \n",
       "min      1.000000   29.000000   20.000000   26.000000  ...    15.00000   \n",
       "25%    116.750000  112.000000   62.250000   62.250000  ...    52.25000   \n",
       "50%    186.500000  184.500000  100.000000  116.500000  ...   236.00000   \n",
       "75%    201.500000  235.500000  165.000000  130.250000  ...   276.25000   \n",
       "max    292.000000  292.000000  218.000000  292.000000  ...   284.00000   \n",
       "\n",
       "       Distance81  Distance82  Distance83  Distance84  Distance85  Distance86  \\\n",
       "count    10.00000   10.000000   10.000000   10.000000   10.000000   10.000000   \n",
       "mean    167.30000  168.900000  111.800000  137.000000  144.300000  196.100000   \n",
       "std      93.43215   77.585007   89.319402   78.528127   82.483736   66.998259   \n",
       "min       9.00000   44.000000   10.000000   47.000000   19.000000  125.000000   \n",
       "25%     130.50000  103.500000   49.750000   60.750000   76.500000  138.250000   \n",
       "50%     195.50000  168.500000   77.500000  131.000000  161.000000  183.500000   \n",
       "75%     220.00000  233.750000  174.000000  201.250000  191.250000  244.750000   \n",
       "max     298.00000  272.000000  254.000000  243.000000  270.000000  299.000000   \n",
       "\n",
       "       Distance87  Distance88  Distance89  \n",
       "count    10.00000   10.000000   10.000000  \n",
       "mean    165.80000  122.800000  165.000000  \n",
       "std      89.54676   72.680121   76.213151  \n",
       "min      13.00000   12.000000   54.000000  \n",
       "25%     120.25000   77.000000  104.500000  \n",
       "50%     160.00000  106.000000  176.000000  \n",
       "75%     228.50000  185.750000  218.000000  \n",
       "max     299.00000  235.000000  266.000000  \n",
       "\n",
       "[8 rows x 91 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Type', axis = 1).values\n",
    "y = df['Type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[147, 277, 173, 233, 268, 226, 231, 103, 118,  45, 153, 145, 220,\n",
       "         97, 148,  52,  58, 132, 109, 135,  79, 267, 269,  23, 232, 218,\n",
       "        248, 284,  92, 296, 129, 213,  76,  36, 109, 288, 277, 186, 267,\n",
       "        189,  28,  56, 270,  87, 277,  93, 136, 283, 265, 141,  37, 195,\n",
       "        108,  90,  59, 175,  79, 283, 243, 167, 196, 140, 101, 161, 129,\n",
       "        151, 175, 103,  69,  78, 227, 234,  59, 269,  97, 279,  95, 114,\n",
       "        265,  94, 264, 197, 185, 254, 243, 234, 205, 299, 201, 264],\n",
       "       [ 96, 265, 184, 175, 107, 116,  49,  97, 144, 276, 156,   7, 167,\n",
       "        131,  59, 203, 201, 294,  89, 168, 269,  93, 179, 296, 274,  53,\n",
       "          7,  61, 121, 189, 299,  36,  77, 166, 202, 129, 255, 180, 185,\n",
       "        283, 146, 179, 112,  22,  32, 207, 218,  32,  56, 247,  46,  40,\n",
       "        290, 175, 299, 102, 133, 228, 216, 240, 141, 155, 174, 243,  49,\n",
       "        286, 158, 273, 250, 122, 131, 134, 243,  30, 219, 286, 175, 157,\n",
       "         62,  64, 278, 298, 236,  70,  53, 270, 125, 272, 235, 224],\n",
       "       [244,  12, 204, 299,   9, 173, 140,  20, 133, 198,   2,  85,  55,\n",
       "          7, 300, 267,  80,  66, 298, 250,  53,  65, 283, 251,  41, 211,\n",
       "        294, 218,  76, 186, 223, 295, 124, 290,  57, 177, 166, 153, 121,\n",
       "        223, 139,  86, 212,  88, 286, 107, 283, 166, 276, 236,  46, 208,\n",
       "        119,  68, 137, 182, 111, 232,  70,  57, 163,  35,  18, 101, 133,\n",
       "        279, 220, 220, 124, 281, 279, 139, 136, 112, 218,  98, 165, 238,\n",
       "        259,   9, 271, 239,  92, 198, 136, 189, 131, 164, 140,  80],\n",
       "       [ 42, 208, 182, 121, 227,  17, 115,  38,  26, 216,  76,  94,  98,\n",
       "         66,  60,  57, 200, 252, 113, 196, 269, 156, 249, 149, 237, 101,\n",
       "        103,   2, 192, 206, 110,  81,  86, 122,  86, 149, 202, 140, 169,\n",
       "        162,  80, 237, 248, 242, 288, 216,  89,  46, 205, 188,  94, 143,\n",
       "         75, 230,  29,  18, 156, 171, 124,  81, 231, 262, 248,   1, 132,\n",
       "        211, 191,  56, 203, 110, 137, 147, 246,  32,  98, 289,  22,  97,\n",
       "        240, 182, 284, 115, 138, 102, 206,  69, 258, 156,  74, 200],\n",
       "       [ 45, 271, 144,  36, 292, 119, 266,  56,  66, 183, 127, 291,  40,\n",
       "        286,  54,  36,  30,  17, 105,  67,  82, 286, 234, 112,  77, 287,\n",
       "        181, 287, 241, 218, 187, 236, 163, 164,  54, 129, 117, 265, 213,\n",
       "        253, 161, 242, 240, 242,  51, 111, 238, 238, 189,  58, 272,  55,\n",
       "        294, 259, 173,  15, 150, 107, 170, 171, 248,  80, 274,  39, 296,\n",
       "         96,  25, 277, 210,  84, 196, 224, 190, 163, 280,  11, 211, 238,\n",
       "         81,  45, 278,  18,  92,  10,  47, 192, 162, 200, 115, 106],\n",
       "       [ 92, 296, 220, 275,  46, 200,  29, 218,  61, 207,  28,  68, 175,\n",
       "        275, 147, 268, 101,  99,  40, 275, 148, 282, 112, 175, 121,  83,\n",
       "        148,  54, 111, 134, 146, 266,  18, 108, 205, 114, 131, 166, 192,\n",
       "         45, 137, 218,  63, 213, 148,  78,  11,  50, 293, 223, 181, 101,\n",
       "        249, 120,  84, 204, 200, 228,  95, 193,  43, 287, 201, 292,   1,\n",
       "        189, 291, 252, 241, 247, 132, 147,  20,  72,  29, 122, 209, 223,\n",
       "        113, 204,  65, 194, 251, 250, 126, 166, 125,  67,  12, 159],\n",
       "       [267, 212, 205,   9, 256, 292, 292, 179, 115,  51,  34,  15, 154,\n",
       "        177, 257,  57,  35, 272,  89, 216, 181, 134,  25, 250,  70,  12,\n",
       "         82,  49,  75, 149,  47, 192, 165, 218, 241, 120, 296, 176, 137,\n",
       "         44, 172, 141, 291, 202, 162, 239, 152, 213, 287, 256, 112, 173,\n",
       "        247, 109, 244, 108,   9,  19, 264,   2, 244, 168, 145, 220, 226,\n",
       "        111, 263,  79, 178, 116,  46, 169,  78,  43, 166,  92, 130,  72,\n",
       "         17,  39, 208, 199,  44,  43,  84,  99, 299, 116, 210, 104],\n",
       "       [248, 122, 108, 111, 190, 202, 111, 186, 122, 243, 224, 159, 203,\n",
       "         50, 159, 259, 276, 240, 278, 168,  15,  12,  66, 106, 294,  39,\n",
       "        237, 160,  32,  49,  12, 203, 239, 139, 242,  94,  53, 262, 285,\n",
       "        299, 119, 185,  65, 152, 244, 286, 292, 293, 294, 141,  77,  41,\n",
       "         24, 240, 154, 237, 237, 184,  21, 114, 112, 278,  25, 148, 125,\n",
       "         11,  65, 219, 159, 143, 122, 281, 189, 136, 123, 269,  17,  93,\n",
       "        133, 282,  48, 227, 227,  36, 187,  49, 291, 238,  58, 193],\n",
       "       [228, 217, 159,   3,  99,   1, 229, 123, 292, 124, 217,  89, 269,\n",
       "        151,  21, 104,  16,  64, 228, 197, 202, 145, 233,  94, 163, 279,\n",
       "         29, 146, 180, 226, 168,  86,  67,  13, 215, 236, 100,  98, 149,\n",
       "        292,  97, 156,  80, 198, 149, 118, 216,  37,  65, 178,  53, 103,\n",
       "         99, 112, 260,  43, 131, 105, 170, 249,  66, 151, 154,  20, 175,\n",
       "         70,  82, 201,  16,  34, 131, 227, 151, 161, 257, 232, 179, 140,\n",
       "        179, 249,  15, 177, 272,  75,  47, 156, 160, 133,  86,  54],\n",
       "       [ 49,  98,  43, 215,  73, 200, 237,  81,  39,  48, 215,  28, 285,\n",
       "         15, 112, 191, 242, 209,  82, 255, 141, 148, 268, 245, 240, 232,\n",
       "         38, 168, 243, 203,  86, 166, 283, 254, 233, 216,  92,   9,  34,\n",
       "         19, 251, 213,  60, 204, 197,  61,  67,  19, 231, 203, 125,  34,\n",
       "        256, 247,  94, 154, 157, 255, 265, 230, 119, 109, 220, 154, 181,\n",
       "        119, 150,  21, 181, 263, 195,  14, 263,  61, 296, 210, 278, 284,\n",
       "         29,  66,  31,   9, 152,  80, 241,  19, 205,  13,  97, 266]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0, 1, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 90)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30, activation = 'relu'))\n",
    "model.add(Dense(15, activation = 'relu'))\n",
    "\n",
    "# BINARY CLASSIFICATION\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.6942 - val_loss: 0.7881\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6783 - val_loss: 0.7917\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6667 - val_loss: 0.7919\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6561 - val_loss: 0.7926\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6456 - val_loss: 0.7950\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6350 - val_loss: 0.7973\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6241 - val_loss: 0.7987\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6138 - val_loss: 0.7988\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6037 - val_loss: 0.7992\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5938 - val_loss: 0.8033\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5832 - val_loss: 0.8102\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5725 - val_loss: 0.8200\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5622 - val_loss: 0.8305\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5517 - val_loss: 0.8395\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5408 - val_loss: 0.8497\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5304 - val_loss: 0.8605\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5198 - val_loss: 0.8726\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5094 - val_loss: 0.8865\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.4992 - val_loss: 0.9045\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.4893 - val_loss: 0.9209\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4790 - val_loss: 0.9383\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4668 - val_loss: 0.9583\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4545 - val_loss: 0.9840\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4414 - val_loss: 1.0082\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.4281 - val_loss: 1.0245\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4150 - val_loss: 1.0373\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.4012 - val_loss: 1.0459\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3876 - val_loss: 1.0554\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3734 - val_loss: 1.0601\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3588 - val_loss: 1.0692\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3431 - val_loss: 1.0740\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3269 - val_loss: 1.0773\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3122 - val_loss: 1.0841\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2983 - val_loss: 1.0942\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2843 - val_loss: 1.1071\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2710 - val_loss: 1.1155\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2583 - val_loss: 1.1257\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2462 - val_loss: 1.1351\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2350 - val_loss: 1.1450\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2242 - val_loss: 1.1570\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2137 - val_loss: 1.1689\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2034 - val_loss: 1.1806\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1933 - val_loss: 1.1946\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1829 - val_loss: 1.2113\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1732 - val_loss: 1.2288\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1642 - val_loss: 1.2462\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1559 - val_loss: 1.2621\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1479 - val_loss: 1.2754\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1404 - val_loss: 1.2866\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1332 - val_loss: 1.2904\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1263 - val_loss: 1.2909\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1197 - val_loss: 1.2888\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1136 - val_loss: 1.2918\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1079 - val_loss: 1.2991\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1025 - val_loss: 1.3114\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0974 - val_loss: 1.3274\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0925 - val_loss: 1.3458\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0879 - val_loss: 1.3659\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0837 - val_loss: 1.3840\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0797 - val_loss: 1.3993\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0760 - val_loss: 1.4109\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0725 - val_loss: 1.4196\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0692 - val_loss: 1.4267\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0661 - val_loss: 1.4325\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0631 - val_loss: 1.4372\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0603 - val_loss: 1.4428\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0576 - val_loss: 1.4496\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0551 - val_loss: 1.4583\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0527 - val_loss: 1.4675\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0506 - val_loss: 1.4785\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0485 - val_loss: 1.4879\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0466 - val_loss: 1.4956\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0448 - val_loss: 1.5016\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0431 - val_loss: 1.5075\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0414 - val_loss: 1.5136\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0398 - val_loss: 1.5223\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0384 - val_loss: 1.5309\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0370 - val_loss: 1.5407\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0356 - val_loss: 1.5508\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0344 - val_loss: 1.5607\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0332 - val_loss: 1.5701\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0320 - val_loss: 1.5779\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0310 - val_loss: 1.5830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0299 - val_loss: 1.5881\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0289 - val_loss: 1.5937\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0280 - val_loss: 1.5999\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0271 - val_loss: 1.6060\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0262 - val_loss: 1.6114\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0254 - val_loss: 1.6160\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0246 - val_loss: 1.6203\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0238 - val_loss: 1.6243\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0231 - val_loss: 1.6290\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0224 - val_loss: 1.6337\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0218 - val_loss: 1.6407\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0212 - val_loss: 1.6489\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0206 - val_loss: 1.6575\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0200 - val_loss: 1.6660\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0194 - val_loss: 1.6745\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0189 - val_loss: 1.6816\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0184 - val_loss: 1.6888\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0179 - val_loss: 1.6956\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0175 - val_loss: 1.7028\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0170 - val_loss: 1.7100\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0166 - val_loss: 1.7157\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0162 - val_loss: 1.7208\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0158 - val_loss: 1.7262\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0154 - val_loss: 1.7323\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0151 - val_loss: 1.7398\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0147 - val_loss: 1.7469\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0144 - val_loss: 1.7532\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0141 - val_loss: 1.7575\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0137 - val_loss: 1.7613\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0134 - val_loss: 1.7662\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0132 - val_loss: 1.7710\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0129 - val_loss: 1.7750\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0126 - val_loss: 1.7779\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0123 - val_loss: 1.7809\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0121 - val_loss: 1.7855\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0118 - val_loss: 1.7908\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0116 - val_loss: 1.7954\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0114 - val_loss: 1.7977\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0112 - val_loss: 1.8003\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0109 - val_loss: 1.8040\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0107 - val_loss: 1.8082\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0105 - val_loss: 1.8118\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0103 - val_loss: 1.8157\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0101 - val_loss: 1.8188\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0100 - val_loss: 1.8217\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0098 - val_loss: 1.8261\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0096 - val_loss: 1.8317\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0094 - val_loss: 1.8368\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0093 - val_loss: 1.8404\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0091 - val_loss: 1.8425\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0090 - val_loss: 1.8459\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0088 - val_loss: 1.8501\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0087 - val_loss: 1.8540\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0085 - val_loss: 1.8582\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0084 - val_loss: 1.8617\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0082 - val_loss: 1.8643\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0081 - val_loss: 1.8684\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0080 - val_loss: 1.8725\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0079 - val_loss: 1.8770\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0077 - val_loss: 1.8807\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0076 - val_loss: 1.8835\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0075 - val_loss: 1.8874\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0074 - val_loss: 1.8923\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0073 - val_loss: 1.8971\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0071 - val_loss: 1.9006\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0070 - val_loss: 1.9035\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0069 - val_loss: 1.9076\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0068 - val_loss: 1.9133\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0067 - val_loss: 1.9171\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0066 - val_loss: 1.9196\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0065 - val_loss: 1.9213\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0064 - val_loss: 1.9240\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0063 - val_loss: 1.9280\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0063 - val_loss: 1.9328\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0062 - val_loss: 1.9363\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0061 - val_loss: 1.9396\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0060 - val_loss: 1.9427\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0059 - val_loss: 1.9458\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0058 - val_loss: 1.9485\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0058 - val_loss: 1.9519\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0057 - val_loss: 1.9544\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0056 - val_loss: 1.9567\n",
      "Epoch 166/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0055 - val_loss: 1.9595\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0055 - val_loss: 1.9633\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0054 - val_loss: 1.9677\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0053 - val_loss: 1.9711\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0052 - val_loss: 1.9730\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0052 - val_loss: 1.9747\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0051 - val_loss: 1.9774\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0050 - val_loss: 1.9812\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0050 - val_loss: 1.9841\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0049 - val_loss: 1.9863\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0049 - val_loss: 1.9879\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0048 - val_loss: 1.9898\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0047 - val_loss: 1.9927\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0047 - val_loss: 1.9963\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0046 - val_loss: 1.9995\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0046 - val_loss: 2.0020\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0045 - val_loss: 2.0042\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0045 - val_loss: 2.0059\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0044 - val_loss: 2.0084\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0044 - val_loss: 2.0119\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0043 - val_loss: 2.0150\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0043 - val_loss: 2.0178\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0042 - val_loss: 2.0194\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0042 - val_loss: 2.0208\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0041 - val_loss: 2.0234\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0041 - val_loss: 2.0263\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0040 - val_loss: 2.0292\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0040 - val_loss: 2.0319\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0039 - val_loss: 2.0339\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0039 - val_loss: 2.0354\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0039 - val_loss: 2.0373\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0038 - val_loss: 2.0389\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 2.0413\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0037 - val_loss: 2.0441\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0037 - val_loss: 2.0471\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0036 - val_loss: 2.0498\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0036 - val_loss: 2.0517\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0036 - val_loss: 2.0527\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0035 - val_loss: 2.0540\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0035 - val_loss: 2.0566\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0035 - val_loss: 2.0598\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0034 - val_loss: 2.0631\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0034 - val_loss: 2.0650\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0034 - val_loss: 2.0662\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0033 - val_loss: 2.0674\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0033 - val_loss: 2.0693\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0033 - val_loss: 2.0723\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0032 - val_loss: 2.0747\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0032 - val_loss: 2.0761\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0032 - val_loss: 2.0779\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 2.0802\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 2.0825\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0031 - val_loss: 2.0844\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0030 - val_loss: 2.0863\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0030 - val_loss: 2.0875\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0030 - val_loss: 2.0894\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0030 - val_loss: 2.0923\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0029 - val_loss: 2.0939\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0029 - val_loss: 2.0958\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0029 - val_loss: 2.0980\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0028 - val_loss: 2.0996\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0028 - val_loss: 2.1009\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0028 - val_loss: 2.1027\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0028 - val_loss: 2.1044\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 2.1067\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 2.1088\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0027 - val_loss: 2.1101\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 2.1115\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0026 - val_loss: 2.1137\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 2.1161\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 2.1178\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0026 - val_loss: 2.1197\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0025 - val_loss: 2.1211\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0025 - val_loss: 2.1225\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0025 - val_loss: 2.1247\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0025 - val_loss: 2.1271\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0025 - val_loss: 2.1288\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0024 - val_loss: 2.1300\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 2.1320\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0024 - val_loss: 2.1346\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0024 - val_loss: 2.1363\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 2.1375\n",
      "Epoch 248/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 2.1395\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0023 - val_loss: 2.1419\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0023 - val_loss: 2.1437\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0023 - val_loss: 2.1459\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 2.1479\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0022 - val_loss: 2.1497\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 2.1513\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 2.1530\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 2.1547\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0022 - val_loss: 2.1564\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 2.1582\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 2.1603\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 2.1621\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 2.1641\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 2.1659\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 2.1673\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 2.1694\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 2.1715\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0020 - val_loss: 2.1729\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 2.1743\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 2.1757\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 2.1774\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 2.1795\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0019 - val_loss: 2.1813\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 2.1826\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 2.1840\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 2.1859\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 2.1880\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0019 - val_loss: 2.1899\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 2.1910\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 2.1926\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0018 - val_loss: 2.1945\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 2.1962\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 2.1982\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 2.1997\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 2.2007\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 2.2020\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 2.2042\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 2.2060\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0017 - val_loss: 2.2073\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 2.2084\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 2.2099\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 2.2116\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 2.2134\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 2.2151\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 2.2164\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 2.2181\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 2.2192\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 2.2204\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 2.2218\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 2.2241\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 2.2263\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 2.2273\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0015 - val_loss: 2.2284\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 2.2298\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 2.2310\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 2.2327\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 2.2349\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 2.2367\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 2.2381\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 2.2391\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 2.2405\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 2.2423\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 2.2443\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 2.2456\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 2.2466\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 2.2480\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 2.2499\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 2.2514\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 2.2525\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 2.2539\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 2.2556\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 2.2574\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0013 - val_loss: 2.2586\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0013 - val_loss: 2.2599\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 2.2615\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 2.2631\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 2.2641\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 2.2653\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 2.2671\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 2.2685\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 2.2694\n",
      "Epoch 330/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 2.2706\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 2.2721\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 2.2734\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.2749\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.2763\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.2776\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.2786\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.2801\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 2.2817\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.2831\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.2843\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.2855\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 2.2865\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.2876\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 2.2890\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 2.2899\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 2.2913\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 2.2932\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0011 - val_loss: 2.2940\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 2.2946\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 2.2955\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 2.2967\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0011 - val_loss: 2.2982\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 2.2999\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 2.3011\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0011 - val_loss: 2.3020\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 2.3028\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 2.3041\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 2.3055\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 2.3069\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 2.3078\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 2.3087\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 2.3096\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 2.3106\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 2.3121\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 2.3132\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 2.3144\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 2.3158\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 2.3168\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.9719e-04 - val_loss: 2.3175\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.9146e-04 - val_loss: 2.3186\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.8567e-04 - val_loss: 2.3203\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.8001e-04 - val_loss: 2.3213\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.7439e-04 - val_loss: 2.3223\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.6881e-04 - val_loss: 2.3235\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.6334e-04 - val_loss: 2.3244\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.5787e-04 - val_loss: 2.3252\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.5244e-04 - val_loss: 2.3260\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.4705e-04 - val_loss: 2.3273\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.4169e-04 - val_loss: 2.3284\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.3639e-04 - val_loss: 2.3295\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.3118e-04 - val_loss: 2.3307\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.2598e-04 - val_loss: 2.3316\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.2079e-04 - val_loss: 2.3329\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.1570e-04 - val_loss: 2.3342\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.1063e-04 - val_loss: 2.3353\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.0562e-04 - val_loss: 2.3362\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.0063e-04 - val_loss: 2.3371\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.9569e-04 - val_loss: 2.3385\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 8.9081e-04 - val_loss: 2.3394\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 8.8589e-04 - val_loss: 2.3404\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.8112e-04 - val_loss: 2.3413\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.7635e-04 - val_loss: 2.3421\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.7157e-04 - val_loss: 2.3431\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.6687e-04 - val_loss: 2.3446\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.6223e-04 - val_loss: 2.3458\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.5761e-04 - val_loss: 2.3464\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.5303e-04 - val_loss: 2.3472\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.4848e-04 - val_loss: 2.3484\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.4395e-04 - val_loss: 2.3496\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.3949e-04 - val_loss: 2.3508\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.3507e-04 - val_loss: 2.3518\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.3066e-04 - val_loss: 2.3526\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.2627e-04 - val_loss: 2.3532\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 8.2196e-04 - val_loss: 2.3543\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.1766e-04 - val_loss: 2.3554\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.1337e-04 - val_loss: 2.3565\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.0914e-04 - val_loss: 2.3575\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.0494e-04 - val_loss: 2.3584\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.0081e-04 - val_loss: 2.3595\n",
      "Epoch 410/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step - loss: 7.9667e-04 - val_loss: 2.3609\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.9253e-04 - val_loss: 2.3623\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.8851e-04 - val_loss: 2.3633\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.8446e-04 - val_loss: 2.3639\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.8045e-04 - val_loss: 2.3650\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.7646e-04 - val_loss: 2.3664\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 7.7255e-04 - val_loss: 2.3675\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.6865e-04 - val_loss: 2.3684\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.6473e-04 - val_loss: 2.3690\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.6089e-04 - val_loss: 2.3698\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.5710e-04 - val_loss: 2.3713\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.5328e-04 - val_loss: 2.3728\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.4954e-04 - val_loss: 2.3739\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.4580e-04 - val_loss: 2.3745\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.4210e-04 - val_loss: 2.3756\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.3841e-04 - val_loss: 2.3769\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.3475e-04 - val_loss: 2.3783\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.3111e-04 - val_loss: 2.3792\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 7.2749e-04 - val_loss: 2.3798\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.2396e-04 - val_loss: 2.3809\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.2039e-04 - val_loss: 2.3824\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 7.1690e-04 - val_loss: 2.3830\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.1339e-04 - val_loss: 2.3837\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.0992e-04 - val_loss: 2.3849\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.0647e-04 - val_loss: 2.3860\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.0307e-04 - val_loss: 2.3868\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.9967e-04 - val_loss: 2.3876\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.9631e-04 - val_loss: 2.3886\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 6.9296e-04 - val_loss: 2.3897\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 6.8963e-04 - val_loss: 2.3908\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.8634e-04 - val_loss: 2.3919\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.8307e-04 - val_loss: 2.3930\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.7980e-04 - val_loss: 2.3938\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.7659e-04 - val_loss: 2.3944\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.7342e-04 - val_loss: 2.3957\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.7021e-04 - val_loss: 2.3970\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.6707e-04 - val_loss: 2.3977\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.6394e-04 - val_loss: 2.3985\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.6081e-04 - val_loss: 2.3995\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.5774e-04 - val_loss: 2.4002\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.5467e-04 - val_loss: 2.4012\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.5162e-04 - val_loss: 2.4023\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.4858e-04 - val_loss: 2.4033\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 6.4557e-04 - val_loss: 2.4040\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.4261e-04 - val_loss: 2.4046\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.3965e-04 - val_loss: 2.4057\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.3669e-04 - val_loss: 2.4071\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.3379e-04 - val_loss: 2.4079\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.3088e-04 - val_loss: 2.4086\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.2801e-04 - val_loss: 2.4093\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.2514e-04 - val_loss: 2.4105\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.2229e-04 - val_loss: 2.4116\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.1948e-04 - val_loss: 2.4127\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.1667e-04 - val_loss: 2.4134\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.1388e-04 - val_loss: 2.4144\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.1111e-04 - val_loss: 2.4151\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 6.0837e-04 - val_loss: 2.4159\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.0565e-04 - val_loss: 2.4169\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.0294e-04 - val_loss: 2.4178\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.0023e-04 - val_loss: 2.4187\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.9757e-04 - val_loss: 2.4196\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.9491e-04 - val_loss: 2.4204\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.9226e-04 - val_loss: 2.4213\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.8965e-04 - val_loss: 2.4222\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.8703e-04 - val_loss: 2.4233\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.8445e-04 - val_loss: 2.4240\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.8190e-04 - val_loss: 2.4249\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.7934e-04 - val_loss: 2.4259\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.7680e-04 - val_loss: 2.4267\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.7429e-04 - val_loss: 2.4273\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.7179e-04 - val_loss: 2.4283\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.6928e-04 - val_loss: 2.4293\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.6683e-04 - val_loss: 2.4299\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.6438e-04 - val_loss: 2.4306\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.6195e-04 - val_loss: 2.4317\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.5953e-04 - val_loss: 2.4328\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.5712e-04 - val_loss: 2.4335\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.5472e-04 - val_loss: 2.4342\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.5236e-04 - val_loss: 2.4348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.5000e-04 - val_loss: 2.4359\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.4766e-04 - val_loss: 2.4369\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.4534e-04 - val_loss: 2.4376\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.4302e-04 - val_loss: 2.4383\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.4074e-04 - val_loss: 2.4394\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.3845e-04 - val_loss: 2.4405\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.3617e-04 - val_loss: 2.4413\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.3392e-04 - val_loss: 2.4419\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.3169e-04 - val_loss: 2.4427\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.2944e-04 - val_loss: 2.4435\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.2724e-04 - val_loss: 2.4442\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.2503e-04 - val_loss: 2.4448\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.2284e-04 - val_loss: 2.4455\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.2066e-04 - val_loss: 2.4464\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.1849e-04 - val_loss: 2.4473\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.1635e-04 - val_loss: 2.4482\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.1422e-04 - val_loss: 2.4492\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.1211e-04 - val_loss: 2.4502\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.1000e-04 - val_loss: 2.4508\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.0788e-04 - val_loss: 2.4512\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.0583e-04 - val_loss: 2.4521\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.0376e-04 - val_loss: 2.4533\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.0171e-04 - val_loss: 2.4542\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.9965e-04 - val_loss: 2.4551\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.9763e-04 - val_loss: 2.4559\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.9561e-04 - val_loss: 2.4568\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.9361e-04 - val_loss: 2.4575\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.9161e-04 - val_loss: 2.4578\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.8962e-04 - val_loss: 2.4584\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.8768e-04 - val_loss: 2.4592\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.8572e-04 - val_loss: 2.4600\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.8374e-04 - val_loss: 2.4610\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.8180e-04 - val_loss: 2.4617\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.7987e-04 - val_loss: 2.4623\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.7795e-04 - val_loss: 2.4632\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.7604e-04 - val_loss: 2.4641\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.7414e-04 - val_loss: 2.4650\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.7225e-04 - val_loss: 2.4658\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.7037e-04 - val_loss: 2.4666\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.6851e-04 - val_loss: 2.4674\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.6664e-04 - val_loss: 2.4681\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.6480e-04 - val_loss: 2.4691\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.6297e-04 - val_loss: 2.4698\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.6114e-04 - val_loss: 2.4705\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.5932e-04 - val_loss: 2.4711\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.5751e-04 - val_loss: 2.4718\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.5573e-04 - val_loss: 2.4728\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.5395e-04 - val_loss: 2.4738\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.5217e-04 - val_loss: 2.4745\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.5040e-04 - val_loss: 2.4751\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.4866e-04 - val_loss: 2.4758\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.4690e-04 - val_loss: 2.4767\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.4519e-04 - val_loss: 2.4776\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.4346e-04 - val_loss: 2.4785\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.4175e-04 - val_loss: 2.4791\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.4005e-04 - val_loss: 2.4799\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.3835e-04 - val_loss: 2.4806\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.3666e-04 - val_loss: 2.4815\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.3500e-04 - val_loss: 2.4822\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.3333e-04 - val_loss: 2.4828\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.3168e-04 - val_loss: 2.4836\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.3004e-04 - val_loss: 2.4843\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.2840e-04 - val_loss: 2.4851\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.2677e-04 - val_loss: 2.4857\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.2516e-04 - val_loss: 2.4867\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.2355e-04 - val_loss: 2.4878\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.2196e-04 - val_loss: 2.4886\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.2037e-04 - val_loss: 2.4890\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.1878e-04 - val_loss: 2.4896\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.1720e-04 - val_loss: 2.4902\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.1563e-04 - val_loss: 2.4909\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.1408e-04 - val_loss: 2.4916\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.1254e-04 - val_loss: 2.4924\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.1100e-04 - val_loss: 2.4932\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.0947e-04 - val_loss: 2.4939\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.0796e-04 - val_loss: 2.4945\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.0644e-04 - val_loss: 2.4952\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.0493e-04 - val_loss: 2.4961\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.0344e-04 - val_loss: 2.4967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.0196e-04 - val_loss: 2.4976\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.0048e-04 - val_loss: 2.4986\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.9901e-04 - val_loss: 2.4995\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.9755e-04 - val_loss: 2.5003\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.9608e-04 - val_loss: 2.5011\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.9464e-04 - val_loss: 2.5016\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.9321e-04 - val_loss: 2.5022\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.9179e-04 - val_loss: 2.5031\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.9036e-04 - val_loss: 2.5041\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.8893e-04 - val_loss: 2.5051\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.8754e-04 - val_loss: 2.5055\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.8612e-04 - val_loss: 2.5059\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.8474e-04 - val_loss: 2.5067\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.8336e-04 - val_loss: 2.5078\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.8198e-04 - val_loss: 2.5085\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.8061e-04 - val_loss: 2.5091\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.7924e-04 - val_loss: 2.5100\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.7789e-04 - val_loss: 2.5111\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.7655e-04 - val_loss: 2.5119\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.7520e-04 - val_loss: 2.5125\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.7387e-04 - val_loss: 2.5130\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.7255e-04 - val_loss: 2.5137\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.7122e-04 - val_loss: 2.5146\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.6991e-04 - val_loss: 2.5155\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.6861e-04 - val_loss: 2.5162\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.6731e-04 - val_loss: 2.5168\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.6601e-04 - val_loss: 2.5175\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.6473e-04 - val_loss: 2.5184\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.6345e-04 - val_loss: 2.5191\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.6218e-04 - val_loss: 2.5198\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.6091e-04 - val_loss: 2.5203\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.5965e-04 - val_loss: 2.5211\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.5839e-04 - val_loss: 2.5221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xabc0d73108>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = X_train, y = y_train, epochs = 600, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xabc6314f88>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('test_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating the exact same model, including its weights and the optimizer\n",
    "model = tf.keras.models.load_model('test_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpy4axv4xs\\assets\n",
      "Size of h5 model: 68.12109375 KB\n",
      "Size of tflite model: 14.1640625 KB\n",
      "Decreased for factor: 4.809431880860452\n"
     ]
    }
   ],
   "source": [
    "# Converting the model without quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Saving the model to the disk\n",
    "open(\"test_quant.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "# size of the .h5 model\n",
    "h5_in_kb = os.path.getsize(\"test_model.h5\") / 1024\n",
    "print(\"Size of h5 model: {} KB\".format(h5_in_kb))\n",
    "\n",
    "# size of the .tflite model\n",
    "tflite_in_kb = os.path.getsize(\"test_quant.tflite\") / 1024\n",
    "print(\"Size of tflite model: {} KB\".format(tflite_in_kb))\n",
    "print(\"Decreased for factor: {}\".format(h5_in_kb/tflite_in_kb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1488: set_learning_phase (from tensorflow.python.keras.backend) is deprecated and will be removed after 2020-10-11.\n",
      "Instructions for updating:\n",
      "Simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1488: set_learning_phase (from tensorflow.python.keras.backend) is deprecated and will be removed after 2020-10-11.\n",
      "Instructions for updating:\n",
      "Simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp9hzz64f0\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp9hzz64f0\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\admin\\AppData\\Local\\Temp\\tmp9hzz64f0\\variables\\variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\admin\\AppData\\Local\\Temp\\tmp9hzz64f0\\variables\\variables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default', '__saved_model_init_op'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default', '__saved_model_init_op'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input tensors info: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input tensors info: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: serving_default_dense_input:0, shape: (-1, 90), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: serving_default_dense_input:0, shape: (-1, 90), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:output tensors info: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:output tensors info: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: dense_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: StatefulPartitionedCall:0, shape: (-1, 1), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: tensor name: StatefulPartitionedCall:0, shape: (-1, 1), type: DT_FLOAT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\admin\\AppData\\Local\\Temp\\tmp9hzz64f0\\variables\\variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\admin\\AppData\\Local\\Temp\\tmp9hzz64f0\\variables\\variables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\util.py:275: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\util.py:275: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\convert_to_constants.py:854: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\convert_to_constants.py:854: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5824"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the model to the tensorFlow lite format with full integer quantization\n",
    "scans = tf.cast(X_test, tf.float32)\n",
    "scans_data = tf.data.Dataset.from_tensor_slices(scans).batch(1)\n",
    "def representative_dataset_gen():\n",
    "  for input in scans_data.take(4):\n",
    "    # Get sample input data as a numpy array in a method of your choosing.\n",
    "    yield [input]\n",
    "    \n",
    "converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(\"test_model.h5\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.float32\n",
    "tflite_model = converter.convert()\n",
    "# Save to disk\n",
    "open(\"test_8bit.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
